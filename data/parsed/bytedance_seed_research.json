[
    {
        "id": "1e036357cdb955e85a05415daa514173",
        "source": "bytedance_seed",
        "type": "research",
        "title": "PXDesign: Fast, Modular, and Accurate De Novo Design of Protein Binders",
        "description": "PXDesign achieves nanomolar binder hit rates of 20\u201373% across five of six diverse protein targets, surpassing prior methods such as AlphaProteo. This experimental success rate is enabled by advances in both binder generation and filtering. We develop both a diffusion-based generative model (PXDesign-d) and a hallucination-based approach (PXDesign-h), each showing strong in silico performance that outperforms existing models. Beyond generation, we systematically analyze confidence-based filtering and ranking strategies from multiple structure predictors, comparing their accuracy, efficiency, and complementarity on datasets spanning de novo binders and mutagenesis. Finally, we validate the full design process experimentally, achieving high hit rates and multiple nanomolar binders.\n\nTo support future work and community use, we release a unified benchmarking framework at https://github.com/bytedance/PXDesignBench, provide public access to PXDesign via a webserver at https://protenix-server.com, and share all designed binder sequences at https://protenix.github.io/pxdesign.",
        "url": "https://seed.bytedance.com/en/research/pxdesign-fast-modular-and-accurate-de-novo-design-of-protein-binders",
        "published_date": "2025-09-01T16:00:00+00:00",
        "categories": [
            "Molecular Biology"
        ],
        "organization": "ByteDance Seed",
        "metadata": {
            "author": "Milong Ren,  Jinyuan Sun, Jiaqi Guan,  Cong Liu,  Chengyue Gong,  Yuzhe Wang, Lan Wang, Qixu Cai,  Xinshi Chen, Wenzhi Xiao, Protenix Team",
            "journal": "arXiv",
            "work_team": [
                "AI for Science"
            ]
        },
        "objects": [],
        "title_localized": {
            "en": "PXDesign: Fast, Modular, and Accurate De Novo Design of Protein Binders",
            "zh": ""
        },
        "description_localized": {
            "en": "PXDesign achieves nanomolar binder hit rates of 20\u201373% across five of six diverse protein targets, surpassing prior methods such as AlphaProteo. This experimental success rate is enabled by advances in both binder generation and filtering. We develop both a diffusion-based generative model (PXDesign-d) and a hallucination-based approach (PXDesign-h), each showing strong in silico performance that outperforms existing models. Beyond generation, we systematically analyze confidence-based filtering and ranking strategies from multiple structure predictors, comparing their accuracy, efficiency, and complementarity on datasets spanning de novo binders and mutagenesis. Finally, we validate the full design process experimentally, achieving high hit rates and multiple nanomolar binders.\n\nTo support future work and community use, we release a unified benchmarking framework at https://github.com/bytedance/PXDesignBench, provide public access to PXDesign via a webserver at https://protenix-server.com, and share all designed binder sequences at https://protenix.github.io/pxdesign.",
            "zh": ""
        },
        "url_localized": {
            "en": "https://seed.bytedance.com/en/research/pxdesign-fast-modular-and-accurate-de-novo-design-of-protein-binders",
            "zh": "https://seed.bytedance.com/zh/research/"
        },
        "external_url": "https://www.biorxiv.org/content/10.1101/2025.08.15.670450v2.article-info"
    },
    {
        "id": "dccf1debfc24b5c01efdc33b3731c87d",
        "source": "bytedance_seed",
        "type": "research",
        "title": "Robix: A Unified Model for Robot Interaction, Reasoning and Planning",
        "description": "We introduce Robix, a unified vision-language model designed to serve as the high-level cognitive layer in a hierarchical robot system, integrating robot reasoning, task planning, and natural language interaction within a single architecture. Robix dynamically generates atomic commands for low-level controllers alongside verbal responses for human interaction, enabling end-to-end execution of complex instructions, long-horizon task planning, and natural human-robot collaboration. The model also introduces novel capabilities such as proactive dialogue, real-time interruption handling, and context-aware commonsense reasoning during task execution. At its core, Robix employs chain-of-thought reasoning and is trained through a three-stage strategy: (1) continued pretraining to enhance embodied reasoning skills like 3D spatial understanding, visual grounding, and task-centric reasoning; (2) supervised finetuning to model human-robot interaction and task planning as a unified reasoning-action sequence; and (3) reinforcement learning to improve reasoning-action consistency and long-horizon task coherence. Extensive experiments show that Robix outperforms both open-source and commercial baselines\u2014including GPT-4o and Gemini 2.5 Pro\u2014in interactive task execution, demonstrating strong generalization across diverse instruction types (e.g., open-ended, multi-stage, constrained, invalid, and interrupted) and various user-involved tasks such as table bussing, grocery shopping, and dietary filtering.",
        "url": "https://seed.bytedance.com/en/research/robix-a-unified-model-for-robot-interaction-reasoning-and-planning",
        "published_date": "2025-08-31T16:00:00+00:00",
        "categories": [
            "Robotics"
        ],
        "organization": "ByteDance Seed",
        "metadata": {
            "author": "Huang Fang, Mengxi Zhang, Heng Dong, Wei Li, Zixuan Wang, Qifeng Zhang, Xueyun Tian, Yucheng Hu, Hang Li",
            "journal": "arXiv",
            "work_team": [
                "Robotics"
            ]
        },
        "objects": [],
        "title_localized": {
            "en": "Robix: A Unified Model for Robot Interaction, Reasoning and Planning",
            "zh": ""
        },
        "description_localized": {
            "en": "We introduce Robix, a unified vision-language model designed to serve as the high-level cognitive layer in a hierarchical robot system, integrating robot reasoning, task planning, and natural language interaction within a single architecture. Robix dynamically generates atomic commands for low-level controllers alongside verbal responses for human interaction, enabling end-to-end execution of complex instructions, long-horizon task planning, and natural human-robot collaboration. The model also introduces novel capabilities such as proactive dialogue, real-time interruption handling, and context-aware commonsense reasoning during task execution. At its core, Robix employs chain-of-thought reasoning and is trained through a three-stage strategy: (1) continued pretraining to enhance embodied reasoning skills like 3D spatial understanding, visual grounding, and task-centric reasoning; (2) supervised finetuning to model human-robot interaction and task planning as a unified reasoning-action sequence; and (3) reinforcement learning to improve reasoning-action consistency and long-horizon task coherence. Extensive experiments show that Robix outperforms both open-source and commercial baselines\u2014including GPT-4o and Gemini 2.5 Pro\u2014in interactive task execution, demonstrating strong generalization across diverse instruction types (e.g., open-ended, multi-stage, constrained, invalid, and interrupted) and various user-involved tasks such as table bussing, grocery shopping, and dietary filtering.",
            "zh": ""
        },
        "url_localized": {
            "en": "https://seed.bytedance.com/en/research/robix-a-unified-model-for-robot-interaction-reasoning-and-planning",
            "zh": "https://seed.bytedance.com/zh/research/"
        },
        "external_url": "https://arxiv.org/pdf/2509.01106"
    },
    {
        "id": "fa1600a231b65e2a2dbc3cdd326f6eed",
        "source": "bytedance_seed",
        "type": "research",
        "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving",
        "description": "LLMs have demonstrated strong mathematical reasoning abilities by leveraging reinforcement learning with long chain-of-thought, yet they continue to struggle with theorem proving due to the lack of clear supervision signals when solely using natural language. Dedicated domain-specific languages like Lean provide clear supervision via formal verification of proofs, enabling effective training through reinforcement learning. In this work, we propose \\textbf{Seed-Prover}, a lemma-style whole-proof reasoning model. Seed-Prover can iteratively refine its proof based on Lean feedback, proved lemmas, and self-summarization. To solve IMO-level contest problems, we design three test-time inference strategies that enable both deep and broad reasoning. Seed-Prover proves 78.1% of formalized past IMO problems, saturates MiniF2F, and achieves over 50\\% on PutnamBench, outperforming the previous state-of-the-art by a large margin. To address the lack of geometry support in Lean, we introduce a geometry reasoning engine \\textbf{Seed-Geometry}, which outperforms previous formal geometry engines. We use these two systems to participate in IMO 2025 and fully prove 5 out of 6 problems. This work represents a significant advancement in automated mathematical reasoning, demonstrating the effectiveness of formal verification with long chain-of-thought reasoning.",
        "url": "https://seed.bytedance.com/en/research/seed-prover-deep-and-broad-reasoning-for-automated-theorem-proving",
        "published_date": "2025-07-30T16:00:00+00:00",
        "categories": [
            "LLM"
        ],
        "organization": "ByteDance Seed",
        "metadata": {
            "author": "Luoxin Chen, Jinming Gu, Liankai Huang, Wenhao Huang, Zhicheng Jiang, Allan Jie, Xiaoran Jin, Xing Jin, Chenggang Li, Kaijing Ma, Cheng Ren, Jiawei Shen, Wenlei Shi, Tong Sun, He Sun, Jiahui Wang, Siran Wang, Zhihong Wang, Chenrui Wei, Shufa Wei, Yonghui Wu, Yuchen Wu, Yihang Xia, Huajian Xin, Fan Yang, Huaiyuan Ying, Hongyi Yuan, Zheng Yuan, Tianyang Zhan, Chi Zhang, Yue Zhang, Ge Zhang, Tianyun Zhao, Jianqiu Zhao, Yichi Zhou, Thomas Hanwen Zhu",
            "journal": "arXiv",
            "work_team": [
                "LLM"
            ]
        },
        "objects": [],
        "title_localized": {
            "en": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving",
            "zh": ""
        },
        "description_localized": {
            "en": "LLMs have demonstrated strong mathematical reasoning abilities by leveraging reinforcement learning with long chain-of-thought, yet they continue to struggle with theorem proving due to the lack of clear supervision signals when solely using natural language. Dedicated domain-specific languages like Lean provide clear supervision via formal verification of proofs, enabling effective training through reinforcement learning. In this work, we propose \\textbf{Seed-Prover}, a lemma-style whole-proof reasoning model. Seed-Prover can iteratively refine its proof based on Lean feedback, proved lemmas, and self-summarization. To solve IMO-level contest problems, we design three test-time inference strategies that enable both deep and broad reasoning. Seed-Prover proves 78.1% of formalized past IMO problems, saturates MiniF2F, and achieves over 50\\% on PutnamBench, outperforming the previous state-of-the-art by a large margin. To address the lack of geometry support in Lean, we introduce a geometry reasoning engine \\textbf{Seed-Geometry}, which outperforms previous formal geometry engines. We use these two systems to participate in IMO 2025 and fully prove 5 out of 6 problems. This work represents a significant advancement in automated mathematical reasoning, demonstrating the effectiveness of formal verification with long chain-of-thought reasoning.",
            "zh": ""
        },
        "url_localized": {
            "en": "https://seed.bytedance.com/en/research/seed-prover-deep-and-broad-reasoning-for-automated-theorem-proving",
            "zh": "https://seed.bytedance.com/zh/research/"
        },
        "external_url": "https://arxiv.org/abs/2507.23726"
    },
    {
        "id": "b6095ba19e6ae163727b023559dbcecf",
        "source": "bytedance_seed",
        "type": "research",
        "title": "Seed LiveInterpret 2.0: End-to-end Simultaneous Speech-to-speech Translation with Your Voice",
        "description": "Simultaneous Interpretation (SI) represents one of the most daunting frontiers in the translation industry, with product-level automatic systems long plagued by intractable challenges: subpar transcription and translation quality, lack of real-time speech generation, multi-speaker confusion, and translated speech inflation, especially in long-form discourses. In this study, we introduce Seed-LiveInterpret 2.0, an end-to-end SI model that delivers high-fidelity, ultra-low-latency speech-to-speech generation with voice cloning capabilities. As a fully operational product-level solution, Seed-LiveInterpret 2.0 tackles these challenges head-on through our novel duplex speech-to-speech understanding-generating framework. Experimental results demonstrate that through large-scale pretraining and reinforcement learning, the model achieves a significantly better balance between translation accuracy and latency, validated by human interpreters to exceed 70% correctness in complex scenarios. Notably, Seed-LiveInterpret 2.0 outperforms commercial SI solutions by significant margins in translation quality, while slashing the average latency of cloned speech from nearly 10 seconds to a near-real-time 3 seconds, which is around a near 70% reduction that drastically enhances practical usability.",
        "url": "https://seed.bytedance.com/en/research/seed-liveinterpret-2-0-end-to-end-simultaneous-speech-to-speech-translation-with-your-voice",
        "published_date": "2025-07-23T16:00:00+00:00",
        "categories": [
            "Speech&Audio"
        ],
        "organization": "ByteDance Seed",
        "metadata": {
            "author": "Seed Speech Team",
            "journal": "arXiv",
            "work_team": [
                "Speech"
            ]
        },
        "objects": [],
        "title_localized": {
            "en": "Seed LiveInterpret 2.0: End-to-end Simultaneous Speech-to-speech Translation with Your Voice",
            "zh": ""
        },
        "description_localized": {
            "en": "Simultaneous Interpretation (SI) represents one of the most daunting frontiers in the translation industry, with product-level automatic systems long plagued by intractable challenges: subpar transcription and translation quality, lack of real-time speech generation, multi-speaker confusion, and translated speech inflation, especially in long-form discourses. In this study, we introduce Seed-LiveInterpret 2.0, an end-to-end SI model that delivers high-fidelity, ultra-low-latency speech-to-speech generation with voice cloning capabilities. As a fully operational product-level solution, Seed-LiveInterpret 2.0 tackles these challenges head-on through our novel duplex speech-to-speech understanding-generating framework. Experimental results demonstrate that through large-scale pretraining and reinforcement learning, the model achieves a significantly better balance between translation accuracy and latency, validated by human interpreters to exceed 70% correctness in complex scenarios. Notably, Seed-LiveInterpret 2.0 outperforms commercial SI solutions by significant margins in translation quality, while slashing the average latency of cloned speech from nearly 10 seconds to a near-real-time 3 seconds, which is around a near 70% reduction that drastically enhances practical usability.",
            "zh": ""
        },
        "url_localized": {
            "en": "https://seed.bytedance.com/en/research/seed-liveinterpret-2-0-end-to-end-simultaneous-speech-to-speech-translation-with-your-voice",
            "zh": "https://seed.bytedance.com/zh/research/"
        },
        "external_url": "https://arxiv.org/pdf/2507.17527"
    },
    {
        "id": "b300005dd642e1d107c0f5161d3f598a",
        "source": "bytedance_seed",
        "type": "research",
        "title": "Seedance 1.0: Exploring the Boundaries of Video Generation Models",
        "description": "Notable advances in diffusion modeling have propelled rapid improvements in video generation, yet current foundational model still confront critical challenges in synergistically balancing prompt following, motion plausibility, and visual quality. In this report, we introduce Seedance 1.0, a high-performance and inference-efficient video foundation generation model that integrates several core technical improvements: (i) multi-source data curation augmented with precision and meaningful video captioning, enabling comprehensive learning across diverse scenarios; (ii) an efficient pre-training paradigm that enables multiple features or functions such as interleaved multimodal positional encoding, native multi-shot generation capacity, and multi-task modeling; (iii) carefully-designed post-training optimization leveraging fine-grained supervised fine-tuning, video-specific RLHF with multi-dimensional reward mechanisms for considerable performance improvements; (iv) excellent model acceleration achieving 10\u00d7 inference speedup through multi- stage distillation strategies and system-level optimizations. Seedance 1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds. Compared to state-of-the-art video generation models, Seedance 1.0 stands out with high-quality and fast video generation with superior spatiotemporal fluidity with structural stability, precise instruction adherence in complex multi-subject contexts, native multi-shot narrative coherence with consistent subject representation, and ultra-fast inference.",
        "url": "https://seed.bytedance.com/en/research/seedance-1-0-exploring-the-boundaries-of-video-generation-models",
        "published_date": "2025-06-11T03:08:06+00:00",
        "categories": [
            "Computer Vision"
        ],
        "organization": "ByteDance Seed",
        "metadata": {
            "author": "Seed Vision Team",
            "journal": "arXiv",
            "work_team": [
                "Vision"
            ]
        },
        "objects": [],
        "title_localized": {
            "en": "Seedance 1.0: Exploring the Boundaries of Video Generation Models",
            "zh": ""
        },
        "description_localized": {
            "en": "Notable advances in diffusion modeling have propelled rapid improvements in video generation, yet current foundational model still confront critical challenges in synergistically balancing prompt following, motion plausibility, and visual quality. In this report, we introduce Seedance 1.0, a high-performance and inference-efficient video foundation generation model that integrates several core technical improvements: (i) multi-source data curation augmented with precision and meaningful video captioning, enabling comprehensive learning across diverse scenarios; (ii) an efficient pre-training paradigm that enables multiple features or functions such as interleaved multimodal positional encoding, native multi-shot generation capacity, and multi-task modeling; (iii) carefully-designed post-training optimization leveraging fine-grained supervised fine-tuning, video-specific RLHF with multi-dimensional reward mechanisms for considerable performance improvements; (iv) excellent model acceleration achieving 10\u00d7 inference speedup through multi- stage distillation strategies and system-level optimizations. Seedance 1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds. Compared to state-of-the-art video generation models, Seedance 1.0 stands out with high-quality and fast video generation with superior spatiotemporal fluidity with structural stability, precise instruction adherence in complex multi-subject contexts, native multi-shot narrative coherence with consistent subject representation, and ultra-fast inference.",
            "zh": ""
        },
        "url_localized": {
            "en": "https://seed.bytedance.com/en/research/seedance-1-0-exploring-the-boundaries-of-video-generation-models",
            "zh": "https://seed.bytedance.com/zh/research/"
        },
        "external_url": "https://arxiv.org/pdf/2506.09113"
    },
    {
        "id": "e1aa581108c4694a9e6091df5ebba6fb",
        "source": "bytedance_seed",
        "type": "research",
        "title": "SeedEdit 3.0: Fast and High-Quality Generative Image Editing",
        "description": "We introduce SeedEdit 3.0, in companion with our T2I model Seedream 3.0, which significantly improves over our previous SeedEdit versions in both aspects of edit instruction following and image content (e.g., ID/IP) preservation on real image inputs. Additional to model upgrading with T2I, in this report, we present several key improvements. First, we develop an enhanced data curation pipeline with a meta-info paradigm and meta-info embedding strategy that help mix images from multiple data sources. This allows us to scale editing data effectively, and meta information is helpfult to connect VLM with diffusion model more closely. Second, we introduce a joint learning pipeline for computing a diffusion loss and reward losses. Finally, we evaluate SeedEdit 3.0 on our testing benchmarks, for real/synthetic image editing, where it achieves a best trade-off between multiple aspects, yielding a high usability rate of 56.1%, compared to SeedEdit 1.6 (38.4%), GPT4o (37.1%) and Gemini 2.0 (30.3%).",
        "url": "https://seed.bytedance.com/en/research/seededit-3-0-fast-and-high-quality-generative-image-editing",
        "published_date": "2025-06-04T16:00:00+00:00",
        "categories": [
            "Computer Vision"
        ],
        "organization": "ByteDance Seed",
        "metadata": {
            "author": "Peng Wang, Yichun Shi, Xiaochen Lian, Zhonghua Zhai, Xin Xia, Xuefeng Xiao, Weilin Huang, Jianchao Yang",
            "journal": "arXiv",
            "work_team": [
                "Vision"
            ]
        },
        "objects": [],
        "title_localized": {
            "en": "SeedEdit 3.0: Fast and High-Quality Generative Image Editing",
            "zh": ""
        },
        "description_localized": {
            "en": "We introduce SeedEdit 3.0, in companion with our T2I model Seedream 3.0, which significantly improves over our previous SeedEdit versions in both aspects of edit instruction following and image content (e.g., ID/IP) preservation on real image inputs. Additional to model upgrading with T2I, in this report, we present several key improvements. First, we develop an enhanced data curation pipeline with a meta-info paradigm and meta-info embedding strategy that help mix images from multiple data sources. This allows us to scale editing data effectively, and meta information is helpfult to connect VLM with diffusion model more closely. Second, we introduce a joint learning pipeline for computing a diffusion loss and reward losses. Finally, we evaluate SeedEdit 3.0 on our testing benchmarks, for real/synthetic image editing, where it achieves a best trade-off between multiple aspects, yielding a high usability rate of 56.1%, compared to SeedEdit 1.6 (38.4%), GPT4o (37.1%) and Gemini 2.0 (30.3%).",
            "zh": ""
        },
        "url_localized": {
            "en": "https://seed.bytedance.com/en/research/seededit-3-0-fast-and-high-quality-generative-image-editing",
            "zh": "https://seed.bytedance.com/zh/research/"
        },
        "external_url": "https://arxiv.org/abs/2506.05083"
    }
]