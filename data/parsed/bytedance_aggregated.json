[
    {
        "id": "dfe46be283f2dd9fd546f536e8e764cf",
        "source": "bytedance_aggregated",
        "type": "activity",
        "title": "[HuggingFace] updated 3 models",
        "description": "huggingface_ByteDanceSeed: updated 3 models",
        "url": "https://huggingface.co/huggingface_ByteDanceSeed",
        "published_date": "2025-07-28T12:25:47.272959+00:00",
        "categories": [],
        "organization": "huggingface_ByteDanceSeed",
        "metadata": {
            "action": "updated 3 models"
        },
        "objects": [
            {
                "title": "ByteDance-Seed/Seed-X-PPO-7B-AWQ-Int4",
                "url": "https://huggingface.co/ByteDance-Seed/Seed-X-PPO-7B-AWQ-Int4",
                "type": "Translation"
            },
            {
                "title": "ByteDance-Seed/Seed-X-Instruct-7B",
                "url": "https://huggingface.co/ByteDance-Seed/Seed-X-Instruct-7B",
                "type": "Translation"
            },
            {
                "title": "ByteDance-Seed/Seed-X-RM-7B",
                "url": "https://huggingface.co/ByteDance-Seed/Seed-X-RM-7B",
                "type": "Updatedabout 6 hours ago"
            }
        ],
        "original_source": "huggingface"
    },
    {
        "id": "0053d68529df1d3bc15d80cfdfa0b847",
        "source": "bytedance_aggregated",
        "type": "activity",
        "title": "[HuggingFace] published 2 models",
        "description": "huggingface_ByteDanceSeed: published 2 models",
        "url": "https://huggingface.co/huggingface_ByteDanceSeed",
        "published_date": "2025-07-28T11:25:47.272959+00:00",
        "categories": [],
        "organization": "huggingface_ByteDanceSeed",
        "metadata": {
            "action": "published 2 models"
        },
        "objects": [
            {
                "title": "ByteDance-Seed/Seed-X-PPO-7B-AWQ-Int4",
                "url": "https://huggingface.co/ByteDance-Seed/Seed-X-PPO-7B-AWQ-Int4",
                "type": "Translation"
            },
            {
                "title": "ByteDance-Seed/Seed-X-PPO-7B-GPTQ-Int8",
                "url": "https://huggingface.co/ByteDance-Seed/Seed-X-PPO-7B-GPTQ-Int8",
                "type": "Translation"
            }
        ],
        "original_source": "huggingface"
    },
    {
        "id": "5be39447d15d33197724b44893f94e92",
        "source": "bytedance_aggregated",
        "type": "activity",
        "title": "[HuggingFace] updated 2 models",
        "description": "huggingface_ByteDanceSeed: updated 2 models",
        "url": "https://huggingface.co/huggingface_ByteDanceSeed",
        "published_date": "2025-07-28T11:25:47.272959+00:00",
        "categories": [],
        "organization": "huggingface_ByteDanceSeed",
        "metadata": {
            "action": "updated 2 models"
        },
        "objects": [
            {
                "title": "ByteDance-Seed/Seed-X-PPO-7B",
                "url": "https://huggingface.co/ByteDance-Seed/Seed-X-PPO-7B",
                "type": "Translation"
            },
            {
                "title": "ByteDance-Seed/Seed-X-PPO-7B-GPTQ-Int8",
                "url": "https://huggingface.co/ByteDance-Seed/Seed-X-PPO-7B-GPTQ-Int8",
                "type": "Translation"
            }
        ],
        "original_source": "huggingface"
    },
    {
        "id": "59cd247bb1ce8dc12c70eb36ef4cd496",
        "source": "bytedance_aggregated",
        "type": "activity",
        "title": "[HuggingFace] updated a model",
        "description": "huggingface_bytedanceresearch: updated a model",
        "url": "https://huggingface.co/huggingface_bytedanceresearch",
        "published_date": "2025-07-28T05:25:47.272959+00:00",
        "categories": [],
        "organization": "huggingface_bytedanceresearch",
        "metadata": {
            "action": "updated a model"
        },
        "objects": [
            {
                "title": "bytedance-research/ChatTS-14B",
                "url": "https://huggingface.co/bytedance-research/ChatTS-14B",
                "type": "Text Generation"
            }
        ],
        "original_source": "huggingface"
    },
    {
        "id": "3b2e9cb356512dc6f92230dcb8b3efc1",
        "source": "bytedance_aggregated",
        "type": "activity",
        "title": "[HuggingFace] updated a model",
        "description": "huggingface_ByteDance: updated a model",
        "url": "https://huggingface.co/huggingface_ByteDance",
        "published_date": "2025-07-24T18:25:47.272959+00:00",
        "categories": [],
        "organization": "huggingface_ByteDance",
        "metadata": {
            "action": "updated a model"
        },
        "objects": [
            {
                "title": "ByteDance/InfiniteYou",
                "url": "https://huggingface.co/ByteDance/InfiniteYou",
                "type": "Text-to-Image"
            }
        ],
        "original_source": "huggingface"
    },
    {
        "id": "b6095ba19e6ae163727b023559dbcecf",
        "source": "bytedance_seed",
        "type": "research",
        "title": "Seed LiveInterpret 2.0: End-to-end Simultaneous Speech-to-speech Translation with Your Voice",
        "description": "Simultaneous Interpretation (SI) represents one of the most daunting frontiers in the translation industry, with product-level automatic systems long plagued by intractable challenges: subpar transcription and translation quality, lack of real-time speech generation, multi-speaker confusion, and translated speech inflation, especially in long-form discourses. In this study, we introduce Seed-LiveInterpret 2.0, an end-to-end SI model that delivers high-fidelity, ultra-low-latency speech-to-speech generation with voice cloning capabilities. As a fully operational product-level solution, Seed-LiveInterpret 2.0 tackles these challenges head-on through our novel duplex speech-to-speech understanding-generating framework. Experimental results demonstrate that through large-scale pretraining and reinforcement learning, the model achieves a significantly better balance between translation accuracy and latency, validated by human interpreters to exceed 70% correctness in complex scenarios. Notably, Seed-LiveInterpret 2.0 outperforms commercial SI solutions by significant margins in translation quality, while slashing the average latency of cloned speech from nearly 10 seconds to a near-real-time 3 seconds, which is around a near 70% reduction that drastically enhances practical usability.",
        "url": "https://seed.bytedance.com/en/research/seed-liveinterpret-2-0-end-to-end-simultaneous-speech-to-speech-translation-with-your-voice",
        "published_date": "2025-07-23T11:00:00+00:00",
        "categories": [
            "Speech&Audio"
        ],
        "organization": "ByteDance Seed",
        "metadata": {
            "author": "Seed Speech Team",
            "journal": "arXiv",
            "work_team": [
                "Speech"
            ]
        },
        "objects": [],
        "title_localized": {
            "en": "Seed LiveInterpret 2.0: End-to-end Simultaneous Speech-to-speech Translation with Your Voice",
            "zh": ""
        },
        "description_localized": {
            "en": "Simultaneous Interpretation (SI) represents one of the most daunting frontiers in the translation industry, with product-level automatic systems long plagued by intractable challenges: subpar transcription and translation quality, lack of real-time speech generation, multi-speaker confusion, and translated speech inflation, especially in long-form discourses. In this study, we introduce Seed-LiveInterpret 2.0, an end-to-end SI model that delivers high-fidelity, ultra-low-latency speech-to-speech generation with voice cloning capabilities. As a fully operational product-level solution, Seed-LiveInterpret 2.0 tackles these challenges head-on through our novel duplex speech-to-speech understanding-generating framework. Experimental results demonstrate that through large-scale pretraining and reinforcement learning, the model achieves a significantly better balance between translation accuracy and latency, validated by human interpreters to exceed 70% correctness in complex scenarios. Notably, Seed-LiveInterpret 2.0 outperforms commercial SI solutions by significant margins in translation quality, while slashing the average latency of cloned speech from nearly 10 seconds to a near-real-time 3 seconds, which is around a near 70% reduction that drastically enhances practical usability.",
            "zh": ""
        },
        "url_localized": {
            "en": "https://seed.bytedance.com/en/research/seed-liveinterpret-2-0-end-to-end-simultaneous-speech-to-speech-translation-with-your-voice",
            "zh": "https://seed.bytedance.com/zh/research/"
        },
        "external_url": "https://arxiv.org/pdf/2507.17527"
    },
    {
        "id": "0a250cd7c4cc2e4f3ae29337b4a499ff",
        "source": "bytedance_seed",
        "type": "blog",
        "title": "ByteDance Seed Prover Achieves Silver Medal Score in IMO 2025",
        "description": "Formalization + Test-time scaling, deep thinking in math reasoning",
        "url": "https://seed.bytedance.com/en/blog/bytedance-seed-prover-achieves-silver-medal-score-in-imo-2025",
        "published_date": "2025-07-22T11:00:00+00:00",
        "categories": [
            "Technology Launch"
        ],
        "organization": "ByteDance Seed",
        "metadata": {},
        "objects": [],
        "title_localized": {
            "en": "ByteDance Seed Prover Achieves Silver Medal Score in IMO 2025",
            "zh": "\u5b57\u8282\u8df3\u52a8 Seed Prover \u53d6\u5f97 IMO 2025 \u94f6\u724c\u5206\u6570"
        },
        "description_localized": {
            "en": "Formalization + Test-time scaling, deep thinking in math reasoning",
            "zh": "\u5f62\u5f0f\u5316\u9a8c\u8bc1+\u6d4b\u7b97\u65f6\u7b97\u529b\u62d3\u5c55\uff0c\u63d0\u5347\u6570\u5b66\u601d\u8003\u6df1\u5ea6"
        },
        "url_localized": {
            "en": "https://seed.bytedance.com/en/blog/bytedance-seed-prover-achieves-silver-medal-score-in-imo-2025",
            "zh": "https://seed.bytedance.com/zh/blog/\u5b57\u8282\u8df3\u52a8-seed-prover-\u53d6\u5f97-imo-2025-\u94f6\u724c\u5206\u6570"
        }
    },
    {
        "id": "f6b99d486ee1bb500ef202a8514fb83d",
        "source": "bytedance_seed",
        "type": "blog",
        "title": "Seed Research\uffe8GR-3 Released: A Generalist Robot Model for Generalization, Long-Horizon Tasks, and Bi-Manual Deformable Object Manipulation",
        "description": "A generalizable robot operation model that supports long-range and complex tasks",
        "url": "https://seed.bytedance.com/en/blog/seed-research-gr-3-released-a-generalist-robot-model-for-generalization-long-horizon-tasks-and-bi-manual-deformable-object-manipulation",
        "published_date": "2025-07-21T11:00:00+00:00",
        "categories": [
            "Seed Research"
        ],
        "organization": "ByteDance Seed",
        "metadata": {},
        "objects": [],
        "title_localized": {
            "en": "Seed Research\uffe8GR-3 Released: A Generalist Robot Model for Generalization, Long-Horizon Tasks, and Bi-Manual Deformable Object Manipulation",
            "zh": "Seed Research\uffe8\u901a\u7528\u673a\u5668\u4eba\u6a21\u578b GR-3 \u53d1\u5e03\uff01\u652f\u6301\u9ad8\u6cdb\u5316\u3001\u957f\u7a0b\u4efb\u52a1\u3001\u67d4\u6027\u7269\u4f53\u53cc\u81c2\u64cd\u4f5c"
        },
        "description_localized": {
            "en": "A generalizable robot operation model that supports long-range and complex tasks",
            "zh": "\u4e00\u4e2a\u53ef\u6cdb\u5316\u3001\u652f\u6301\u957f\u7a0b\u4ee5\u53ca\u590d\u6742\u4efb\u52a1\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u5927\u6a21\u578b"
        },
        "url_localized": {
            "en": "https://seed.bytedance.com/en/blog/seed-research-gr-3-released-a-generalist-robot-model-for-generalization-long-horizon-tasks-and-bi-manual-deformable-object-manipulation",
            "zh": "https://seed.bytedance.com/zh/blog/seed-research-\u901a\u7528\u673a\u5668\u4eba\u6a21\u578b-gr-3-\u53d1\u5e03-\u652f\u6301\u9ad8\u6cdb\u5316-\u957f\u7a0b\u4efb\u52a1-\u67d4\u6027\u7269\u4f53\u53cc\u81c2\u64cd\u4f5c"
        }
    },
    {
        "id": "766165d858739f72b3016593fcaea388",
        "source": "bytedance_aggregated",
        "type": "activity",
        "title": "[HuggingFace] published 3 models",
        "description": "huggingface_ByteDanceSeed: published 3 models",
        "url": "https://huggingface.co/huggingface_ByteDanceSeed",
        "published_date": "2025-07-17T18:25:47.272959+00:00",
        "categories": [],
        "organization": "huggingface_ByteDanceSeed",
        "metadata": {
            "action": "published 3 models"
        },
        "objects": [
            {
                "title": "ByteDance-Seed/Seed-X-Instruct-7B",
                "url": "https://huggingface.co/ByteDance-Seed/Seed-X-Instruct-7B",
                "type": "Translation"
            },
            {
                "title": "ByteDance-Seed/Seed-X-RM-7B",
                "url": "https://huggingface.co/ByteDance-Seed/Seed-X-RM-7B",
                "type": "Updatedabout 6 hours ago"
            },
            {
                "title": "ByteDance-Seed/Seed-X-PPO-7B",
                "url": "https://huggingface.co/ByteDance-Seed/Seed-X-PPO-7B",
                "type": "Translation"
            }
        ],
        "original_source": "huggingface"
    },
    {
        "id": "9c082e61a56bdfde4295d3694e03caf8",
        "source": "bytedance_aggregated",
        "type": "activity",
        "title": "[HuggingFace] authored a paper",
        "description": "huggingface_ByteDance: authored a paper",
        "url": "https://huggingface.co/huggingface_ByteDance",
        "published_date": "2025-07-15T18:25:47.272959+00:00",
        "categories": [],
        "organization": "huggingface_ByteDance",
        "metadata": {
            "action": "authored a paper"
        },
        "objects": [
            {
                "title": "MoVieS: Motion-Aware 4D Dynamic View Synthesis in One Second",
                "url": "https://huggingface.co/papers/2507.10065",
                "type": ""
            }
        ],
        "original_source": "huggingface"
    },
    {
        "id": "4bab773d3b449d3b143ba44841e47436",
        "source": "bytedance_aggregated",
        "type": "activity",
        "title": "[HuggingFace] updated a dataset",
        "description": "huggingface_ByteDanceSeed: updated a dataset",
        "url": "https://huggingface.co/huggingface_ByteDanceSeed",
        "published_date": "2025-07-14T18:25:47.272959+00:00",
        "categories": [],
        "organization": "huggingface_ByteDanceSeed",
        "metadata": {
            "action": "updated a dataset"
        },
        "objects": [
            {
                "title": "ByteDance-Seed/Multi-SWE-bench-flash",
                "url": "https://huggingface.co/datasets/ByteDance-Seed/Multi-SWE-bench-flash",
                "type": ""
            }
        ],
        "original_source": "huggingface"
    },
    {
        "id": "a6f63103a81c7d1c4fae2df872853569",
        "source": "bytedance_aggregated",
        "type": "activity",
        "title": "[HuggingFace] updated a model",
        "description": "huggingface_ByteDanceSeed: updated a model",
        "url": "https://huggingface.co/huggingface_ByteDanceSeed",
        "published_date": "2025-07-14T18:25:47.272959+00:00",
        "categories": [],
        "organization": "huggingface_ByteDanceSeed",
        "metadata": {
            "action": "updated a model"
        },
        "objects": [
            {
                "title": "ByteDance-Seed/SeedVR2-7B",
                "url": "https://huggingface.co/ByteDance-Seed/SeedVR2-7B",
                "type": "Video-to-Video"
            }
        ],
        "original_source": "huggingface"
    },
    {
        "id": "e1cd672572b664f0d86285adc18e617d",
        "source": "bytedance_aggregated",
        "type": "activity",
        "title": "[HuggingFace] published a dataset",
        "description": "huggingface_ByteDanceSeed: published a dataset",
        "url": "https://huggingface.co/huggingface_ByteDanceSeed",
        "published_date": "2025-07-14T18:25:47.272959+00:00",
        "categories": [],
        "organization": "huggingface_ByteDanceSeed",
        "metadata": {
            "action": "published a dataset"
        },
        "objects": [
            {
                "title": "ByteDance-Seed/Multi-SWE-bench-flash",
                "url": "https://huggingface.co/datasets/ByteDance-Seed/Multi-SWE-bench-flash",
                "type": ""
            }
        ],
        "original_source": "huggingface"
    },
    {
        "id": "196cb2767a5f12ce27033d2edfe814c7",
        "source": "bytedance_seed",
        "type": "blog",
        "title": "4B Model Rivals 235B Performance in Mathematical Reasoning: Seed\u2019s Latest RL Training Recipe Fully Open-Sourced",
        "description": "700-step RL training significantly enhances mathematical reasoning capabilities in smaller models.",
        "url": "https://seed.bytedance.com/en/blog/4b-model-rivals-235b-performance-in-mathematical-reasoning-seed-s-latest-rl-training-recipe-fully-open-sourced",
        "published_date": "2025-07-14T11:00:00+00:00",
        "categories": [
            "Seed Research"
        ],
        "organization": "ByteDance Seed",
        "metadata": {},
        "objects": [],
        "title_localized": {
            "en": "4B Model Rivals 235B Performance in Mathematical Reasoning: Seed\u2019s Latest RL Training Recipe Fully Open-Sourced",
            "zh": "Seed Research\uff5c4B \u6a21\u578b\u6570\u5b66\u63a8\u7406\u63a5\u8fd1 235B \u8868\u73b0\uff0cSeed \u6700\u65b0\u5f3a\u5316\u5b66\u4e60\u914d\u65b9\u5168\u9762\u5f00\u6e90"
        },
        "description_localized": {
            "en": "700-step RL training significantly enhances mathematical reasoning capabilities in smaller models.",
            "zh": "700 \u6b65 RL \u8bad\u7ec3\uff0c\u5927\u5e45\u63d0\u5347\u5c0f\u6a21\u578b\u6570\u5b66\u63a8\u7406\u80fd\u529b"
        },
        "url_localized": {
            "en": "https://seed.bytedance.com/en/blog/4b-model-rivals-235b-performance-in-mathematical-reasoning-seed-s-latest-rl-training-recipe-fully-open-sourced",
            "zh": "https://seed.bytedance.com/zh/blog/seed-research-4b-\u6a21\u578b\u6570\u5b66\u63a8\u7406\u63a5\u8fd1-235b-\u8868\u73b0-seed-\u6700\u65b0\u5f3a\u5316\u5b66\u4e60\u914d\u65b9\u5168\u9762\u5f00\u6e90"
        }
    },
    {
        "id": "681fbb30ff8fa07ac8003c9c27db1853",
        "source": "bytedance_seed",
        "type": "blog",
        "title": "Meet Seed at ICML 2025: 25 Papers Accepted",
        "description": "Welcome on-site communication!",
        "url": "https://seed.bytedance.com/en/blog/meet-seed-at-icml-2025-25-papers-accepted",
        "published_date": "2025-07-13T11:00:00+00:00",
        "categories": [
            "Academic Collaboration and Events"
        ],
        "organization": "ByteDance Seed",
        "metadata": {},
        "objects": [],
        "title_localized": {
            "en": "Meet Seed at ICML 2025: 25 Papers Accepted",
            "zh": "25 \u7bc7\u6210\u679c\u5165\u9009\uff0c\u5b57\u8282\u8df3\u52a8 Seed \u4e0e\u4f60\u5171\u8d74 ICML 2025"
        },
        "description_localized": {
            "en": "Welcome on-site communication!",
            "zh": "\u671f\u5f85\u4e0e\u4f60\u73b0\u573a\u4ea4\u6d41\uff01"
        },
        "url_localized": {
            "en": "https://seed.bytedance.com/en/blog/meet-seed-at-icml-2025-25-papers-accepted",
            "zh": "https://seed.bytedance.com/zh/blog/25-\u7bc7\u6210\u679c\u5165\u9009-\u5b57\u8282\u8df3\u52a8-seed-\u4e0e\u4f60\u5171\u8d74-icml-2025"
        }
    },
    {
        "id": "c9c807ee1035928c49b45d13e4417e83",
        "source": "bytedance_aggregated",
        "type": "activity",
        "title": "[HuggingFace] authored a paper",
        "description": "huggingface_ByteDanceSeed: authored a paper",
        "url": "https://huggingface.co/huggingface_ByteDanceSeed",
        "published_date": "2025-07-02T18:25:47.272959+00:00",
        "categories": [],
        "organization": "huggingface_ByteDanceSeed",
        "metadata": {
            "action": "authored a paper"
        },
        "objects": [
            {
                "title": "VINCIE: Unlocking In-context Image Editing from Video",
                "url": "https://huggingface.co/papers/2506.10941",
                "type": ""
            }
        ],
        "original_source": "huggingface"
    },
    {
        "id": "17e8355d13349a6a4b6cbb88dc4dc223",
        "source": "bytedance_aggregated",
        "type": "activity",
        "title": "[HuggingFace] published a model",
        "description": "huggingface_ByteDanceSeed: published a model",
        "url": "https://huggingface.co/huggingface_ByteDanceSeed",
        "published_date": "2025-07-02T18:25:47.272959+00:00",
        "categories": [],
        "organization": "huggingface_ByteDanceSeed",
        "metadata": {
            "action": "published a model"
        },
        "objects": [
            {
                "title": "ByteDance-Seed/VINCIE-3B",
                "url": "https://huggingface.co/ByteDance-Seed/VINCIE-3B",
                "type": "Image-to-Image"
            }
        ],
        "original_source": "huggingface"
    },
    {
        "id": "3f28aa50eb6c6efe7ed58abb9e91e3f4",
        "source": "bytedance_aggregated",
        "type": "activity",
        "title": "[HuggingFace] published a model",
        "description": "huggingface_ByteDance: published a model",
        "url": "https://huggingface.co/huggingface_ByteDance",
        "published_date": "2025-06-28T18:25:47.272959+00:00",
        "categories": [],
        "organization": "huggingface_ByteDance",
        "metadata": {
            "action": "published a model"
        },
        "objects": [
            {
                "title": "ByteDance/XVerse",
                "url": "https://huggingface.co/ByteDance/XVerse",
                "type": "Text-to-Image"
            }
        ],
        "original_source": "huggingface"
    },
    {
        "id": "fd0f4e1bead7da64e4491f94f3d60590",
        "source": "bytedance_aggregated",
        "type": "activity",
        "title": "[HuggingFace] published 2 datasets",
        "description": "huggingface_ByteDanceSeed: published 2 datasets",
        "url": "https://huggingface.co/huggingface_ByteDanceSeed",
        "published_date": "2025-06-28T18:25:47.272959+00:00",
        "categories": [],
        "organization": "huggingface_ByteDanceSeed",
        "metadata": {
            "action": "published 2 datasets"
        },
        "objects": [
            {
                "title": "ByteDance-Seed/BeyondAIME",
                "url": "https://huggingface.co/datasets/ByteDance-Seed/BeyondAIME",
                "type": ""
            },
            {
                "title": "ByteDance-Seed/ScienceOlympiad",
                "url": "https://huggingface.co/datasets/ByteDance-Seed/ScienceOlympiad",
                "type": ""
            }
        ],
        "original_source": "huggingface"
    },
    {
        "id": "d06ffd9c3a95c5c7d9de200ada912a05",
        "source": "bytedance_aggregated",
        "type": "activity",
        "title": "[HuggingFace] authored a paper",
        "description": "huggingface_bytedanceresearch: authored a paper",
        "url": "https://huggingface.co/huggingface_bytedanceresearch",
        "published_date": "2025-06-28T18:25:47.272959+00:00",
        "categories": [],
        "organization": "huggingface_bytedanceresearch",
        "metadata": {
            "action": "authored a paper"
        },
        "objects": [
            {
                "title": "Phantom-Data : Towards a General Subject-Consistent Video Generation Dataset",
                "url": "https://huggingface.co/papers/2506.18851",
                "type": ""
            }
        ],
        "original_source": "huggingface"
    },
    {
        "id": "af47b35389641a370a92613eb99f9e0a",
        "source": "bytedance_seed",
        "type": "blog",
        "title": "Seed-1.6-Embedding Launched: A Powerful Embedding Model Built on Seed1.6-Flash",
        "description": "It stands out with Multimodal Hybrid Retrieval, SOTA Performance and Flexibility.",
        "url": "https://seed.bytedance.com/en/blog/seed-1-6-embedding-launched-a-powerful-embedding-model-built-on-seed1-6-flash",
        "published_date": "2025-06-27T11:00:00+00:00",
        "categories": [
            "Technology Launch"
        ],
        "organization": "ByteDance Seed",
        "metadata": {},
        "objects": [],
        "title_localized": {
            "en": "Seed-1.6-Embedding Launched: A Powerful Embedding Model Built on Seed1.6-Flash",
            "zh": "Seed-1.6-Embedding\uff1a\u57fa\u4e8eSeed1.6-Flash\u6784\u5efa\u7684\u591a\u6a21\u6001\u5411\u91cf\u5316\u6a21\u578b"
        },
        "description_localized": {
            "en": "It stands out with Multimodal Hybrid Retrieval, SOTA Performance and Flexibility.",
            "zh": "\u652f\u6301\u591a\u6a21\u6001\u6df7\u5408\u68c0\u7d22"
        },
        "url_localized": {
            "en": "https://seed.bytedance.com/en/blog/seed-1-6-embedding-launched-a-powerful-embedding-model-built-on-seed1-6-flash",
            "zh": "https://seed.bytedance.com/zh/blog/seed-1-6-embedding-\u57fa\u4e8eseed1-6-flash\u6784\u5efa\u7684\u591a\u6a21\u6001\u5411\u91cf\u5316\u6a21\u578b"
        }
    },
    {
        "id": "cf906a1d9f5e74e50d4f1c3ca44c6a61",
        "source": "bytedance_seed",
        "type": "blog",
        "title": "Introduction to Techniques Used in Seed1.6",
        "description": "Adaptive Thinking Powered by Multimodal Capabilities (Including college entrance examination  Scores)",
        "url": "https://seed.bytedance.com/en/blog/introduction-to-techniques-used-in-seed1-6",
        "published_date": "2025-06-24T11:00:00+00:00",
        "categories": [
            "Technology Launch"
        ],
        "organization": "ByteDance Seed",
        "metadata": {},
        "objects": [],
        "title_localized": {
            "en": "Introduction to Techniques Used in Seed1.6",
            "zh": "Seed1.6 \u7cfb\u5217\u6a21\u578b\u6280\u672f\u4ecb\u7ecd"
        },
        "description_localized": {
            "en": "Adaptive Thinking Powered by Multimodal Capabilities (Including college entrance examination  Scores)",
            "zh": "\u878d\u5408\u591a\u6a21\u6001\uff0c\u53ef\u201c\u81ea\u9002\u5e94\u601d\u8003\u201d\uff08\u542b\u9ad8\u8003\u5206\u6570\uff09"
        },
        "url_localized": {
            "en": "https://seed.bytedance.com/en/blog/introduction-to-techniques-used-in-seed1-6",
            "zh": "https://seed.bytedance.com/zh/blog/seed1-6-\u7cfb\u5217\u6a21\u578b\u6280\u672f\u4ecb\u7ecd"
        }
    },
    {
        "id": "5fba19d12ee386a07ea3d76ec8830ddf",
        "source": "bytedance_seed",
        "type": "blog",
        "title": "ByteDance Seed and BYD Lithium Battery Forge Partnership: AI Joint Laboratory to Accelerate Battery R&D",
        "description": "AI for Science + High-Throughput Experimentation: Tackling Critical Challenges in Power Batteries",
        "url": "https://seed.bytedance.com/en/blog/bytedance-seed-and-byd-lithium-battery-forge-partnership-ai-joint-laboratory-to-accelerate-battery-r-d",
        "published_date": "2025-06-17T11:00:00+00:00",
        "categories": [
            "Academic cooperation"
        ],
        "organization": "ByteDance Seed",
        "metadata": {},
        "objects": [],
        "title_localized": {
            "en": "ByteDance Seed and BYD Lithium Battery Forge Partnership: AI Joint Laboratory to Accelerate Battery R&D",
            "zh": "\u5b57\u8282\u8df3\u52a8Seed\u4e0e\u6bd4\u4e9a\u8fea\u9502\u7535\u6c60\u6df1\u5316\u5408\u4f5c\uff1a\u5c06\u6210\u7acbAI\u8054\u5408\u5b9e\u9a8c\u5ba4\u52a0\u901f\u7535\u6c60\u7814\u53d1"
        },
        "description_localized": {
            "en": "AI for Science + High-Throughput Experimentation: Tackling Critical Challenges in Power Batteries",
            "zh": "AI for Science + \u9ad8\u901a\u91cf\u5b9e\u9a8c\uff0c\u653b\u5173\u52a8\u529b\u7535\u6c60\u5173\u952e\u95ee\u9898"
        },
        "url_localized": {
            "en": "https://seed.bytedance.com/en/blog/bytedance-seed-and-byd-lithium-battery-forge-partnership-ai-joint-laboratory-to-accelerate-battery-r-d",
            "zh": "https://seed.bytedance.com/zh/blog/\u5b57\u8282\u8df3\u52a8seed\u4e0e\u6bd4\u4e9a\u8fea\u9502\u7535\u6c60\u6df1\u5316\u5408\u4f5c-\u5c06\u6210\u7acbai\u8054\u5408\u5b9e\u9a8c\u5ba4\u52a0\u901f\u7535\u6c60\u7814\u53d1"
        }
    },
    {
        "id": "b300005dd642e1d107c0f5161d3f598a",
        "source": "bytedance_seed",
        "type": "research",
        "title": "Seedance 1.0: Exploring the Boundaries of Video Generation Models",
        "description": "Notable advances in diffusion modeling have propelled rapid improvements in video generation, yet current foundational model still confront critical challenges in synergistically balancing prompt following, motion plausibility, and visual quality. In this report, we introduce Seedance 1.0, a high-performance and inference-efficient video foundation generation model that integrates several core technical improvements: (i) multi-source data curation augmented with precision and meaningful video captioning, enabling comprehensive learning across diverse scenarios; (ii) an efficient pre-training paradigm that enables multiple features or functions such as interleaved multimodal positional encoding, native multi-shot generation capacity, and multi-task modeling; (iii) carefully-designed post-training optimization leveraging fine-grained supervised fine-tuning, video-specific RLHF with multi-dimensional reward mechanisms for considerable performance improvements; (iv) excellent model acceleration achieving 10\u00d7 inference speedup through multi- stage distillation strategies and system-level optimizations. Seedance 1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds. Compared to state-of-the-art video generation models, Seedance 1.0 stands out with high-quality and fast video generation with superior spatiotemporal fluidity with structural stability, precise instruction adherence in complex multi-subject contexts, native multi-shot narrative coherence with consistent subject representation, and ultra-fast inference.",
        "url": "https://seed.bytedance.com/en/research/seedance-1-0-exploring-the-boundaries-of-video-generation-models",
        "published_date": "2025-06-10T22:08:06+00:00",
        "categories": [
            "Computer Vision"
        ],
        "organization": "ByteDance Seed",
        "metadata": {
            "author": "Seed Vision Team",
            "journal": "arXiv",
            "work_team": [
                "Vision"
            ]
        },
        "objects": [],
        "title_localized": {
            "en": "Seedance 1.0: Exploring the Boundaries of Video Generation Models",
            "zh": ""
        },
        "description_localized": {
            "en": "Notable advances in diffusion modeling have propelled rapid improvements in video generation, yet current foundational model still confront critical challenges in synergistically balancing prompt following, motion plausibility, and visual quality. In this report, we introduce Seedance 1.0, a high-performance and inference-efficient video foundation generation model that integrates several core technical improvements: (i) multi-source data curation augmented with precision and meaningful video captioning, enabling comprehensive learning across diverse scenarios; (ii) an efficient pre-training paradigm that enables multiple features or functions such as interleaved multimodal positional encoding, native multi-shot generation capacity, and multi-task modeling; (iii) carefully-designed post-training optimization leveraging fine-grained supervised fine-tuning, video-specific RLHF with multi-dimensional reward mechanisms for considerable performance improvements; (iv) excellent model acceleration achieving 10\u00d7 inference speedup through multi- stage distillation strategies and system-level optimizations. Seedance 1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds. Compared to state-of-the-art video generation models, Seedance 1.0 stands out with high-quality and fast video generation with superior spatiotemporal fluidity with structural stability, precise instruction adherence in complex multi-subject contexts, native multi-shot narrative coherence with consistent subject representation, and ultra-fast inference.",
            "zh": ""
        },
        "url_localized": {
            "en": "https://seed.bytedance.com/en/research/seedance-1-0-exploring-the-boundaries-of-video-generation-models",
            "zh": "https://seed.bytedance.com/zh/research/"
        },
        "external_url": "https://arxiv.org/pdf/2506.09113"
    },
    {
        "id": "1aeb0a0ffb9726c0f663a4fa2cc41362",
        "source": "bytedance_seed",
        "type": "blog",
        "title": "Tech Report of Seedance 1.0 Is Now Publicly Available",
        "description": "Tech Report of Seedance 1.0 Is Now Publicly Available",
        "url": "https://seed.bytedance.com/en/blog/tech-report-of-seedance-1-0-is-now-publicly-available",
        "published_date": "2025-06-10T11:00:00+00:00",
        "categories": [
            "Technology Launch"
        ],
        "organization": "ByteDance Seed",
        "metadata": {},
        "objects": [],
        "title_localized": {
            "en": "Tech Report of Seedance 1.0 Is Now Publicly Available",
            "zh": "Seedance 1.0 \u89c6\u9891\u751f\u6210\u6a21\u578b\u6280\u672f\u62a5\u544a\u516c\u5f00"
        },
        "description_localized": {
            "en": "Tech Report of Seedance 1.0 Is Now Publicly Available",
            "zh": "\u539f\u751f\u652f\u63012-3\u4e2a\u4ee5\u4e0a\u955c\u5934\u751f\u6210\uff0c\u4e00\u5206\u949f\u5feb\u901f\u51fa\u7247"
        },
        "url_localized": {
            "en": "https://seed.bytedance.com/en/blog/tech-report-of-seedance-1-0-is-now-publicly-available",
            "zh": "https://seed.bytedance.com/zh/blog/seedance-1-0-\u89c6\u9891\u751f\u6210\u6a21\u578b\u6280\u672f\u62a5\u544a\u516c\u5f00"
        }
    },
    {
        "id": "4f9a4ddff877b4ded1a4a89bc4b28a6a",
        "source": "bytedance_seed",
        "type": "blog",
        "title": "Meet Seed at CVPR 2025: 12 Papers Accepted and 2 Talks",
        "description": "Welcome to our booth",
        "url": "https://seed.bytedance.com/en/blog/meet-seed-at-cvpr-2025-12-papers-accepted-and-2-talks",
        "published_date": "2025-06-09T11:00:00+00:00",
        "categories": [
            "Academic Collaboration and Events"
        ],
        "organization": "ByteDance Seed",
        "metadata": {},
        "objects": [],
        "title_localized": {
            "en": "Meet Seed at CVPR 2025: 12 Papers Accepted and 2 Talks",
            "zh": "12\u7bc7\u6210\u679c\u5165\u9009\uff0c2\u573aTalk\uff0c\u5b57\u8282\u8df3\u52a8Seed\u9080\u4f60\u76f8\u805aCVPR 2025"
        },
        "description_localized": {
            "en": "Welcome to our booth",
            "zh": "\u6b22\u8fce\u73b0\u573a\u4ea4\u6d41"
        },
        "url_localized": {
            "en": "https://seed.bytedance.com/en/blog/meet-seed-at-cvpr-2025-12-papers-accepted-and-2-talks",
            "zh": "https://seed.bytedance.com/zh/blog/12\u7bc7\u6210\u679c\u5165\u9009-2\u573atalk-\u5b57\u8282\u8df3\u52a8seed\u9080\u4f60\u76f8\u805acvpr-2025"
        }
    },
    {
        "id": "e78233fe17cd98ee5020a08a39dad37d",
        "source": "bytedance_seed",
        "type": "blog",
        "title": "ByteDance Released Image Editing Model SeedEdit 3.0: Enhanced Image Consistency and Usability Rate",
        "description": "This model is now open for testing on the web page of Jimeng and will be applied in the Doubao app soon. ",
        "url": "https://seed.bytedance.com/en/blog/bytedance-released-image-editing-model-seededit-3-0-enhanced-image-consistency-and-usability-rate",
        "published_date": "2025-06-05T11:00:00+00:00",
        "categories": [
            "Technology Launch"
        ],
        "organization": "ByteDance Seed",
        "metadata": {},
        "objects": [],
        "title_localized": {
            "en": "ByteDance Released Image Editing Model SeedEdit 3.0: Enhanced Image Consistency and Usability Rate",
            "zh": "\u56fe\u50cf\u7f16\u8f91\u6a21\u578bSeedEdit 3.0\u53d1\u5e03\uff01\u66f4\u5f3a\u4fdd\u6301\u529b\uff0c\u66f4\u9ad8\u53ef\u7528\u7387"
        },
        "description_localized": {
            "en": "This model is now open for testing on the web page of Jimeng and will be applied in the Doubao app soon. ",
            "zh": "\u5df2\u5728\u5373\u68a6\u7f51\u9875\u7aef\u5f00\u542f\u6d4b\u8bd5\uff0c\u8c46\u5305App\u5373\u5c06\u4e0a\u7ebf"
        },
        "url_localized": {
            "en": "https://seed.bytedance.com/en/blog/bytedance-released-image-editing-model-seededit-3-0-enhanced-image-consistency-and-usability-rate",
            "zh": "https://seed.bytedance.com/zh/blog/\u56fe\u50cf\u7f16\u8f91\u6a21\u578bseededit-3-0\u53d1\u5e03-\u66f4\u5f3a\u4fdd\u6301\u529b-\u66f4\u9ad8\u53ef\u7528\u7387"
        }
    },
    {
        "id": "e1aa581108c4694a9e6091df5ebba6fb",
        "source": "bytedance_seed",
        "type": "research",
        "title": "SeedEdit 3.0: Fast and High-Quality Generative Image Editing",
        "description": "We introduce SeedEdit 3.0, in companion with our T2I model Seedream 3.0, which significantly improves over our previous SeedEdit versions in both aspects of edit instruction following and image content (e.g., ID/IP) preservation on real image inputs. Additional to model upgrading with T2I, in this report, we present several key improvements. First, we develop an enhanced data curation pipeline with a meta-info paradigm and meta-info embedding strategy that help mix images from multiple data sources. This allows us to scale editing data effectively, and meta information is helpfult to connect VLM with diffusion model more closely. Second, we introduce a joint learning pipeline for computing a diffusion loss and reward losses. Finally, we evaluate SeedEdit 3.0 on our testing benchmarks, for real/synthetic image editing, where it achieves a best trade-off between multiple aspects, yielding a high usability rate of 56.1%, compared to SeedEdit 1.6 (38.4%), GPT4o (37.1%) and Gemini 2.0 (30.3%).",
        "url": "https://seed.bytedance.com/en/research/seededit-3-0-fast-and-high-quality-generative-image-editing",
        "published_date": "2025-06-04T11:00:00+00:00",
        "categories": [
            "Computer Vision"
        ],
        "organization": "ByteDance Seed",
        "metadata": {
            "author": "Peng Wang, Yichun Shi, Xiaochen Lian, Zhonghua Zhai, Xin Xia, Xuefeng Xiao, Weilin Huang, Jianchao Yang",
            "journal": "arXiv",
            "work_team": [
                "Vision"
            ]
        },
        "objects": [],
        "title_localized": {
            "en": "SeedEdit 3.0: Fast and High-Quality Generative Image Editing",
            "zh": ""
        },
        "description_localized": {
            "en": "We introduce SeedEdit 3.0, in companion with our T2I model Seedream 3.0, which significantly improves over our previous SeedEdit versions in both aspects of edit instruction following and image content (e.g., ID/IP) preservation on real image inputs. Additional to model upgrading with T2I, in this report, we present several key improvements. First, we develop an enhanced data curation pipeline with a meta-info paradigm and meta-info embedding strategy that help mix images from multiple data sources. This allows us to scale editing data effectively, and meta information is helpfult to connect VLM with diffusion model more closely. Second, we introduce a joint learning pipeline for computing a diffusion loss and reward losses. Finally, we evaluate SeedEdit 3.0 on our testing benchmarks, for real/synthetic image editing, where it achieves a best trade-off between multiple aspects, yielding a high usability rate of 56.1%, compared to SeedEdit 1.6 (38.4%), GPT4o (37.1%) and Gemini 2.0 (30.3%).",
            "zh": ""
        },
        "url_localized": {
            "en": "https://seed.bytedance.com/en/research/seededit-3-0-fast-and-high-quality-generative-image-editing",
            "zh": "https://seed.bytedance.com/zh/research/"
        },
        "external_url": "https://arxiv.org/abs/2506.05083"
    },
    {
        "id": "876e03ad4d0a6b7d8fe74f0a3619e6ce",
        "source": "bytedance_aggregated",
        "type": "activity",
        "title": "[HuggingFace] updated a dataset",
        "description": "huggingface_ByteDance: updated a dataset",
        "url": "https://huggingface.co/huggingface_ByteDance",
        "published_date": "2025-05-28T18:25:47.272959+00:00",
        "categories": [],
        "organization": "huggingface_ByteDance",
        "metadata": {
            "action": "updated a dataset"
        },
        "objects": [
            {
                "title": "ByteDance/WildDoc",
                "url": "https://huggingface.co/datasets/ByteDance/WildDoc",
                "type": ""
            }
        ],
        "original_source": "huggingface"
    },
    {
        "id": "ba597ae40e4958c9ce45d8ae497ebc82",
        "source": "bytedance_aggregated",
        "type": "activity",
        "title": "[HuggingFace] updated a dataset",
        "description": "huggingface_bytedanceresearch: updated a dataset",
        "url": "https://huggingface.co/huggingface_bytedanceresearch",
        "published_date": "2025-05-28T18:25:47.272959+00:00",
        "categories": [],
        "organization": "huggingface_bytedanceresearch",
        "metadata": {
            "action": "updated a dataset"
        },
        "objects": [
            {
                "title": "bytedance-research/ToolHop",
                "url": "https://huggingface.co/datasets/bytedance-research/ToolHop",
                "type": ""
            }
        ],
        "original_source": "huggingface"
    },
    {
        "id": "aa7a071d14a401c630b52e5e03094fdc",
        "source": "bytedance_aggregated",
        "type": "activity",
        "title": "[HuggingFace] authored 2 papers",
        "description": "huggingface_ByteDance: authored 2 papers",
        "url": "https://huggingface.co/huggingface_ByteDance",
        "published_date": "2025-05-28T18:25:47.272959+00:00",
        "categories": [],
        "organization": "huggingface_ByteDance",
        "metadata": {
            "action": "authored 2 papers"
        },
        "objects": [
            {
                "title": "JarvisIR: Elevating Autonomous Driving Perception with Intelligent Image Restoration",
                "url": "https://huggingface.co/papers/2504.04158",
                "type": ""
            },
            {
                "title": "PartCrafter: Structured 3D Mesh Generation via Compositional Latent Diffusion Transformers",
                "url": "https://huggingface.co/papers/2506.05573",
                "type": ""
            }
        ],
        "original_source": "huggingface"
    },
    {
        "id": "88ec5e0064edcd0c14d8dd6f4ebd0d18",
        "source": "bytedance_aggregated",
        "type": "activity",
        "title": "[HuggingFace] updated 2 datasets",
        "description": "huggingface_ByteDanceSeed: updated 2 datasets",
        "url": "https://huggingface.co/huggingface_ByteDanceSeed",
        "published_date": "2025-05-28T18:25:47.272959+00:00",
        "categories": [],
        "organization": "huggingface_ByteDanceSeed",
        "metadata": {
            "action": "updated 2 datasets"
        },
        "objects": [
            {
                "title": "ByteDance-Seed/BM-6M-Demo",
                "url": "https://huggingface.co/datasets/ByteDance-Seed/BM-6M-Demo",
                "type": ""
            },
            {
                "title": "ByteDance-Seed/BM-Bench",
                "url": "https://huggingface.co/datasets/ByteDance-Seed/BM-Bench",
                "type": ""
            }
        ],
        "original_source": "huggingface"
    },
    {
        "id": "a43fed2c16dfbb4534a6753f3c928c2a",
        "source": "bytedance_aggregated",
        "type": "activity",
        "title": "[HuggingFace] updated 2 models",
        "description": "huggingface_ByteDance: updated 2 models",
        "url": "https://huggingface.co/huggingface_ByteDance",
        "published_date": "2025-05-28T18:25:47.272959+00:00",
        "categories": [],
        "organization": "huggingface_ByteDance",
        "metadata": {
            "action": "updated 2 models"
        },
        "objects": [
            {
                "title": "ByteDance/LatentSync-1.5",
                "url": "https://huggingface.co/ByteDance/LatentSync-1.5",
                "type": "UpdatedJun 12"
            },
            {
                "title": "ByteDance/LatentSync-1.6",
                "url": "https://huggingface.co/ByteDance/LatentSync-1.6",
                "type": "UpdatedJun 12"
            }
        ],
        "original_source": "huggingface"
    },
    {
        "id": "9c12d8004c9aa28de95c3c80a144a88c",
        "source": "bytedance_aggregated",
        "type": "activity",
        "title": "[HuggingFace] updated 3 datasets",
        "description": "huggingface_ByteDanceSeed: updated 3 datasets",
        "url": "https://huggingface.co/huggingface_ByteDanceSeed",
        "published_date": "2025-05-28T18:25:47.272959+00:00",
        "categories": [],
        "organization": "huggingface_ByteDanceSeed",
        "metadata": {
            "action": "updated 3 datasets"
        },
        "objects": [
            {
                "title": "ByteDance-Seed/BM-6M-Demo",
                "url": "https://huggingface.co/datasets/ByteDance-Seed/BM-6M-Demo",
                "type": ""
            },
            {
                "title": "ByteDance-Seed/BM-6M",
                "url": "https://huggingface.co/datasets/ByteDance-Seed/BM-6M",
                "type": ""
            },
            {
                "title": "ByteDance-Seed/BM-Bench",
                "url": "https://huggingface.co/datasets/ByteDance-Seed/BM-Bench",
                "type": ""
            }
        ],
        "original_source": "huggingface"
    },
    {
        "id": "2c82469ea190f081d09b752b8223cc24",
        "source": "bytedance_aggregated",
        "type": "activity",
        "title": "[HuggingFace] published a dataset",
        "description": "huggingface_ByteDance: published a dataset",
        "url": "https://huggingface.co/huggingface_ByteDance",
        "published_date": "2025-05-28T18:25:47.272959+00:00",
        "categories": [],
        "organization": "huggingface_ByteDance",
        "metadata": {
            "action": "published a dataset"
        },
        "objects": [
            {
                "title": "ByteDance/BandwidthEstimationDataset",
                "url": "https://huggingface.co/datasets/ByteDance/BandwidthEstimationDataset",
                "type": ""
            }
        ],
        "original_source": "huggingface"
    },
    {
        "id": "dede4c2414d0f2503c6b514106668921",
        "source": "bytedance_aggregated",
        "type": "activity",
        "title": "[HuggingFace] published a model",
        "description": "huggingface_bytedanceresearch: published a model",
        "url": "https://huggingface.co/huggingface_bytedanceresearch",
        "published_date": "2025-05-28T18:25:47.272959+00:00",
        "categories": [],
        "organization": "huggingface_bytedanceresearch",
        "metadata": {
            "action": "published a model"
        },
        "objects": [
            {
                "title": "bytedance-research/ATI",
                "url": "https://huggingface.co/bytedance-research/ATI",
                "type": "Image-to-Video"
            }
        ],
        "original_source": "huggingface"
    },
    {
        "id": "9149a61c01688e438aa7631129a9cf21",
        "source": "bytedance_aggregated",
        "type": "activity",
        "title": "[HuggingFace] updated 2 models",
        "description": "huggingface_bytedanceresearch: updated 2 models",
        "url": "https://huggingface.co/huggingface_bytedanceresearch",
        "published_date": "2025-05-28T18:25:47.272959+00:00",
        "categories": [],
        "organization": "huggingface_bytedanceresearch",
        "metadata": {
            "action": "updated 2 models"
        },
        "objects": [
            {
                "title": "bytedance-research/Valley2-DPO",
                "url": "https://huggingface.co/bytedance-research/Valley2-DPO",
                "type": "9B"
            },
            {
                "title": "bytedance-research/Valley-Eagle-7B",
                "url": "https://huggingface.co/bytedance-research/Valley-Eagle-7B",
                "type": "9B"
            }
        ],
        "original_source": "huggingface"
    },
    {
        "id": "f7a8f1e4587001227817701885f47a46",
        "source": "bytedance_seed",
        "type": "blog",
        "title": "Seed Research\uff5cBAGEL: The Open-Source Unified Multimodal Model - An All-in-One Model!",
        "description": "Supports text, images, videos, and multimodal interleaving.",
        "url": "https://seed.bytedance.com/en/blog/seed-research-bagel-the-open-source-unified-multimodal-model-an-all-in-one-model",
        "published_date": "2025-05-27T11:00:00+00:00",
        "categories": [
            "Technology Launch"
        ],
        "organization": "ByteDance Seed",
        "metadata": {},
        "objects": [],
        "title_localized": {
            "en": "Seed Research\uff5cBAGEL: The Open-Source Unified Multimodal Model - An All-in-One Model!",
            "zh": "Seed Research\uff5c\u7406\u89e3\u4e0e\u751f\u6210\u7edf\u4e00\u6a21\u578b BAGEL \u5f00\u6e90\uff0cAll-in-One Model\uff01"
        },
        "description_localized": {
            "en": "Supports text, images, videos, and multimodal interleaving.",
            "zh": "\u652f\u6301\u6587\u672c\u3001\u56fe\u50cf\u3001\u89c6\u9891\u53ca\u591a\u6a21\u6001\u4ea4\u9519"
        },
        "url_localized": {
            "en": "https://seed.bytedance.com/en/blog/seed-research-bagel-the-open-source-unified-multimodal-model-an-all-in-one-model",
            "zh": "https://seed.bytedance.com/zh/blog/seed-research-\u7406\u89e3\u4e0e\u751f\u6210\u7edf\u4e00\u6a21\u578b-bagel-\u5f00\u6e90-all-in-one-model"
        }
    },
    {
        "id": "a017383bd0d40d10b4ca9bf35b20b583",
        "source": "bytedance_seed",
        "type": "research",
        "title": "MMaDA: Multimodal Large Diffusion Language Models",
        "description": "We introduce MMaDA, a novel class of multimodal diffusion foundation models designed to achieve superior performance across diverse domains such as textual reasoning, multimodal understanding, and text-to-image generation. The approach is distinguished by three key innovations: (i) MMaDA adopts a unified diffusion architecture with a shared probabilistic formulation and a modality-agnostic design, eliminating the need for modality-specific components. This architecture ensures seamless integration and processing across different data types. (ii) We implement a mixed long chain-of-thought (CoT) fine-tuning strategy that curates a unified CoT format across modalities. By aligning reasoning processes between textual and visual domains, this strategy facilitates cold-start training for the final reinforcement learning (RL) stage, thereby enhancing the model's ability to handle complex tasks from the outset. (iii) We propose UniGRPO, a unified policy-gradient-based RL algorithm specifically tailored for diffusion foundation models. Utilizing diversified reward modeling, UniGRPO unifies post-training across both reasoning and generation tasks, ensuring consistent performance improvements. Experimental results demonstrate that MMaDA-8B exhibits strong generalization capabilities as a unified multimodal foundation model. It surpasses powerful models like LLaMA-3-7B and Qwen2-7B in textual reasoning, outperforms Show-o and SEED-X in multimodal understanding, and excels over SDXL and Janus in text-to-image generation. These achievements highlight MMaDA's effectiveness in bridging the gap between pretraining and post-training within unified diffusion architectures, providing a comprehensive framework for future research and development. We open-source our code and trained models at:\nhttps://github.com/Gen-Verse/MMaDA",
        "url": "https://seed.bytedance.com/en/research/mmada-multimodal-large-diffusion-language-models",
        "published_date": "2025-05-20T11:00:00+00:00",
        "categories": [
            "Computer Vision"
        ],
        "organization": "ByteDance Seed",
        "metadata": {
            "author": "Ling Yang, Ye Tian, Bowen Li, Xinchen Zhang, Ke Shen, Yunhai Tong, Mengdi Wang",
            "journal": "arXiv",
            "work_team": [
                "LLM"
            ]
        },
        "objects": [],
        "title_localized": {
            "en": "MMaDA: Multimodal Large Diffusion Language Models",
            "zh": ""
        },
        "description_localized": {
            "en": "We introduce MMaDA, a novel class of multimodal diffusion foundation models designed to achieve superior performance across diverse domains such as textual reasoning, multimodal understanding, and text-to-image generation. The approach is distinguished by three key innovations: (i) MMaDA adopts a unified diffusion architecture with a shared probabilistic formulation and a modality-agnostic design, eliminating the need for modality-specific components. This architecture ensures seamless integration and processing across different data types. (ii) We implement a mixed long chain-of-thought (CoT) fine-tuning strategy that curates a unified CoT format across modalities. By aligning reasoning processes between textual and visual domains, this strategy facilitates cold-start training for the final reinforcement learning (RL) stage, thereby enhancing the model's ability to handle complex tasks from the outset. (iii) We propose UniGRPO, a unified policy-gradient-based RL algorithm specifically tailored for diffusion foundation models. Utilizing diversified reward modeling, UniGRPO unifies post-training across both reasoning and generation tasks, ensuring consistent performance improvements. Experimental results demonstrate that MMaDA-8B exhibits strong generalization capabilities as a unified multimodal foundation model. It surpasses powerful models like LLaMA-3-7B and Qwen2-7B in textual reasoning, outperforms Show-o and SEED-X in multimodal understanding, and excels over SDXL and Janus in text-to-image generation. These achievements highlight MMaDA's effectiveness in bridging the gap between pretraining and post-training within unified diffusion architectures, providing a comprehensive framework for future research and development. We open-source our code and trained models at:\nhttps://github.com/Gen-Verse/MMaDA",
            "zh": ""
        },
        "url_localized": {
            "en": "https://seed.bytedance.com/en/research/mmada-multimodal-large-diffusion-language-models",
            "zh": "https://seed.bytedance.com/zh/research/"
        },
        "external_url": "https://arxiv.org/abs/2505.15809"
    },
    {
        "id": "3dea7fdfb8c70c4459a640bab80a198b",
        "source": "bytedance_seed",
        "type": "blog",
        "title": "Seed-Coder Released: A Powerful Family of Open-Source Code Models for Curating Code Data Based on LLMs",
        "description": "Dataflow Implementation Released, BF16 Precision Version Now Available for Download.",
        "url": "https://seed.bytedance.com/en/blog/seed-coder-released-a-powerful-family-of-open-source-code-models-for-curating-code-data-based-on-llms",
        "published_date": "2025-05-18T11:00:00+00:00",
        "categories": [
            "Technology Launch"
        ],
        "organization": "ByteDance Seed",
        "metadata": {},
        "objects": [],
        "title_localized": {
            "en": "Seed-Coder Released: A Powerful Family of Open-Source Code Models for Curating Code Data Based on LLMs",
            "zh": "Seed-Coder\u5f00\u6e90\u4ee3\u7801\u6a21\u578b\u53d1\u5e03\uff0c\u4f9d\u6258LLM\u6784\u5efa\u4ee3\u7801\u6570\u636e"
        },
        "description_localized": {
            "en": "Dataflow Implementation Released, BF16 Precision Version Now Available for Download.",
            "zh": "\u6570\u636e\u6d41\u5b9e\u73b0\u65b9\u6cd5\u516c\u5f00\uff0cBF16\u7cbe\u5ea6\u7248\u672c\u5f00\u653e\u4e0b\u8f7d\u3002"
        },
        "url_localized": {
            "en": "https://seed.bytedance.com/en/blog/seed-coder-released-a-powerful-family-of-open-source-code-models-for-curating-code-data-based-on-llms",
            "zh": "https://seed.bytedance.com/zh/blog/seed-coder\u5f00\u6e90\u4ee3\u7801\u6a21\u578b\u53d1\u5e03-\u4f9d\u6258llm\u6784\u5efa\u4ee3\u7801\u6570\u636e"
        }
    },
    {
        "id": "1c61b776ea7f2d751188a46329c0224c",
        "source": "bytedance_seed",
        "type": "research",
        "title": "Model Merging in Pre-training of Large Language Models",
        "description": "Model merging has emerged as a promising technique for enhancing large language models, though its application in large-scale pre-training remains relatively unexplored. In this paper, we present a comprehensive investigation of model merging techniques during the pre-training process. Through extensive experiments with both dense and Mixture-of-Experts (MoE) architectures ranging from millions to over 100 billion parameters, we demonstrate that merging checkpoints trained with constant learning rates not only achieves significant performance improvements but also enables accurate prediction of annealing behavior. These improvements lead to both more efficient model development and significantly lower training costs. Our detailed ablation studies on merging strategies and hyperparameters provide new insights into the underlying mechanisms while uncovering novel applications. Through comprehensive experimental analysis, we offer the open-source community practical pre-training guidelines for effective model merging.",
        "url": "https://seed.bytedance.com/en/research/model-merging-in-pre-training-of-large-language-models",
        "published_date": "2025-05-16T11:00:00+00:00",
        "categories": [
            "LLM"
        ],
        "organization": "ByteDance Seed",
        "metadata": {
            "author": "Yunshui Li, Yiyuan Ma, Shen Yan, Chaoyi Zhang, Jing Liu, Jianqiao Lu, Ziwen Xu, Mengzhao Chen, Minrui Wang, Shiyi Zhan, Jin Ma, Xunhao Lai, Deyi Liu, Yao Luo, Xingyan Bin, Hongbin Ren, Mingji Han, Wenhao Hao, Bairen Yi, LingJun Liu, Bole Ma, Xiaoying Jia, Xun Zhou, Siyuan Qiao, Liang Xiang, Yonghui Wu",
            "journal": "arXiv",
            "work_team": [
                "LLM"
            ]
        },
        "objects": [],
        "title_localized": {
            "en": "Model Merging in Pre-training of Large Language Models",
            "zh": ""
        },
        "description_localized": {
            "en": "Model merging has emerged as a promising technique for enhancing large language models, though its application in large-scale pre-training remains relatively unexplored. In this paper, we present a comprehensive investigation of model merging techniques during the pre-training process. Through extensive experiments with both dense and Mixture-of-Experts (MoE) architectures ranging from millions to over 100 billion parameters, we demonstrate that merging checkpoints trained with constant learning rates not only achieves significant performance improvements but also enables accurate prediction of annealing behavior. These improvements lead to both more efficient model development and significantly lower training costs. Our detailed ablation studies on merging strategies and hyperparameters provide new insights into the underlying mechanisms while uncovering novel applications. Through comprehensive experimental analysis, we offer the open-source community practical pre-training guidelines for effective model merging.",
            "zh": ""
        },
        "url_localized": {
            "en": "https://seed.bytedance.com/en/research/model-merging-in-pre-training-of-large-language-models",
            "zh": "https://seed.bytedance.com/zh/research/"
        },
        "external_url": "https://arxiv.org/abs/2505.12082"
    },
    {
        "id": "560133989e4b493c5e6e6a0ccf076fd6",
        "source": "bytedance_seed",
        "type": "research",
        "title": "Seed1.5-VL Technical Report",
        "description": "We present Seed1.5-VL, a vision-language foundation model designed to advance general-purpose multimodal understanding and reasoning. Seed1.5-VL is composed with a 532M-parameter vision encoder and a Mixture-of-Experts (MoE) LLM of 20B active parameters. Despite its relatively compact architecture, it delivers strong performance across a wide spectrum of public VLM benchmarks and internal evaluation suites, achieving the state-of-the-art performance on 38 out of 60 public benchmarks. Moreover, in agent-centric tasks such as GUI control and gameplay, Seed1.5-VL outperforms leading multimodal systems, including OpenAI CUA and Claude 3.7. Beyond visual and video understanding, it also demonstrates strong reasoning abilities, making it particularly effective for multimodal reasoning challenges such as visual puzzles. We believe these capabilities will empower broader applications across diverse tasks. In this report, we mainly provide a comprehensive review of our experiences in building Seed1.5-VL across model design, data construction, and training at various stages, hoping that this report can inspire further research. Seed1.5-VL is now accessible at this https URL (Volcano Engine Model ID: doubao-1-5-thinking-vision-pro-250428)",
        "url": "https://seed.bytedance.com/en/research/seed1-5-vl-technical-report",
        "published_date": "2025-05-12T11:00:00+00:00",
        "categories": [
            "LLM"
        ],
        "organization": "ByteDance Seed",
        "metadata": {
            "author": "Seed Multimodal Team",
            "journal": "arXiv",
            "work_team": [
                "Multimodal"
            ]
        },
        "objects": [],
        "title_localized": {
            "en": "Seed1.5-VL Technical Report",
            "zh": ""
        },
        "description_localized": {
            "en": "We present Seed1.5-VL, a vision-language foundation model designed to advance general-purpose multimodal understanding and reasoning. Seed1.5-VL is composed with a 532M-parameter vision encoder and a Mixture-of-Experts (MoE) LLM of 20B active parameters. Despite its relatively compact architecture, it delivers strong performance across a wide spectrum of public VLM benchmarks and internal evaluation suites, achieving the state-of-the-art performance on 38 out of 60 public benchmarks. Moreover, in agent-centric tasks such as GUI control and gameplay, Seed1.5-VL outperforms leading multimodal systems, including OpenAI CUA and Claude 3.7. Beyond visual and video understanding, it also demonstrates strong reasoning abilities, making it particularly effective for multimodal reasoning challenges such as visual puzzles. We believe these capabilities will empower broader applications across diverse tasks. In this report, we mainly provide a comprehensive review of our experiences in building Seed1.5-VL across model design, data construction, and training at various stages, hoping that this report can inspire further research. Seed1.5-VL is now accessible at this https URL (Volcano Engine Model ID: doubao-1-5-thinking-vision-pro-250428)",
            "zh": ""
        },
        "url_localized": {
            "en": "https://seed.bytedance.com/en/research/seed1-5-vl-technical-report",
            "zh": "https://seed.bytedance.com/zh/research/"
        },
        "external_url": "https://arxiv.org/abs/2505.07062"
    },
    {
        "id": "1e705737ff78d44f85a4abfc42e6ad38",
        "source": "bytedance_seed",
        "type": "research",
        "title": "Understanding Stragglers in Large Model Training Using What-if Analysis",
        "description": "Large language model (LLM) training is one of the most demanding distributed computations today, often requiring thousands of GPUs with frequent synchronization across machines. Such a workload pattern makes it susceptible to stragglers, where the training can be stalled by few slow workers. At ByteDance we find stragglers are not trivially always caused by hardware failures, but can arise from multiple complex factors. This work aims to present a comprehensive study on the straggler issues in LLM training, using a five-month trace collected from our ByteDance LLM training cluster. The core methodology is what-if analysis that simulates the scenario without any stragglers and contrasts with the actual case. We use this method to study the following questions: (1) how often do stragglers affect training jobs, and what effect do they have on job performance; (2) do stragglers exhibit temporal or spatial patterns; and (3) what are the potential root causes for stragglers?",
        "url": "https://seed.bytedance.com/en/research/understanding-stragglers-in-large-model-training-using-what-if-analysis",
        "published_date": "2025-05-08T11:00:00+00:00",
        "categories": [
            "Cluster Computing"
        ],
        "organization": "ByteDance Seed",
        "metadata": {
            "author": "Jinkun Lin, Ziheng Jiang, Zuquan Song, Sida Zhao, Menghan Yu, Zhanghan Wang, Chenyuan Wang, Zuocheng Shi, Xiang Shi, Wei Jia, Zherui Liu, Shuguang Wang, Haibin Lin, Xin Liu, Aurojit Panda, Jinyang Li",
            "journal": "OSDI 2025",
            "work_team": [
                "Infrastructures"
            ]
        },
        "objects": [],
        "title_localized": {
            "en": "Understanding Stragglers in Large Model Training Using What-if Analysis",
            "zh": ""
        },
        "description_localized": {
            "en": "Large language model (LLM) training is one of the most demanding distributed computations today, often requiring thousands of GPUs with frequent synchronization across machines. Such a workload pattern makes it susceptible to stragglers, where the training can be stalled by few slow workers. At ByteDance we find stragglers are not trivially always caused by hardware failures, but can arise from multiple complex factors. This work aims to present a comprehensive study on the straggler issues in LLM training, using a five-month trace collected from our ByteDance LLM training cluster. The core methodology is what-if analysis that simulates the scenario without any stragglers and contrasts with the actual case. We use this method to study the following questions: (1) how often do stragglers affect training jobs, and what effect do they have on job performance; (2) do stragglers exhibit temporal or spatial patterns; and (3) what are the potential root causes for stragglers?",
            "zh": ""
        },
        "url_localized": {
            "en": "https://seed.bytedance.com/en/research/understanding-stragglers-in-large-model-training-using-what-if-analysis",
            "zh": "https://seed.bytedance.com/zh/research/"
        },
        "external_url": "https://arxiv.org/abs/2505.05713"
    },
    {
        "id": "3151f14a89ccea3432d1931bf23f5b33",
        "source": "bytedance_aggregated",
        "type": "activity",
        "title": "[HuggingFace] authored 2 papers",
        "description": "huggingface_bytedanceresearch: authored 2 papers",
        "url": "https://huggingface.co/huggingface_bytedanceresearch",
        "published_date": "2025-04-28T18:25:47.272959+00:00",
        "categories": [],
        "organization": "huggingface_bytedanceresearch",
        "metadata": {
            "action": "authored 2 papers"
        },
        "objects": [
            {
                "title": "Measuring Data Diversity for Instruction Tuning: A Systematic Analysis and A Reliable Metric",
                "url": "https://huggingface.co/papers/2502.17184",
                "type": ""
            },
            {
                "title": "A Multi-Dimensional Constraint Framework for Evaluating and Improving Instruction Following in Large Language Models",
                "url": "https://huggingface.co/papers/2505.07591",
                "type": ""
            }
        ],
        "original_source": "huggingface"
    },
    {
        "id": "f0442fd307b46d910786c8227830a95c",
        "source": "bytedance_aggregated",
        "type": "activity",
        "title": "[HuggingFace] published a dataset",
        "description": "huggingface_bytedanceresearch: published a dataset",
        "url": "https://huggingface.co/huggingface_bytedanceresearch",
        "published_date": "2025-04-28T18:25:47.272959+00:00",
        "categories": [],
        "organization": "huggingface_bytedanceresearch",
        "metadata": {
            "action": "published a dataset"
        },
        "objects": [
            {
                "title": "bytedance-research/Web-Bench",
                "url": "https://huggingface.co/datasets/bytedance-research/Web-Bench",
                "type": ""
            }
        ],
        "original_source": "huggingface"
    },
    {
        "id": "354fe89d8447233763c54f5b1a16e3c3",
        "source": "bytedance_seed",
        "type": "research",
        "title": "Let the Code LLM Edit Itself When You Edit the Code",
        "description": "In this work, we investigate a typical scenario in code generation where a developer edits existing code in real time and requests a code assistant, e.g., a large language model, to re-predict the next token or next line on the fly. Naively, the LLM needs to re-encode the entire KV cache to provide an accurate prediction. However, this process is computationally expensive, especially when the sequence length is long. Simply encoding the edited subsequence and integrating it to the original KV cache meets the temporal confusion problem, leading to significantly worse performance. We address this efficiency and accuracy trade-off by introducing \\underline{\\textbf{Positional \\textbf{I}ntegrity \\textbf{E}ncoding} (PIE). Building upon the rotary positional encoding, PIE first removes the rotary matrices in the Key cache that introduce temporal confusion and then reapplies the correct rotary matrices. This process ensures that positional relationships between tokens are correct and requires only a single round of matrix multiplication. We validate the effectiveness of PIE through extensive experiments on the RepoBench-C-8k dataset, utilizing DeepSeek-Coder models with 1.3B, 6.7B, and 33B parameters. Our evaluation includes three real-world coding tasks: code insertion, code deletion, and multi-place code editing. Results demonstrate that PIE reduces computational overhead by over 85% compared to the standard full recomputation approach across all model sizes and tasks while well approximating the model performance.",
        "url": "https://seed.bytedance.com/en/research/let-the-code-llm-edit-itself-when-you-edit-the-code",
        "published_date": "2025-04-23T11:00:00+00:00",
        "categories": [
            "NLP"
        ],
        "organization": "ByteDance Seed",
        "metadata": {
            "author": "Zhenyu He, Jun Zhang, Shengjie Luo, Jingjing Xu, Zhi Zhang, Di He",
            "journal": "ICLR 2025",
            "work_team": [
                "Infrastructures"
            ]
        },
        "objects": [],
        "title_localized": {
            "en": "Let the Code LLM Edit Itself When You Edit the Code",
            "zh": ""
        },
        "description_localized": {
            "en": "In this work, we investigate a typical scenario in code generation where a developer edits existing code in real time and requests a code assistant, e.g., a large language model, to re-predict the next token or next line on the fly. Naively, the LLM needs to re-encode the entire KV cache to provide an accurate prediction. However, this process is computationally expensive, especially when the sequence length is long. Simply encoding the edited subsequence and integrating it to the original KV cache meets the temporal confusion problem, leading to significantly worse performance. We address this efficiency and accuracy trade-off by introducing \\underline{\\textbf{Positional \\textbf{I}ntegrity \\textbf{E}ncoding} (PIE). Building upon the rotary positional encoding, PIE first removes the rotary matrices in the Key cache that introduce temporal confusion and then reapplies the correct rotary matrices. This process ensures that positional relationships between tokens are correct and requires only a single round of matrix multiplication. We validate the effectiveness of PIE through extensive experiments on the RepoBench-C-8k dataset, utilizing DeepSeek-Coder models with 1.3B, 6.7B, and 33B parameters. Our evaluation includes three real-world coding tasks: code insertion, code deletion, and multi-place code editing. Results demonstrate that PIE reduces computational overhead by over 85% compared to the standard full recomputation approach across all model sizes and tasks while well approximating the model performance.",
            "zh": ""
        },
        "url_localized": {
            "en": "https://seed.bytedance.com/en/research/let-the-code-llm-edit-itself-when-you-edit-the-code",
            "zh": "https://seed.bytedance.com/zh/research/"
        },
        "external_url": "https://arxiv.org/abs/2407.03157"
    },
    {
        "id": "8a9a3139119a465e530694f273ecf7c8",
        "source": "bytedance_aggregated",
        "type": "activity",
        "title": "[HuggingFace] authored 5 papers",
        "description": "huggingface_bytedanceresearch: authored 5 papers",
        "url": "https://huggingface.co/huggingface_bytedanceresearch",
        "published_date": "2025-03-28T18:25:47.272959+00:00",
        "categories": [],
        "organization": "huggingface_bytedanceresearch",
        "metadata": {
            "action": "authored 5 papers"
        },
        "objects": [
            {
                "title": "DreamIdentity: Improved Editability for Efficient Face-identity Preserved Image Generation",
                "url": "https://huggingface.co/papers/2307.00300",
                "type": ""
            },
            {
                "title": "CustomContrast: A Multilevel Contrastive Perspective For Subject-Driven Text-to-Image Customization",
                "url": "https://huggingface.co/papers/2409.05606",
                "type": ""
            },
            {
                "title": "VMix: Improving Text-to-Image Diffusion Model with Cross-Attention Mixing Control",
                "url": "https://huggingface.co/papers/2412.20800",
                "type": ""
            },
            {
                "title": "RealCustom++: Representing Images as Real-Word for Real-Time Customization",
                "url": "https://huggingface.co/papers/2408.09744",
                "type": ""
            },
            {
                "title": "RealGeneral: Unifying Visual Generation via Temporal In-Context Learning with Video Models",
                "url": "https://huggingface.co/papers/2503.10406",
                "type": ""
            }
        ],
        "original_source": "huggingface"
    },
    {
        "id": "a7c3765c84426509780845c2593bac65",
        "source": "bytedance_aggregated",
        "type": "activity",
        "title": "[HuggingFace] published 2 datasets",
        "description": "huggingface_bytedanceresearch: published 2 datasets",
        "url": "https://huggingface.co/huggingface_bytedanceresearch",
        "published_date": "2025-03-28T18:25:47.272959+00:00",
        "categories": [],
        "organization": "huggingface_bytedanceresearch",
        "metadata": {
            "action": "published 2 datasets"
        },
        "objects": [
            {
                "title": "ByteDance-Seed/Multi-SWE-RL",
                "url": "https://huggingface.co/datasets/ByteDance-Seed/Multi-SWE-RL",
                "type": ""
            },
            {
                "title": "ByteDance-Seed/Multi-SWE-bench",
                "url": "https://huggingface.co/datasets/ByteDance-Seed/Multi-SWE-bench",
                "type": ""
            }
        ],
        "original_source": "huggingface"
    },
    {
        "id": "dd0a49d0aefc7d1d7c0f17b173d42e16",
        "source": "bytedance_seed",
        "type": "research",
        "title": "Hyper-Connections",
        "description": "We present hyper-connections, a simple yet effective method that can serve as an alternative to residual connections. This approach specifically addresses common drawbacks observed in residual connection variants, such as the seesaw effect between gradient vanishing and representation collapse. Theoretically, hyper-connections allow the network to adjust the strength of connections between features at different depths and dynamically rearrange layers. We conduct experiments focusing on the pre-training of large language models, including dense and sparse models, where hyper-connections show significant performance improvements over residual connections. Additional experiments conducted on vision tasks also demonstrate similar improvements. We anticipate that this method will be broadly applicable and beneficial across a wide range of AI problems.",
        "url": "https://seed.bytedance.com/en/research/hyper-connections",
        "published_date": "2025-03-17T11:00:00+00:00",
        "categories": [
            "LLM"
        ],
        "organization": "ByteDance Seed",
        "metadata": {
            "author": "Defa Zhu, Hongzhi Huang, Zihao Huang, Yutao Zeng, Yunyao Mao, Banggu Wu, Qiyang Min, Xun Zhou",
            "journal": "ICLR 2025",
            "work_team": [
                "LLM"
            ]
        },
        "objects": [],
        "title_localized": {
            "en": "Hyper-Connections",
            "zh": ""
        },
        "description_localized": {
            "en": "We present hyper-connections, a simple yet effective method that can serve as an alternative to residual connections. This approach specifically addresses common drawbacks observed in residual connection variants, such as the seesaw effect between gradient vanishing and representation collapse. Theoretically, hyper-connections allow the network to adjust the strength of connections between features at different depths and dynamically rearrange layers. We conduct experiments focusing on the pre-training of large language models, including dense and sparse models, where hyper-connections show significant performance improvements over residual connections. Additional experiments conducted on vision tasks also demonstrate similar improvements. We anticipate that this method will be broadly applicable and beneficial across a wide range of AI problems.",
            "zh": ""
        },
        "url_localized": {
            "en": "https://seed.bytedance.com/en/research/hyper-connections",
            "zh": "https://seed.bytedance.com/zh/research/"
        },
        "external_url": "https://arxiv.org/abs/2409.19606"
    },
    {
        "id": "04d2ddd4b9c43025dfdca8947128d1a6",
        "source": "bytedance_aggregated",
        "type": "activity",
        "title": "[HuggingFace] authored 3 papers",
        "description": "huggingface_ByteDanceSeed: authored 3 papers",
        "url": "https://huggingface.co/huggingface_ByteDanceSeed",
        "published_date": "2025-02-28T18:25:47.272959+00:00",
        "categories": [],
        "organization": "huggingface_ByteDanceSeed",
        "metadata": {
            "action": "authored 3 papers"
        },
        "objects": [
            {
                "title": "Knowledge Mining with Scene Text for Fine-Grained Recognition",
                "url": "https://huggingface.co/papers/2203.14215",
                "type": ""
            },
            {
                "title": "GaussTR: Foundation Model-Aligned Gaussian Transformer for Self-Supervised 3D Spatial Understanding",
                "url": "https://huggingface.co/papers/2412.13193",
                "type": ""
            },
            {
                "title": "Multimodal Mamba: Decoder-only Multimodal State Space Model via Quadratic to Linear Distillation",
                "url": "https://huggingface.co/papers/2502.13145",
                "type": ""
            }
        ],
        "original_source": "huggingface"
    },
    {
        "id": "8fa6c00ae497614ee4c03cc75f608f80",
        "source": "bytedance_aggregated",
        "type": "activity",
        "title": "[HuggingFace] updated 3 datasets",
        "description": "huggingface_bytedanceresearch: updated 3 datasets",
        "url": "https://huggingface.co/huggingface_bytedanceresearch",
        "published_date": "2025-02-28T18:25:47.272959+00:00",
        "categories": [],
        "organization": "huggingface_bytedanceresearch",
        "metadata": {
            "action": "updated 3 datasets"
        },
        "objects": [
            {
                "title": "ByteDance-Seed/mga-fineweb-edu",
                "url": "https://huggingface.co/datasets/ByteDance-Seed/mga-fineweb-edu",
                "type": ""
            },
            {
                "title": "ByteDance-Seed/mga-fineweb-edu",
                "url": "https://huggingface.co/datasets/ByteDance-Seed/mga-fineweb-edu",
                "type": ""
            },
            {
                "title": "ByteDance-Seed/mga-fineweb-edu",
                "url": "https://huggingface.co/datasets/ByteDance-Seed/mga-fineweb-edu",
                "type": ""
            }
        ],
        "original_source": "huggingface"
    },
    {
        "id": "e20ed697e21ba4f6380983be956a2593",
        "source": "bytedance_seed",
        "type": "research",
        "title": "FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference",
        "description": "Large language models (LLMs) encounter computational challenges during long-sequence inference, especially in the attention pre-filling phase, where the complexity grows quadratically with the prompt length. Previous efforts to mitigate these challenges have relied on fixed sparse attention patterns or identifying sparse attention patterns based on limited cases. However, these methods lacked the flexibility to efficiently adapt to varying input demands. In this paper, we introduce FlexPrefill, a Flexible sparse Pre-filling mechanism that dynamically adjusts sparse attention patterns and computational budget in real-time to meet the specific requirements of each input and attention head. The flexibility of our method is demonstrated through two key innovations: 1) Query-Aware Sparse Pattern Determination: By measuring Jensen-Shannon divergence, this component adaptively switches between query-specific diverse attention patterns and predefined attention patterns. 2) Cumulative-Attention Based Index Selection: This component dynamically selects query-key indexes to be computed based on different attention patterns, ensuring the sum of attention scores meets a predefined threshold. FlexPrefill adaptively optimizes the sparse pattern and sparse ratio of each attention head based on the prompt, enhancing efficiency in long-sequence inference tasks. Experimental results show significant improvements in both speed and accuracy over prior methods, providing a more flexible and efficient solution for LLM inference.",
        "url": "https://seed.bytedance.com/en/research/flexprefill-a-context-aware-sparse-attention-mechanism-for-efficient-long-sequence-inference",
        "published_date": "2025-02-27T10:00:00+00:00",
        "categories": [
            "Core Machine Learning"
        ],
        "organization": "ByteDance Seed",
        "metadata": {
            "author": "Xunhao Lai, Jianqiao Lu, Yao Luo, Yiyuan Ma, Xun Zhou",
            "journal": "ICLR 2025 Oral",
            "work_team": [
                "LLM"
            ]
        },
        "objects": [],
        "title_localized": {
            "en": "FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference",
            "zh": ""
        },
        "description_localized": {
            "en": "Large language models (LLMs) encounter computational challenges during long-sequence inference, especially in the attention pre-filling phase, where the complexity grows quadratically with the prompt length. Previous efforts to mitigate these challenges have relied on fixed sparse attention patterns or identifying sparse attention patterns based on limited cases. However, these methods lacked the flexibility to efficiently adapt to varying input demands. In this paper, we introduce FlexPrefill, a Flexible sparse Pre-filling mechanism that dynamically adjusts sparse attention patterns and computational budget in real-time to meet the specific requirements of each input and attention head. The flexibility of our method is demonstrated through two key innovations: 1) Query-Aware Sparse Pattern Determination: By measuring Jensen-Shannon divergence, this component adaptively switches between query-specific diverse attention patterns and predefined attention patterns. 2) Cumulative-Attention Based Index Selection: This component dynamically selects query-key indexes to be computed based on different attention patterns, ensuring the sum of attention scores meets a predefined threshold. FlexPrefill adaptively optimizes the sparse pattern and sparse ratio of each attention head based on the prompt, enhancing efficiency in long-sequence inference tasks. Experimental results show significant improvements in both speed and accuracy over prior methods, providing a more flexible and efficient solution for LLM inference.",
            "zh": ""
        },
        "url_localized": {
            "en": "https://seed.bytedance.com/en/research/flexprefill-a-context-aware-sparse-attention-mechanism-for-efficient-long-sequence-inference",
            "zh": "https://seed.bytedance.com/zh/research/"
        },
        "external_url": "https://arxiv.org/abs/2502.20766"
    },
    {
        "id": "332fbabd3bf042180a41e26d3db60b1b",
        "source": "bytedance_seed",
        "type": "research",
        "title": "Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts",
        "description": "Mixture-of-experts (MoE) has been extensively employed to scale large language models to trillion-plus parameters while maintaining a fixed computational cost. The development of large MoE models in the distributed scenario encounters the problem of large communication overhead. The inter-device communication of a MoE layer can occupy 47% time of the entire model execution with popular models and frameworks. Therefore, existing methods suggest the communication in a MoE layer to be pipelined with the computation for overlapping. However, these coarse grained overlapping schemes introduce a notable impairment of computational efficiency and the latency concealing is sub-optimal.\nTo this end, we present COMET, an optimized MoE system with fine-grained communication-computation overlapping. Leveraging data dependency analysis and task rescheduling, COMET achieves precise fine-grained overlapping of communication and computation. Through adaptive workload assignment, COMET effectively eliminates fine-grained communication bottlenecks and enhances its adaptability across various scenarios. Our evaluation shows that COMET accelerates the execution of a single MoE layer by 1.96\u00d7 and for end-to-end execution, COMET delivers a 1.71\u00d7 speedup on average. COMET has been adopted in the production environment of clusters with ten-thousand-scale of GPUs, achieving savings of millions of GPU hours.",
        "url": "https://seed.bytedance.com/en/research/comet-fine-grained-computation-communication-overlapping-for-mixture-of-experts",
        "published_date": "2025-02-26T10:00:00+00:00",
        "categories": [
            "System Research"
        ],
        "organization": "ByteDance Seed",
        "metadata": {
            "author": "Shulai Zhang, Ningxin Zheng, Haibin Lin, Ziheng Jiang, Wenlei Bao, Chengquan Jiang, Qi Hou, Weihao Cui, Size Zheng, Li-Wen Chang, Quan Chen, Xin Liu",
            "journal": "MLSys 2025",
            "work_team": [
                "Infrastructures"
            ]
        },
        "objects": [],
        "title_localized": {
            "en": "Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts",
            "zh": ""
        },
        "description_localized": {
            "en": "Mixture-of-experts (MoE) has been extensively employed to scale large language models to trillion-plus parameters while maintaining a fixed computational cost. The development of large MoE models in the distributed scenario encounters the problem of large communication overhead. The inter-device communication of a MoE layer can occupy 47% time of the entire model execution with popular models and frameworks. Therefore, existing methods suggest the communication in a MoE layer to be pipelined with the computation for overlapping. However, these coarse grained overlapping schemes introduce a notable impairment of computational efficiency and the latency concealing is sub-optimal.\nTo this end, we present COMET, an optimized MoE system with fine-grained communication-computation overlapping. Leveraging data dependency analysis and task rescheduling, COMET achieves precise fine-grained overlapping of communication and computation. Through adaptive workload assignment, COMET effectively eliminates fine-grained communication bottlenecks and enhances its adaptability across various scenarios. Our evaluation shows that COMET accelerates the execution of a single MoE layer by 1.96\u00d7 and for end-to-end execution, COMET delivers a 1.71\u00d7 speedup on average. COMET has been adopted in the production environment of clusters with ten-thousand-scale of GPUs, achieving savings of millions of GPU hours.",
            "zh": ""
        },
        "url_localized": {
            "en": "https://seed.bytedance.com/en/research/comet-fine-grained-computation-communication-overlapping-for-mixture-of-experts",
            "zh": "https://seed.bytedance.com/zh/research/"
        },
        "external_url": "https://arxiv.org/abs/2502.19811"
    },
    {
        "id": "6eda73661c79900e17a91a419ca58119",
        "source": "bytedance_seed",
        "type": "research",
        "title": "Ultra-Sparse Memory Network",
        "description": "It is widely acknowledged that the performance of Transformer models is logarithmically related to their number of parameters and computational complexity. While approaches like Mixture of Experts (MoE) decouple parameter count from computational complexity, they still face challenges in inference due to high memory access costs. This work introduces UltraMem, incorporating large-scale, ultra-sparse memory layer to address these limitations. Our approach significantly reduces inference latency while maintaining model performance. We also investigate the scaling laws of this new architecture, demonstrating that it not only exhibits favorable scaling properties but outperforms MoE. In experiments, the largest UltraMem we train has 20 million memory slots. The results show that our method achieves state-of-the-art inference speed and model performance within a given computational budget, paving the way for billions of slots or experts.",
        "url": "https://seed.bytedance.com/en/research/ultra-sparse-memory-network",
        "published_date": "2025-02-05T10:00:00+00:00",
        "categories": [
            "LLM"
        ],
        "organization": "ByteDance Seed",
        "metadata": {
            "author": "Zihao Huang, Qiyang Min, Hongzhi Huang, Defa Zhu, Yutao Zeng, Ran Guo, Xun Zhou",
            "journal": "ICLR 2025",
            "work_team": [
                "LLM"
            ]
        },
        "objects": [],
        "title_localized": {
            "en": "Ultra-Sparse Memory Network",
            "zh": ""
        },
        "description_localized": {
            "en": "It is widely acknowledged that the performance of Transformer models is logarithmically related to their number of parameters and computational complexity. While approaches like Mixture of Experts (MoE) decouple parameter count from computational complexity, they still face challenges in inference due to high memory access costs. This work introduces UltraMem, incorporating large-scale, ultra-sparse memory layer to address these limitations. Our approach significantly reduces inference latency while maintaining model performance. We also investigate the scaling laws of this new architecture, demonstrating that it not only exhibits favorable scaling properties but outperforms MoE. In experiments, the largest UltraMem we train has 20 million memory slots. The results show that our method achieves state-of-the-art inference speed and model performance within a given computational budget, paving the way for billions of slots or experts.",
            "zh": ""
        },
        "url_localized": {
            "en": "https://seed.bytedance.com/en/research/ultra-sparse-memory-network",
            "zh": "https://seed.bytedance.com/zh/research/"
        },
        "external_url": "https://arxiv.org/abs/2411.12364"
    },
    {
        "id": "854fe3d82848b97839c5a6ec02d3dccb",
        "source": "bytedance_seed",
        "type": "research",
        "title": "BFS-Prover: Scalable Best-First Tree Search for LLM-based Automatic Theorem Proving",
        "description": "Recent advancements in large language models (LLMs) have spurred growing interest in automatic theorem proving using Lean4, where effective tree search methods are crucial for navigating proof search spaces. While the existing approaches primarily rely on value functions and Monte Carlo Tree Search (MCTS), the potential of simpler methods like Best-First Search (BFS) remains underexplored. This paper investigates whether BFS can achieve competitive performance in large-scale theorem proving tasks. We present \\texttt{BFS-Prover}, a scalable expert iteration framework, featuring three key innovations. First, we implement strategic data filtering at each expert iteration round, excluding problems solvable via beam search node expansion to focus on harder cases. Second, we improve the sample efficiency of BFS through Direct Preference Optimization (DPO) applied to state-tactic pairs automatically annotated with compiler error feedback, refining the LLM's policy to prioritize productive expansions. Third, we employ length normalization in BFS to encourage exploration of deeper proof paths. \\texttt{BFS-Prover} achieves a score of 71.31 on the MiniF2F test set and therefore challenges the perceived necessity of complex tree search methods, demonstrating that BFS can achieve competitive performance when properly scaled.",
        "url": "https://seed.bytedance.com/en/research/bfs-prover-scalable-best-first-tree-search-for-llm-based-automatic-theorem-proving",
        "published_date": "2025-02-04T10:00:00+00:00",
        "categories": [
            "LLM"
        ],
        "organization": "ByteDance Seed",
        "metadata": {
            "author": "Ran Xin, Chenguang Xi, Jie Yang, Feng Chen, Hang Wu, Xia Xiao, Yifan Sun, Shen Zheng, Kai Shen",
            "journal": "arXiv",
            "work_team": [
                "LLM"
            ]
        },
        "objects": [],
        "title_localized": {
            "en": "BFS-Prover: Scalable Best-First Tree Search for LLM-based Automatic Theorem Proving",
            "zh": ""
        },
        "description_localized": {
            "en": "Recent advancements in large language models (LLMs) have spurred growing interest in automatic theorem proving using Lean4, where effective tree search methods are crucial for navigating proof search spaces. While the existing approaches primarily rely on value functions and Monte Carlo Tree Search (MCTS), the potential of simpler methods like Best-First Search (BFS) remains underexplored. This paper investigates whether BFS can achieve competitive performance in large-scale theorem proving tasks. We present \\texttt{BFS-Prover}, a scalable expert iteration framework, featuring three key innovations. First, we implement strategic data filtering at each expert iteration round, excluding problems solvable via beam search node expansion to focus on harder cases. Second, we improve the sample efficiency of BFS through Direct Preference Optimization (DPO) applied to state-tactic pairs automatically annotated with compiler error feedback, refining the LLM's policy to prioritize productive expansions. Third, we employ length normalization in BFS to encourage exploration of deeper proof paths. \\texttt{BFS-Prover} achieves a score of 71.31 on the MiniF2F test set and therefore challenges the perceived necessity of complex tree search methods, demonstrating that BFS can achieve competitive performance when properly scaled.",
            "zh": ""
        },
        "url_localized": {
            "en": "https://seed.bytedance.com/en/research/bfs-prover-scalable-best-first-tree-search-for-llm-based-automatic-theorem-proving",
            "zh": "https://seed.bytedance.com/zh/research/"
        },
        "external_url": "https://arxiv.org/abs/2502.03438"
    },
    {
        "id": "c84a93be2f4e9f6c272e882b620524b9",
        "source": "bytedance_aggregated",
        "type": "activity",
        "title": "[HuggingFace] authored 3 papers",
        "description": "huggingface_ByteDance: authored 3 papers",
        "url": "https://huggingface.co/huggingface_ByteDance",
        "published_date": "2025-01-28T18:25:47.272959+00:00",
        "categories": [],
        "organization": "huggingface_ByteDance",
        "metadata": {
            "action": "authored 3 papers"
        },
        "objects": [
            {
                "title": "4K4DGen: Panoramic 4D Generation at 4K Resolution",
                "url": "https://huggingface.co/papers/2406.13527",
                "type": ""
            },
            {
                "title": "InstructLayout: Instruction-Driven 2D and 3D Layout Synthesis with Semantic Graph Prior",
                "url": "https://huggingface.co/papers/2407.07580",
                "type": ""
            },
            {
                "title": "DiffSplat: Repurposing Image Diffusion Models for Scalable Gaussian Splat Generation",
                "url": "https://huggingface.co/papers/2501.16764",
                "type": ""
            }
        ],
        "original_source": "huggingface"
    },
    {
        "id": "866987ecb5fba48c97064c80eaa4014b",
        "source": "bytedance_aggregated",
        "type": "activity",
        "title": "[HuggingFace] authored 2 papers",
        "description": "huggingface_ByteDanceSeed: authored 2 papers",
        "url": "https://huggingface.co/huggingface_ByteDanceSeed",
        "published_date": "2025-01-28T18:25:47.272959+00:00",
        "categories": [],
        "organization": "huggingface_ByteDanceSeed",
        "metadata": {
            "action": "authored 2 papers"
        },
        "objects": [
            {
                "title": "DiffPortrait3D: Controllable Diffusion for Zero-Shot Portrait View Synthesis",
                "url": "https://huggingface.co/papers/2312.13016",
                "type": ""
            },
            {
                "title": "MagicPose4D: Crafting Articulated Models with Appearance and Motion Control",
                "url": "https://huggingface.co/papers/2405.14017",
                "type": ""
            }
        ],
        "original_source": "huggingface"
    },
    {
        "id": "191274ce8280034df09ab67a0b8ab227",
        "source": "bytedance_seed",
        "type": "research",
        "title": "DSTC: Direct Preference Learning with Only Self-Generated Tests and Code to Improve Code LMs",
        "description": "Direct preference learning offers a promising and computation-efficient beyond supervised fine-tuning (SFT) for improving code generation in coding large language models (LMs). However, the scarcity of reliable preference data is a bottleneck for the performance of direct preference learning to improve the coding accuracy of code LMs. In this paper, we introduce \\underline{\\textbf{D}}irect Preference Learning with Only \\underline{\\textbf{S}}elf-Generated \\underline{\\textbf{T}}ests and \\underline{\\textbf{C}}ode (DSTC), a framework that leverages only self-generated code snippets and tests to construct reliable preference pairs such that direct preference learning can improve LM coding accuracy without external annotations. DSTC combines a minimax selection process and test-code concatenation to improve preference pair quality, reducing the influence of incorrect self-generated tests and enhancing model performance without the need for costly reward models. When applied with direct preference learning methods such as Direct Preference Optimization (DPO) and Kahneman-Tversky Optimization (KTO), DSTC yields stable improvements in coding accuracy (pass@1 score) across diverse coding benchmarks, including HumanEval, MBPP, and BigCodeBench, demonstrating both its effectiveness and scalability for models of various sizes. This approach autonomously enhances code generation accuracy across LLMs of varying sizes, reducing reliance on expensive annotated coding datasets.",
        "url": "https://seed.bytedance.com/en/research/dstc-direct-preference-learning-with-only-self-generated-tests-and-code-to-improve-code-lms",
        "published_date": "2024-11-19T10:00:00+00:00",
        "categories": [
            "Code Generation"
        ],
        "organization": "ByteDance Seed",
        "metadata": {
            "author": "Zhihan Liu, Shenao Zhang, Yongfei Liu, Boyi Liu, Yingxiang Yang, Zhaoran Wang",
            "journal": "arXiv",
            "work_team": [
                "Infrastructures"
            ]
        },
        "objects": [],
        "title_localized": {
            "en": "DSTC: Direct Preference Learning with Only Self-Generated Tests and Code to Improve Code LMs",
            "zh": ""
        },
        "description_localized": {
            "en": "Direct preference learning offers a promising and computation-efficient beyond supervised fine-tuning (SFT) for improving code generation in coding large language models (LMs). However, the scarcity of reliable preference data is a bottleneck for the performance of direct preference learning to improve the coding accuracy of code LMs. In this paper, we introduce \\underline{\\textbf{D}}irect Preference Learning with Only \\underline{\\textbf{S}}elf-Generated \\underline{\\textbf{T}}ests and \\underline{\\textbf{C}}ode (DSTC), a framework that leverages only self-generated code snippets and tests to construct reliable preference pairs such that direct preference learning can improve LM coding accuracy without external annotations. DSTC combines a minimax selection process and test-code concatenation to improve preference pair quality, reducing the influence of incorrect self-generated tests and enhancing model performance without the need for costly reward models. When applied with direct preference learning methods such as Direct Preference Optimization (DPO) and Kahneman-Tversky Optimization (KTO), DSTC yields stable improvements in coding accuracy (pass@1 score) across diverse coding benchmarks, including HumanEval, MBPP, and BigCodeBench, demonstrating both its effectiveness and scalability for models of various sizes. This approach autonomously enhances code generation accuracy across LLMs of varying sizes, reducing reliance on expensive annotated coding datasets.",
            "zh": ""
        },
        "url_localized": {
            "en": "https://seed.bytedance.com/en/research/dstc-direct-preference-learning-with-only-self-generated-tests-and-code-to-improve-code-lms",
            "zh": "https://seed.bytedance.com/zh/research/"
        },
        "external_url": "https://arxiv.org/abs/2411.13611"
    },
    {
        "id": "bcfaf2dc3cc4d99c9de472551d36c7f3",
        "source": "bytedance_seed",
        "type": "research",
        "title": "LSH-MoE: Communication-efficient MoE Training via Locality-Sensitive Hashing",
        "description": "Larger transformer models always perform better on various tasks but require more costs to scale up the model size. To efficiently enlarge models, the mixture-of-experts (MoE) architecture is widely adopted, which consists of a gate network and a series of experts and keep the training cost constant by routing the input data to a fixed number of experts instead of all. In existing large-scale MoE training systems, experts would be distributed among different GPUs for parallelization, and thus input data requires additional all-to-all communications to access the target experts and conduct corresponding computations. However, upon evaluating the training process of three mainstream MoE models on commonly used GPU clusters, we found that the all-to-all communication ratio averaged around 45%, which significantly hinders the efficiency and scalability of training MoE models.\nIn this paper, we propose LSH-MoE, a communication-efficient MoE training framework using locality-sensitive hashing (LSH). We first present the problems of scaling MoE training in existing systems and highlight the potential of exploiting token similarity to facilitate data compression. Then, we introduce an efficient LSH-based compression technique, which utilizes the cross-polytope hashing for rapid clustering and implements a residual-based error compensation scheme to alleviate the adverse impact of compression. To verify the effectiveness of our methods, we conduct experiments on both language models (e.g., RoBERTa, GPT, and T5) and vision models (e.g., Swin) for pre-training and fine-tuning tasks. The results demonstrate that our method substantially outperforms its counterparts across different tasks by 1.28x - 2.2x of speedup.",
        "url": "https://seed.bytedance.com/en/research/lsh-moe-communication-efficient-moe-training-via-locality-sensitive-hashing",
        "published_date": "2024-11-12T10:00:00+00:00",
        "categories": [
            "LLM"
        ],
        "organization": "ByteDance Seed",
        "metadata": {
            "author": "Xiaonan Nie, Qibin Liu, Fangcheng Fu, Shenhan Zhu, Xupeng Miao, Xiaoyang Li, Yang Zhang, Shouda Liu, Bin Cui",
            "journal": "NeurIPS 2024",
            "work_team": [
                "Infrastructures"
            ]
        },
        "objects": [],
        "title_localized": {
            "en": "LSH-MoE: Communication-efficient MoE Training via Locality-Sensitive Hashing",
            "zh": ""
        },
        "description_localized": {
            "en": "Larger transformer models always perform better on various tasks but require more costs to scale up the model size. To efficiently enlarge models, the mixture-of-experts (MoE) architecture is widely adopted, which consists of a gate network and a series of experts and keep the training cost constant by routing the input data to a fixed number of experts instead of all. In existing large-scale MoE training systems, experts would be distributed among different GPUs for parallelization, and thus input data requires additional all-to-all communications to access the target experts and conduct corresponding computations. However, upon evaluating the training process of three mainstream MoE models on commonly used GPU clusters, we found that the all-to-all communication ratio averaged around 45%, which significantly hinders the efficiency and scalability of training MoE models.\nIn this paper, we propose LSH-MoE, a communication-efficient MoE training framework using locality-sensitive hashing (LSH). We first present the problems of scaling MoE training in existing systems and highlight the potential of exploiting token similarity to facilitate data compression. Then, we introduce an efficient LSH-based compression technique, which utilizes the cross-polytope hashing for rapid clustering and implements a residual-based error compensation scheme to alleviate the adverse impact of compression. To verify the effectiveness of our methods, we conduct experiments on both language models (e.g., RoBERTa, GPT, and T5) and vision models (e.g., Swin) for pre-training and fine-tuning tasks. The results demonstrate that our method substantially outperforms its counterparts across different tasks by 1.28x - 2.2x of speedup.",
            "zh": ""
        },
        "url_localized": {
            "en": "https://seed.bytedance.com/en/research/lsh-moe-communication-efficient-moe-training-via-locality-sensitive-hashing",
            "zh": "https://seed.bytedance.com/zh/research/"
        },
        "external_url": "https://arxiv.org/abs/2411.08446"
    },
    {
        "id": "42f779fff6e040639f69415ab08dc5b2",
        "source": "bytedance_seed",
        "type": "research",
        "title": "Polynomial Composition Activations: Unleashing the Dynamics of Large Language Models",
        "description": "Transformers have found extensive applications across various domains due to the powerful fitting capabilities. This success can be partially attributed to their inherent nonlinearity. Thus, in addition to the ReLU function employed in the original transformer architecture, researchers have explored alternative modules such as GeLU and SwishGLU to enhance nonlinearity and thereby augment representational capacity. In this paper, we propose a novel category of polynomial composition activations (PolyCom), designed to optimize the dynamics of transformers. Theoretically, we provide a comprehensive mathematical analysis of PolyCom, highlighting its enhanced expressivity and efficacy relative to other activation functions. Notably, we demonstrate that networks incorporating PolyCom achieve the optimal approximation rate, indicating that PolyCom networks require minimal parameters to approximate general smooth functions in Sobolev spaces. We conduct empirical experiments on the pre-training configurations of large language models (LLMs), including both dense and sparse architectures. By substituting conventional activation functions with PolyCom, we enable LLMs to capture higher-order interactions within the data, thus improving performance metrics in terms of accuracy and convergence rates. Extensive experimental results demonstrate the effectiveness of our method, showing substantial improvements over other activation functions. Code is available at this https URL.",
        "url": "https://seed.bytedance.com/en/research/polynomial-composition-activations-unleashing-the-dynamics-of-large-language-models",
        "published_date": "2024-11-05T10:00:00+00:00",
        "categories": [
            "LLM"
        ],
        "organization": "ByteDance Seed",
        "metadata": {
            "author": "Zhijian Zhuo, Ya Wang, Yutao Zeng, Xiaoqing Li, Xun Zhou, Jinwen Ma",
            "journal": "ICLR 2025",
            "work_team": [
                "LLM"
            ]
        },
        "objects": [],
        "title_localized": {
            "en": "Polynomial Composition Activations: Unleashing the Dynamics of Large Language Models",
            "zh": ""
        },
        "description_localized": {
            "en": "Transformers have found extensive applications across various domains due to the powerful fitting capabilities. This success can be partially attributed to their inherent nonlinearity. Thus, in addition to the ReLU function employed in the original transformer architecture, researchers have explored alternative modules such as GeLU and SwishGLU to enhance nonlinearity and thereby augment representational capacity. In this paper, we propose a novel category of polynomial composition activations (PolyCom), designed to optimize the dynamics of transformers. Theoretically, we provide a comprehensive mathematical analysis of PolyCom, highlighting its enhanced expressivity and efficacy relative to other activation functions. Notably, we demonstrate that networks incorporating PolyCom achieve the optimal approximation rate, indicating that PolyCom networks require minimal parameters to approximate general smooth functions in Sobolev spaces. We conduct empirical experiments on the pre-training configurations of large language models (LLMs), including both dense and sparse architectures. By substituting conventional activation functions with PolyCom, we enable LLMs to capture higher-order interactions within the data, thus improving performance metrics in terms of accuracy and convergence rates. Extensive experimental results demonstrate the effectiveness of our method, showing substantial improvements over other activation functions. Code is available at this https URL.",
            "zh": ""
        },
        "url_localized": {
            "en": "https://seed.bytedance.com/en/research/polynomial-composition-activations-unleashing-the-dynamics-of-large-language-models",
            "zh": "https://seed.bytedance.com/zh/research/"
        },
        "external_url": "https://arxiv.org/abs/2411.03884"
    },
    {
        "id": "5f77273dd97bf3fb11b3ca558865d34f",
        "source": "bytedance_seed",
        "type": "research",
        "title": "Reward-Augmented Data Enhances Direct Preference Alignment of LLMs",
        "description": "Preference alignment in Large Language Models (LLMs) has significantly improved their ability to adhere to human instructions and intentions. However, existing direct alignment algorithms primarily focus on relative preferences and often overlook the qualitative aspects of responses. Striving to maximize the implicit reward gap between the chosen and the slightly inferior rejected responses can cause overfitting and unnecessary unlearning of the high-quality rejected responses. The unawareness of the reward scores also drives the LLM to indiscriminately favor the low-quality chosen responses and fail to generalize to responses with the highest rewards, which are sparse in data. To overcome these shortcomings, our study introduces reward-conditioned LLM policies that discern and learn from the entire spectrum of response quality within the dataset, helping extrapolate to more optimal regions. We propose an effective yet simple data relabeling method that conditions the preference pairs on quality scores to construct a reward-augmented dataset. This dataset is easily integrated with existing direct alignment algorithms and is applicable to any preference dataset. The experimental results across instruction-following benchmarks including AlpacaEval, MT-Bench, and Arena-Hard-Auto demonstrate that our approach consistently boosts the performance of DPO by a considerable margin across diverse models. Additionally, our method improves the average accuracy on various academic benchmarks. When applying our method to on-policy data, the resulting DPO model achieves SOTA results on AlpacaEval. Through ablation studies, we demonstrate that our method not only maximizes the utility of preference data but also mitigates the issue of unlearning, demonstrating its broad effectiveness beyond mere dataset expansion.",
        "url": "https://seed.bytedance.com/en/research/reward-augmented-data-enhances-direct-preference-alignment-of-llms",
        "published_date": "2024-10-09T11:00:00+00:00",
        "categories": [
            "LLM"
        ],
        "organization": "ByteDance Seed",
        "metadata": {
            "author": "Shenao Zhang, Zhihan Liu, Zhaoran Wang",
            "journal": "arXiv",
            "work_team": [
                "Infrastructures"
            ]
        },
        "objects": [],
        "title_localized": {
            "en": "Reward-Augmented Data Enhances Direct Preference Alignment of LLMs",
            "zh": ""
        },
        "description_localized": {
            "en": "Preference alignment in Large Language Models (LLMs) has significantly improved their ability to adhere to human instructions and intentions. However, existing direct alignment algorithms primarily focus on relative preferences and often overlook the qualitative aspects of responses. Striving to maximize the implicit reward gap between the chosen and the slightly inferior rejected responses can cause overfitting and unnecessary unlearning of the high-quality rejected responses. The unawareness of the reward scores also drives the LLM to indiscriminately favor the low-quality chosen responses and fail to generalize to responses with the highest rewards, which are sparse in data. To overcome these shortcomings, our study introduces reward-conditioned LLM policies that discern and learn from the entire spectrum of response quality within the dataset, helping extrapolate to more optimal regions. We propose an effective yet simple data relabeling method that conditions the preference pairs on quality scores to construct a reward-augmented dataset. This dataset is easily integrated with existing direct alignment algorithms and is applicable to any preference dataset. The experimental results across instruction-following benchmarks including AlpacaEval, MT-Bench, and Arena-Hard-Auto demonstrate that our approach consistently boosts the performance of DPO by a considerable margin across diverse models. Additionally, our method improves the average accuracy on various academic benchmarks. When applying our method to on-policy data, the resulting DPO model achieves SOTA results on AlpacaEval. Through ablation studies, we demonstrate that our method not only maximizes the utility of preference data but also mitigates the issue of unlearning, demonstrating its broad effectiveness beyond mere dataset expansion.",
            "zh": ""
        },
        "url_localized": {
            "en": "https://seed.bytedance.com/en/research/reward-augmented-data-enhances-direct-preference-alignment-of-llms",
            "zh": "https://seed.bytedance.com/zh/research/"
        },
        "external_url": "https://arxiv.org/abs/2410.08067"
    },
    {
        "id": "36c3096dd80939f865809ff47cecf79c",
        "source": "bytedance_seed",
        "type": "research",
        "title": "HybridFlow: A Flexible and Efficient RLHF Framework",
        "description": "Reinforcement Learning from Human Feedback (RLHF) is widely used in Large Language Model (LLM) alignment. Traditional RL can be modeled as a dataflow, where each node represents computation of a neural network (NN) and each edge denotes data dependencies between the NNs. RLHF complicates the dataflow by expanding each node into a distributed LLM training or generation program, and each edge into a many-to-many multicast. Traditional RL frameworks execute the dataflow using a single controller to instruct both intra-node computation and inter-node communication, which can be inefficient in RLHF due to large control dispatch overhead for distributed intra-node computation. Existing RLHF systems adopt a multi-controller paradigm, which can be inflexible due to nesting distributed computation and data communication. We propose HybridFlow, which combines single-controller and multi-controller paradigms in a hybrid manner to enable flexible representation and efficient execution of the RLHF dataflow. We carefully design a set of hierarchical APIs that decouple and encapsulate computation and data dependencies in the complex RLHF dataflow, allowing efficient operation orchestration to implement RLHF algorithms and flexible mapping of the computation onto various devices. We further design a 3D-HybridEngine for efficient actor model resharding between training and generation phases, with zero memory redundancy and significantly reduced communication overhead. Our experimental results demonstrate 1.53\u00d7~20.57\u00d7 throughput improvement when running various RLHF algorithms using HybridFlow, as compared with state-of-the-art baselines. HybridFlow source code will be available at this https URL\uff08https://github.com/volcengine/verl\uff09.",
        "url": "https://seed.bytedance.com/en/research/hybridflow-a-flexible-and-efficient-rlhf-framework",
        "published_date": "2024-10-01T11:00:00+00:00",
        "categories": [
            "Reinforcement Learning",
            "System Research"
        ],
        "organization": "ByteDance Seed",
        "metadata": {
            "author": "Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, Chuan Wu",
            "journal": "EuroSys 2025",
            "work_team": [
                "Infrastructures"
            ]
        },
        "objects": [],
        "title_localized": {
            "en": "HybridFlow: A Flexible and Efficient RLHF Framework",
            "zh": ""
        },
        "description_localized": {
            "en": "Reinforcement Learning from Human Feedback (RLHF) is widely used in Large Language Model (LLM) alignment. Traditional RL can be modeled as a dataflow, where each node represents computation of a neural network (NN) and each edge denotes data dependencies between the NNs. RLHF complicates the dataflow by expanding each node into a distributed LLM training or generation program, and each edge into a many-to-many multicast. Traditional RL frameworks execute the dataflow using a single controller to instruct both intra-node computation and inter-node communication, which can be inefficient in RLHF due to large control dispatch overhead for distributed intra-node computation. Existing RLHF systems adopt a multi-controller paradigm, which can be inflexible due to nesting distributed computation and data communication. We propose HybridFlow, which combines single-controller and multi-controller paradigms in a hybrid manner to enable flexible representation and efficient execution of the RLHF dataflow. We carefully design a set of hierarchical APIs that decouple and encapsulate computation and data dependencies in the complex RLHF dataflow, allowing efficient operation orchestration to implement RLHF algorithms and flexible mapping of the computation onto various devices. We further design a 3D-HybridEngine for efficient actor model resharding between training and generation phases, with zero memory redundancy and significantly reduced communication overhead. Our experimental results demonstrate 1.53\u00d7~20.57\u00d7 throughput improvement when running various RLHF algorithms using HybridFlow, as compared with state-of-the-art baselines. HybridFlow source code will be available at this https URL\uff08https://github.com/volcengine/verl\uff09.",
            "zh": ""
        },
        "url_localized": {
            "en": "https://seed.bytedance.com/en/research/hybridflow-a-flexible-and-efficient-rlhf-framework",
            "zh": "https://seed.bytedance.com/zh/research/"
        },
        "external_url": "https://arxiv.org/abs/2409.19256"
    },
    {
        "id": "c1e3ddb9c963404f531826228de71cf4",
        "source": "bytedance_seed",
        "type": "research",
        "title": "MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs",
        "description": "We present the design, implementation and engineering experience in building and deploying MegaScale, a production system for training large language models (LLMs) at the scale of more than 10,000 GPUs. Training LLMs at this scale brings unprecedented challenges to training efficiency and stability. We take a full-stack approach that co-designs the algorithmic and system components across model block and optimizer design, computation and communication overlapping, operator optimization, data pipeline, and network performance tuning. Maintaining high efficiency throughout the training process (i.e., stability) is an important consideration in production given the long extent of LLM training jobs. Many hard stability issues only emerge at large scale, and in-depth observability is the key to address them. We develop a set of diagnosis tools to monitor system components and events deep in the stack, identify root causes, and derive effective techniques to achieve fault tolerance and mitigate stragglers. MegaScale achieves 55.2% Model FLOPs Utilization (MFU) when training a 175B LLM model on 12,288 GPUs, improving the MFU by 1.34x compared to Megatron-LM. We share our operational experience in identifying and fixing failures and stragglers. We hope by articulating the problems and sharing our experience from a systems perspective, this work can inspire future LLM systems research.",
        "url": "https://seed.bytedance.com/en/research/megascale-scaling-large-language-model-training-to-more-than-10-000-gpus",
        "published_date": "2024-02-22T10:00:00+00:00",
        "categories": [
            "System Research"
        ],
        "organization": "ByteDance Seed",
        "metadata": {
            "author": "Ziheng Jiang,Haibin Lin,Yinmin Zhong,Qi Huang,Yangrui Chen,Zhi Zhang,Yanghua Peng,Xiang Li,Cong Xie,Shibiao Nong,Yulu Jia,Sun He,Hongmin Chen,Zhihao Bai,Qi Hou,Shipeng Yan,Ding Zhou,Yiyao Sheng,Zhuo Jiang,Haohan Xu,Haoran Wei,Zhang Zhang,Pengfei Nie,Leqi Zou,Sida Zhao,Liang Xiang,Zherui Liu,Zhe Li,Xiaoying Jia,Jianxi Ye,Xin Jin,Xin Liu",
            "journal": "Nsdi 2024",
            "work_team": [
                "Infrastructures"
            ]
        },
        "objects": [],
        "title_localized": {
            "en": "MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs",
            "zh": ""
        },
        "description_localized": {
            "en": "We present the design, implementation and engineering experience in building and deploying MegaScale, a production system for training large language models (LLMs) at the scale of more than 10,000 GPUs. Training LLMs at this scale brings unprecedented challenges to training efficiency and stability. We take a full-stack approach that co-designs the algorithmic and system components across model block and optimizer design, computation and communication overlapping, operator optimization, data pipeline, and network performance tuning. Maintaining high efficiency throughout the training process (i.e., stability) is an important consideration in production given the long extent of LLM training jobs. Many hard stability issues only emerge at large scale, and in-depth observability is the key to address them. We develop a set of diagnosis tools to monitor system components and events deep in the stack, identify root causes, and derive effective techniques to achieve fault tolerance and mitigate stragglers. MegaScale achieves 55.2% Model FLOPs Utilization (MFU) when training a 175B LLM model on 12,288 GPUs, improving the MFU by 1.34x compared to Megatron-LM. We share our operational experience in identifying and fixing failures and stragglers. We hope by articulating the problems and sharing our experience from a systems perspective, this work can inspire future LLM systems research.",
            "zh": ""
        },
        "url_localized": {
            "en": "https://seed.bytedance.com/en/research/megascale-scaling-large-language-model-training-to-more-than-10-000-gpus",
            "zh": "https://seed.bytedance.com/zh/research/"
        },
        "external_url": "https://arxiv.org/abs/2402.15627"
    }
]