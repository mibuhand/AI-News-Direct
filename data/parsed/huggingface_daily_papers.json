[
  {
    "id": "0139ab6619114102f84bcc9c449fd176",
    "source": "huggingface",
    "type": "paper",
    "title": "Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation",
    "description": "This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kand...<br/>Upvotes: 219<br/>GitHub Stars: 508<br/>Authors: Vladimir Arkhipkin, Vladimir Korviakov, Nikolai Gerasimenko<br/>ðŸ”— <a href=\"https://github.com/kandinskylab/kandinsky-5\">GitHub</a><br/>ðŸ”— <a href=\"https://kandinskylab.ai/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2511.14993\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2511.14993",
    "external_url": "https://github.com/kandinskylab/kandinsky-5",
    "published_date": "2025-11-18T19:23:22.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2511.14993",
      "upvotes": 219,
      "github_stars": 508,
      "github_url": "https://github.com/kandinskylab/kandinsky-5",
      "project_url": "https://kandinskylab.ai/",
      "authors": [
        "Vladimir Arkhipkin",
        "Vladimir Korviakov",
        "Nikolai Gerasimenko",
        "Denis Parkhomenko",
        "Viacheslav Vasilev",
        "Alexey Letunovskiy",
        "Maria Kovaleva",
        "Nikolai Vaulin",
        "Ivan Kirillov",
        "Lev Novitskiy",
        "Denis Koposov",
        "Nikita Kiselev",
        "Alexander Varlamov",
        "Dmitrii Mikhailov",
        "Vladimir Polovnikov",
        "Andrey Shutkin",
        "Ilya Vasiliev",
        "Julia Agafonova",
        "Anastasiia Kargapoltseva",
        "Anna Dmitrienko",
        "Anastasia Maltseva",
        "Anna Averchenkova",
        "Olga Kim",
        "Tatiana Nikulina",
        "Denis Dimitrov"
      ],
      "summary": "This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kandinsky 5.0 Image Lite - a line-up of 6B parameter image generation models, Kandinsky 5.0 Video Lite - a fast and lightweight 2B parameter text-to-video and image-to-video models, and Kandinsky 5.0 Video Pro - 19B parameter models that achieves superior video generation quality. We provide a comprehensive review of the data curation lifecycle - including collection, processing, filtering and clustering - for the multi-stage training pipeline that involves extensive pre-training and incorporates quality-enhancement techniques such as self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. We also present novel architectural, training, and inference optimizations that enable Kandinsky 5.0 to achieve high generation speeds and state-of-the-art performance across various tasks, as demonstrated by human evaluation. As a large-scale, publicly available generative framework, Kandinsky 5.0 leverages the full potential of its pre-training and subsequent stages to be adapted for a wide range of generative applications. We hope that this report, together with the release of our open-source code and training checkpoints, will substantially advance the development and accessibility of high-quality generative models for the research community.",
      "fetch_date": "2025-11-20",
      "num_comments": 5,
      "ai_summary": "Kandinsky 5.0 is a family of state-of-the-art generative models for high-resolution images and short videos, featuring model lineups with varying parameters and enhanced training techniques to achieve superior quality and performance.",
      "ai_keywords": [
        "foundation models",
        "high-resolution image synthesis",
        "10-second video synthesis",
        "image generation models",
        "text-to-video models",
        "image-to-video models",
        "multi-stage training pipeline",
        "self-supervised fine-tuning",
        "reinforcement learning",
        "pre-training",
        "quality-enhancement techniques",
        "architectural optimizations",
        "training optimizations",
        "inference optimizations",
        "human evaluation",
        "generative framework",
        "open-source code",
        "training checkpoints"
      ]
    }
  },
  {
    "id": "b81e62a9e2bd28a9fe78c760c27308b6",
    "source": "huggingface",
    "type": "paper",
    "title": "Thinking with Video: Video Generation as a Promising Multimodal\n  Reasoning Paradigm",
    "description": "\"Thinking with Text\" and \"Thinking with Images\" paradigm significantly\nimprove the reasoning ability of large language models (LLMs) and Vision\nLanguage Models (VLMs). However, these paradigms have in...<br/>Upvotes: 207<br/>GitHub Stars: 214<br/>Authors: Jingqi Tong, Yurong Mou, Hangcheng Li<br/>ðŸ”— <a href=\"https://github.com/tongjingqi/Thinking-with-Video\">GitHub</a><br/>ðŸ”— <a href=\"https://thinking-with-video.github.io/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2511.04570\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2511.04570",
    "external_url": "https://github.com/tongjingqi/Thinking-with-Video",
    "published_date": "2025-11-06T12:25:23.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2511.04570",
      "upvotes": 207,
      "github_stars": 214,
      "github_url": "https://github.com/tongjingqi/Thinking-with-Video",
      "project_url": "https://thinking-with-video.github.io/",
      "authors": [
        "Jingqi Tong",
        "Yurong Mou",
        "Hangcheng Li",
        "Mingzhe Li",
        "Yongzhuo Yang",
        "Ming Zhang",
        "Qiguang Chen",
        "Tianyi Liang",
        "Xiaomeng Hu",
        "Yining Zheng",
        "Xinchi Chen",
        "Jun Zhao",
        "Xuanjing Huang",
        "Xipeng Qiu"
      ],
      "summary": "\"Thinking with Text\" and \"Thinking with Images\" paradigm significantly\nimprove the reasoning ability of large language models (LLMs) and Vision\nLanguage Models (VLMs). However, these paradigms have inherent limitations. (1)\nImages capture only single moments and fail to represent dynamic processes or\ncontinuous changes, and (2) The separation of text and vision as distinct\nmodalities, hindering unified multimodal understanding and generation. To\novercome these limitations, we introduce \"Thinking with Video\", a new paradigm\nthat leverages video generation models, such as Sora-2, to bridge visual and\ntextual reasoning in a unified temporal framework. To support this exploration,\nwe developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBench\nencompasses two task categories: (1) vision-centric tasks (e.g., Eyeballing\nPuzzles), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Our\nevaluation establishes Sora-2 as a capable reasoner. On vision-centric tasks,\nSora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even\nsurpasses VLMs on several tasks, such as Eyeballing Games. On text-centric\ntasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU.\nFurthermore, we systematically analyse the source of these abilities. We also\nfind that self-consistency and in-context learning can improve Sora-2's\nperformance. In summary, our findings demonstrate that the video generation\nmodel is the potential unified multimodal understanding and generation model,\npositions \"thinking with video\" as a unified multimodal reasoning paradigm.",
      "fetch_date": "2025-11-07",
      "num_comments": 4,
      "ai_summary": "The \"Thinking with Video\" paradigm enhances multimodal reasoning by integrating video generation models, demonstrated through the Video Thinking Benchmark and improved performance on both vision and text tasks.",
      "ai_keywords": [
        "Thinking with Text",
        "Thinking with Images",
        "large language models",
        "Vision Language Models",
        "Thinking with Video",
        "video generation models",
        "Video Thinking Benchmark",
        "vision-centric tasks",
        "text-centric tasks",
        "Eyeballing Puzzles",
        "GSM8K",
        "MMMU",
        "self-consistency",
        "in-context learning"
      ]
    }
  },
  {
    "id": "4622a47084403f1652a1baac4335b4fa",
    "source": "huggingface",
    "type": "paper",
    "title": "Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds",
    "description": "We introduce Lumine, the first open recipe for developing generalist agents capable of completing hours-long complex missions in real time within challenging 3D open-world environments. Lumine adopts ...<br/>Upvotes: 191<br/>Authors: Weihao Tan, Xiangyang Li, Yunhao Fang<br/>ðŸ”— <a href=\"https://www.lumine-ai.org/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2511.08892\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2511.08892",
    "external_url": "",
    "published_date": "2025-11-11T21:01:26.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2511.08892",
      "upvotes": 191,
      "github_stars": 0,
      "github_url": "",
      "project_url": "https://www.lumine-ai.org/",
      "authors": [
        "Weihao Tan",
        "Xiangyang Li",
        "Yunhao Fang",
        "Heyuan Yao",
        "Shi Yan",
        "Hao Luo",
        "Tenglong Ao",
        "Huihui Li",
        "Hongbin Ren",
        "Bairen Yi",
        "Yujia Qin",
        "Bo An",
        "Libin Liu",
        "Guang Shi"
      ],
      "summary": "We introduce Lumine, the first open recipe for developing generalist agents capable of completing hours-long complex missions in real time within challenging 3D open-world environments. Lumine adopts a human-like interaction paradigm that unifies perception, reasoning, and action in an end-to-end manner, powered by a vision-language model. It processes raw pixels at 5 Hz to produce precise 30 Hz keyboard-mouse actions and adaptively invokes reasoning only when necessary. Trained in Genshin Impact, Lumine successfully completes the entire five-hour Mondstadt main storyline on par with human-level efficiency and follows natural language instructions to perform a broad spectrum of tasks in both 3D open-world exploration and 2D GUI manipulation across collection, combat, puzzle-solving, and NPC interaction. In addition to its in-domain performance, Lumine demonstrates strong zero-shot cross-game generalization. Without any fine-tuning, it accomplishes 100-minute missions in Wuthering Waves and the full five-hour first chapter of Honkai: Star Rail. These promising results highlight Lumine's effectiveness across distinct worlds and interaction dynamics, marking a concrete step toward generalist agents in open-ended environments.",
      "fetch_date": "2025-11-13",
      "num_comments": 12,
      "ai_summary": "Lumine, a vision-language model-based agent, completes complex missions in real-time across different 3D open-world environments with human-like efficiency and zero-shot cross-game generalization.",
      "ai_keywords": [
        "vision-language model",
        "end-to-end",
        "3D open-world environments",
        "human-like interaction",
        "real-time",
        "zero-shot cross-game generalization"
      ]
    }
  },
  {
    "id": "307d918f05f7df6902e7de22a3fa64bc",
    "source": "huggingface",
    "type": "paper",
    "title": "ROOT: Robust Orthogonalized Optimizer for Neural Network Training",
    "description": "The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advance...<br/>Upvotes: 165<br/>GitHub Stars: 921<br/>Authors: Wei He, Kai Han, Hang Zhou<br/>ðŸ”— <a href=\"https://github.com/huawei-noah/noah-research\">GitHub</a><br/>ðŸ”— <a href=\"https://github.com/huawei-noah/noah-research/tree/master/ROOT\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2511.20626\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2511.20626",
    "external_url": "https://github.com/huawei-noah/noah-research",
    "published_date": "2025-11-25T13:48:05.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2511.20626",
      "upvotes": 165,
      "github_stars": 921,
      "github_url": "https://github.com/huawei-noah/noah-research",
      "project_url": "https://github.com/huawei-noah/noah-research/tree/master/ROOT",
      "authors": [
        "Wei He",
        "Kai Han",
        "Hang Zhou",
        "Hanting Chen",
        "Zhicheng Liu",
        "Xinghao Chen",
        "Yunhe Wang"
      ],
      "summary": "The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.",
      "fetch_date": "2025-11-26",
      "num_comments": 4,
      "ai_summary": "ROOT, a robust optimizer, enhances training stability and convergence for large language models by addressing dimensional fragility and outlier noise through adaptive Newton iterations and proximal optimization.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "momentum orthogonalization",
        "dimensional fragility",
        "outlier-induced noise",
        "adaptive Newton iterations",
        "proximal optimization",
        "Muon",
        "Adam-based optimizers",
        "robust optimizer"
      ]
    }
  },
  {
    "id": "0a60cb0f535685cc7eb12b1b01de62bc",
    "source": "huggingface",
    "type": "paper",
    "title": "MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling",
    "description": "We present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or co...<br/>Upvotes: 156<br/>GitHub Stars: 1185<br/>Authors: MiroMind Team, Song Bai, Lidong Bing<br/>ðŸ”— <a href=\"https://github.com/MiroMindAI/MiroThinker\">GitHub</a><br/>ðŸ”— <a href=\"https://dr.miromind.ai/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2511.11793\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2511.11793",
    "external_url": "https://github.com/MiroMindAI/MiroThinker",
    "published_date": "2025-11-14T13:52:07.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2511.11793",
      "upvotes": 156,
      "github_stars": 1185,
      "github_url": "https://github.com/MiroMindAI/MiroThinker",
      "project_url": "https://dr.miromind.ai/",
      "authors": [
        "MiroMind Team",
        "Song Bai",
        "Lidong Bing",
        "Carson Chen",
        "Guanzheng Chen",
        "Yuntao Chen",
        "Zhe Chen",
        "Ziyi Chen",
        "Jifeng Dai",
        "Xuan Dong",
        "Yue Deng",
        "Yunjie Fu",
        "Junqi Ge",
        "Chenxia Han",
        "Tammy Huang",
        "Zhenhang Huang",
        "Jerry Jiao",
        "Shilei Jiang",
        "Tianyu Jiao",
        "Xiaoqi Jian",
        "Lei Lei",
        "Ruilin Li",
        "Ryan Luo",
        "Tiantong Li",
        "Xiang Lin",
        "Ziyuan Liu",
        "Zhiqi Li",
        "Jie Ni",
        "Qiang Ren",
        "Pax Sun",
        "Shiqian Su",
        "Chenxin Tao",
        "Bin Wang",
        "Hellen Wang",
        "Haonan Wang",
        "James Wang",
        "Jin Wang",
        "Jojo Wang",
        "Letian Wang",
        "Shizun Wang",
        "Weizhi Wang",
        "Zixuan Wang",
        "Jinfan Xu",
        "Sen Xing",
        "Chenyu Yang",
        "Hai Ye",
        "Jiaheng Yu",
        "Yue Yu",
        "Muyan Zhong",
        "Tianchen Zhao",
        "Xizhou Zhu",
        "Yanpeng Zhou",
        "Yifan Zhang",
        "Zhi Zhu"
      ],
      "summary": "We present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or context length, MiroThinker explores interaction scaling at the model level, systematically training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement. Unlike LLM test-time scaling, which operates in isolation and risks degradation with longer reasoning chains, interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories. Through reinforcement learning, the model achieves efficient interaction scaling: with a 256K context window, it can perform up to 600 tool calls per task, enabling sustained multi-turn reasoning and complex real-world research workflows. Across four representative benchmarks-GAIA, HLE, BrowseComp, and BrowseComp-ZH-the 72B variant achieves up to 81.9%, 37.7%, 47.1%, and 55.6% accuracy respectively, surpassing previous open-source agents and approaching commercial counterparts such as GPT-5-high. Our analysis reveals that MiroThinker benefits from interactive scaling consistently: research performance improves predictably as the model engages in deeper and more frequent agent-environment interactions, demonstrating that interaction depth exhibits scaling behaviors analogous to model size and context length. These findings establish interaction scaling as a third critical dimension for building next-generation open research agents, complementing model capacity and context windows.",
      "fetch_date": "2025-11-18",
      "num_comments": 4,
      "ai_summary": "",
      "ai_keywords": []
    }
  },
  {
    "id": "07bfc80d1c8c45f5b6088d438bb272d2",
    "source": "huggingface",
    "type": "paper",
    "title": "General Agentic Memory Via Deep Research",
    "description": "Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitati...<br/>Upvotes: 150<br/>GitHub Stars: 626<br/>Authors: B. Y. Yan, Chaofan Li, Hongjin Qian<br/>ðŸ”— <a href=\"https://github.com/VectorSpaceLab/general-agentic-memory\">GitHub</a><br/>ðŸ”— <a href=\"https://github.com/VectorSpaceLab/general-agentic-memory\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2511.18423\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2511.18423",
    "external_url": "https://github.com/VectorSpaceLab/general-agentic-memory",
    "published_date": "2025-11-23T07:29:33.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2511.18423",
      "upvotes": 150,
      "github_stars": 626,
      "github_url": "https://github.com/VectorSpaceLab/general-agentic-memory",
      "project_url": "https://github.com/VectorSpaceLab/general-agentic-memory",
      "authors": [
        "B. Y. Yan",
        "Chaofan Li",
        "Hongjin Qian",
        "Shuqi Lu",
        "Zheng Liu"
      ],
      "summary": "Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called general agentic memory (GAM). GAM follows the principle of \"just-in time (JIT) compilation\" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) Memorizer, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) Researcher, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.",
      "fetch_date": "2025-11-25",
      "num_comments": 2,
      "ai_summary": "GAM, a novel framework that employs JIT compilation principles, improves memory efficiency and task completion by leveraging a lightweight memorizer and researcher in conjunction with reinforcement learning.",
      "ai_keywords": [
        "general agentic memory",
        "GAM",
        "just-in time compilation",
        "JIT compilation",
        "memorizer",
        "researcher",
        "universal page-store",
        "large language models",
        "LLMs",
        "reinforcement learning"
      ]
    }
  },
  {
    "id": "f37a636c7cc5f515c6866d60bda27643",
    "source": "huggingface",
    "type": "paper",
    "title": "UFO^3: Weaving the Digital Agent Galaxy",
    "description": "Large language model (LLM)-powered agents are transforming digital devices from passive tools into proactive intelligent collaborators. However, most existing frameworks remain confined to a single OS...<br/>Upvotes: 18<br/>GitHub Stars: 7751<br/>Authors: Chaoyun Zhang, Liqun Li, He Huang<br/>ðŸ”— <a href=\"https://github.com/microsoft/UFO/\">GitHub</a><br/>ðŸ”— <a href=\"https://microsoft.github.io/UFO/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2511.11332\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2511.11332",
    "external_url": "https://github.com/microsoft/UFO/",
    "published_date": "2025-11-14T09:05:31.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2511.11332",
      "upvotes": 18,
      "github_stars": 7751,
      "github_url": "https://github.com/microsoft/UFO/",
      "project_url": "https://microsoft.github.io/UFO/",
      "authors": [
        "Chaoyun Zhang",
        "Liqun Li",
        "He Huang",
        "Chiming Ni",
        "Bo Qiao",
        "Si Qin",
        "Yu Kang",
        "Minghua Ma",
        "Qingwei Lin",
        "Saravan Rajmohan",
        "Dongmei Zhang"
      ],
      "summary": "Large language model (LLM)-powered agents are transforming digital devices from passive tools into proactive intelligent collaborators. However, most existing frameworks remain confined to a single OS or device, making cross-device workflows brittle and largely manual. We present UFO^3, a system that unifies heterogeneous endpoints, desktops, servers, mobile devices, and edge, into a single orchestration fabric. UFO^3 models each user request as a mutable TaskConstellation: a distributed DAG of atomic subtasks (TaskStars) with explicit control and data dependencies (TaskStarLines). The TaskConstellation continuously evolves as results stream in from distributed devices, enabling asynchronous execution, adaptive recovery, and dynamic optimization. A Constellation Orchestrator} executes tasks safely and asynchronously while applying dynamic DAG updates, and the Agent Interaction Protocol (AIP) provides persistent, low-latency channels for reliable task dispatch and result streaming. These designs dissolve the traditional boundaries between devices and platforms, allowing agents to collaborate seamlessly and amplify their collective intelligence.\n  We evaluate UFO^3 on NebulaBench, a benchmark of 55 cross-device tasks across 5 machines and 10 categories. UFO^3 achieves 83.3% subtask completion, 70.9% task success, exposes parallelism with an average width of 1.72, and reduces end-to-end latency by 31% relative to a sequential baseline. Fault-injection experiments demonstrate graceful degradation and recovery under transient and permanent agent failures. These results show that UFO^3 achieves accurate, efficient, and resilient task orchestration across heterogeneous devices, uniting isolated agents into a coherent, adaptive computing fabric that extends across the landscape of ubiquitous computing.",
      "fetch_date": "2025-11-18",
      "num_comments": 3,
      "ai_summary": "UFO$^3$ unifies heterogeneous devices into a single orchestration fabric, enabling seamless task collaboration and dynamic optimization across distributed environments.",
      "ai_keywords": [
        "TaskConstellation",
        "TaskStars",
        "TaskStarLines",
        "DAG",
        "Constellation Orchestrator",
        "Agent Interaction Protocol",
        "NebulaBench",
        "parallelism",
        "end-to-end latency",
        "fault-injection",
        "adaptive computing fabric",
        "ubiquitous computing"
      ]
    }
  },
  {
    "id": "3c094172f9e0e4e808f9c218d4d266f1",
    "source": "huggingface",
    "type": "paper",
    "title": "SAM 3: Segment Anything with Concepts",
    "description": "We present Segment Anything Model (SAM) 3, a unified model that detects, segments, and tracks objects in images and videos based on concept prompts, which we define as either short noun phrases (e.g.,...<br/>Upvotes: 98<br/>GitHub Stars: 4984<br/>Authors: Nicolas Carion, Laura Gustafson, Yuan-Ting Hu<br/>ðŸ”— <a href=\"https://github.com/facebookresearch/sam3\">GitHub</a><br/>ðŸ”— <a href=\"https://ai.meta.com/sam3/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2511.16719\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2511.16719",
    "external_url": "https://github.com/facebookresearch/sam3",
    "published_date": "2025-11-20T13:59:56.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2511.16719",
      "upvotes": 98,
      "github_stars": 4984,
      "github_url": "https://github.com/facebookresearch/sam3",
      "project_url": "https://ai.meta.com/sam3/",
      "authors": [
        "Nicolas Carion",
        "Laura Gustafson",
        "Yuan-Ting Hu",
        "Shoubhik Debnath",
        "Ronghang Hu",
        "Didac Suris",
        "Chaitanya Ryali",
        "Kalyan Vasudev Alwala",
        "Haitham Khedr",
        "Andrew Huang",
        "Jie Lei",
        "Tengyu Ma",
        "Baishan Guo",
        "Arpit Kalla",
        "Markus Marks",
        "Joseph Greer",
        "Meng Wang",
        "Peize Sun",
        "Roman RÃ¤dle",
        "Triantafyllos Afouras",
        "Effrosyni Mavroudi",
        "Katherine Xu",
        "Tsung-Han Wu",
        "Yu Zhou",
        "Liliane Momeni",
        "Rishi Hazra",
        "Shuangrui Ding",
        "Sagar Vaze",
        "Francois Porcher",
        "Feng Li",
        "Siyuan Li",
        "Aishwarya Kamath",
        "Ho Kei Cheng",
        "Piotr DollÃ¡r",
        "Nikhila Ravi",
        "Kate Saenko",
        "Pengchuan Zhang",
        "Christoph Feichtenhofer"
      ],
      "summary": "We present Segment Anything Model (SAM) 3, a unified model that detects, segments, and tracks objects in images and videos based on concept prompts, which we define as either short noun phrases (e.g., \"yellow school bus\"), image exemplars, or a combination of both. Promptable Concept Segmentation (PCS) takes such prompts and returns segmentation masks and unique identities for all matching object instances. To advance PCS, we build a scalable data engine that produces a high-quality dataset with 4M unique concept labels, including hard negatives, across images and videos. Our model consists of an image-level detector and a memory-based video tracker that share a single backbone. Recognition and localization are decoupled with a presence head, which boosts detection accuracy. SAM 3 doubles the accuracy of existing systems in both image and video PCS, and improves previous SAM capabilities on visual segmentation tasks. We open source SAM 3 along with our new Segment Anything with Concepts (SA-Co) benchmark for promptable concept segmentation.",
      "fetch_date": "2025-11-24",
      "num_comments": 4,
      "ai_summary": "Segment Anything Model 3 achieves state-of-the-art performance in promptable concept segmentation and tracking by leveraging a unified model architecture with decoupled recognition and localization.",
      "ai_keywords": [
        "Segment Anything Model",
        "Promptable Concept Segmentation",
        "concept prompts",
        "segmentation masks",
        "unique identities",
        "scalable data engine",
        "image-level detector",
        "memory-based video tracker",
        "presence head",
        "visual segmentation tasks",
        "Segment Anything with Concepts benchmark"
      ]
    }
  },
  {
    "id": "979bc288b697d0a635887e56b67befef",
    "source": "huggingface",
    "type": "paper",
    "title": "SAM 3D: 3Dfy Anything in Images",
    "description": "We present SAM 3D, a generative model for visually grounded 3D object reconstruction, predicting geometry, texture, and layout from a single image. SAM 3D excels in natural images, where occlusion and...<br/>Upvotes: 102<br/>GitHub Stars: 4339<br/>Authors: SAM 3D Team, Xingyu Chen, Fu-Jen Chu<br/>ðŸ”— <a href=\"https://github.com/facebookresearch/sam-3d-objects\">GitHub</a><br/>ðŸ”— <a href=\"https://ai.meta.com/sam3d/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2511.16624\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2511.16624",
    "external_url": "https://github.com/facebookresearch/sam-3d-objects",
    "published_date": "2025-11-20T13:31:46.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2511.16624",
      "upvotes": 102,
      "github_stars": 4339,
      "github_url": "https://github.com/facebookresearch/sam-3d-objects",
      "project_url": "https://ai.meta.com/sam3d/",
      "authors": [
        "SAM 3D Team",
        "Xingyu Chen",
        "Fu-Jen Chu",
        "Pierre Gleize",
        "Kevin J Liang",
        "Alexander Sax",
        "Hao Tang",
        "Weiyao Wang",
        "Michelle Guo",
        "Thibaut Hardin",
        "Xiang Li",
        "Aohan Lin",
        "Jiawei Liu",
        "Ziqi Ma",
        "Anushka Sagar",
        "Bowen Song",
        "Xiaodong Wang",
        "Jianing Yang",
        "Bowen Zhang",
        "Piotr DollÃ¡r",
        "Georgia Gkioxari",
        "Matt Feiszli",
        "Jitendra Malik"
      ],
      "summary": "We present SAM 3D, a generative model for visually grounded 3D object reconstruction, predicting geometry, texture, and layout from a single image. SAM 3D excels in natural images, where occlusion and scene clutter are common and visual recognition cues from context play a larger role. We achieve this with a human- and model-in-the-loop pipeline for annotating object shape, texture, and pose, providing visually grounded 3D reconstruction data at unprecedented scale. We learn from this data in a modern, multi-stage training framework that combines synthetic pretraining with real-world alignment, breaking the 3D \"data barrier\". We obtain significant gains over recent work, with at least a 5:1 win rate in human preference tests on real-world objects and scenes. We will release our code and model weights, an online demo, and a new challenging benchmark for in-the-wild 3D object reconstruction.",
      "fetch_date": "2025-11-21",
      "num_comments": 3,
      "ai_summary": "SAM 3D is a generative model that reconstructs 3D objects from single images using a multi-stage training framework that includes synthetic pretraining and real-world alignment, achieving high performance in human preference tests.",
      "ai_keywords": [
        "generative model",
        "3D object reconstruction",
        "geometry",
        "texture",
        "layout",
        "human-in-the-loop",
        "object shape",
        "object pose",
        "multi-stage training",
        "synthetic pretraining",
        "real-world alignment",
        "3D data barrier",
        "human preference tests",
        "benchmark"
      ]
    }
  },
  {
    "id": "56964b5c4b70a7c48e204a4541b8da35",
    "source": "huggingface",
    "type": "paper",
    "title": "Black-Box On-Policy Distillation of Large Language Models",
    "description": "Black-box distillation creates student large language models (LLMs) by learning from a proprietary teacher model's text outputs alone, without access to its internal logits or parameters. In this work...<br/>Upvotes: 46<br/>GitHub Stars: 4213<br/>Authors: Tianzhu Ye, Li Dong, Zewen Chi<br/>ðŸ”— <a href=\"https://github.com/microsoft/LMOps/tree/main/gad\">GitHub</a><br/>ðŸ”— <a href=\"https://aka.ms/GAD-project\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2511.10643\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2511.10643",
    "external_url": "https://github.com/microsoft/LMOps/tree/main/gad",
    "published_date": "2025-11-13T13:58:37.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2511.10643",
      "upvotes": 46,
      "github_stars": 4213,
      "github_url": "https://github.com/microsoft/LMOps/tree/main/gad",
      "project_url": "https://aka.ms/GAD-project",
      "authors": [
        "Tianzhu Ye",
        "Li Dong",
        "Zewen Chi",
        "Xun Wu",
        "Shaohan Huang",
        "Furu Wei"
      ],
      "summary": "Black-box distillation creates student large language models (LLMs) by learning from a proprietary teacher model's text outputs alone, without access to its internal logits or parameters. In this work, we introduce Generative Adversarial Distillation (GAD), which enables on-policy and black-box distillation. GAD frames the student LLM as a generator and trains a discriminator to distinguish its responses from the teacher LLM's, creating a minimax game. The discriminator acts as an on-policy reward model that co-evolves with the student, providing stable, adaptive feedback. Experimental results show that GAD consistently surpasses the commonly used sequence-level knowledge distillation. In particular, Qwen2.5-14B-Instruct (student) trained with GAD becomes comparable to its teacher, GPT-5-Chat, on the LMSYS-Chat automatic evaluation. The results establish GAD as a promising and effective paradigm for black-box LLM distillation.",
      "fetch_date": "2025-11-14",
      "num_comments": 3,
      "ai_summary": "Generative Adversarial Distillation (GAD) enhances black-box distillation by framing the student model as a generator and using a discriminator to provide adaptive feedback, surpassing traditional sequence-level knowledge distillation.",
      "ai_keywords": [
        "black-box distillation",
        "large language models (LLMs)",
        "Generative Adversarial Distillation (GAD)",
        "generator",
        "discriminator",
        "minimax game",
        "on-policy reward model",
        "sequence-level knowledge distillation",
        "LMSYS-Chat automatic evaluation"
      ]
    }
  },
  {
    "id": "8e9429d7feaa43a78c484db81c3eddf8",
    "source": "huggingface",
    "type": "paper",
    "title": "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer",
    "description": "The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Imag...<br/>Upvotes: 100<br/>GitHub Stars: 3879<br/>Authors: Z-Image Team, Huanqia Cai, Sihan Cao<br/>ðŸ”— <a href=\"https://github.com/Tongyi-MAI/Z-Image\">GitHub</a><br/>ðŸ”— <a href=\"https://tongyi-mai.github.io/Z-Image-blog/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2511.22699\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2511.22699",
    "external_url": "https://github.com/Tongyi-MAI/Z-Image",
    "published_date": "2025-11-27T13:52:07.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2511.22699",
      "upvotes": 100,
      "github_stars": 3879,
      "github_url": "https://github.com/Tongyi-MAI/Z-Image",
      "project_url": "https://tongyi-mai.github.io/Z-Image-blog/",
      "authors": [
        "Z-Image Team",
        "Huanqia Cai",
        "Sihan Cao",
        "Ruoyi Du",
        "Peng Gao",
        "Steven Hoi",
        "Shijie Huang",
        "Zhaohui Hou",
        "Dengyang Jiang",
        "Xin Jin",
        "Liangchen Li",
        "Zhen Li",
        "Zhong-Yu Li",
        "David Liu",
        "Dongyang Liu",
        "Junhan Shi",
        "Qilong Wu",
        "Feng Yu",
        "Chi Zhang",
        "Shifeng Zhang",
        "Shilin Zhou"
      ],
      "summary": "The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the \"scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.",
      "fetch_date": "2025-12-01",
      "num_comments": 2,
      "ai_summary": "Z-Image, a 6B-parameter Scalable Single-Stream Diffusion Transformer (S3-DiT) model, achieves high-performance image generation with reduced computational cost, offering sub-second inference and compatibility with consumer hardware.",
      "ai_keywords": [
        "Scalable Single-Stream Diffusion Transformer",
        "S3-DiT",
        "diffusion transformer",
        "omni-pre-training",
        "instruction-following capabilities",
        "photorealistic image generation",
        "bilingual text rendering",
        "distillation scheme",
        "reward post-training",
        "H800 GPU",
        "VRAM"
      ]
    }
  },
  {
    "id": "18503dda198b1fcbf7bb01dbf7296d31",
    "source": "huggingface",
    "type": "paper",
    "title": "Decoupled DMD: CFG Augmentation as the Spear, Distribution Matching as the Shield",
    "description": "Diffusion model distillation has emerged as a powerful technique for creating efficient few-step and single-step generators. Among these, Distribution Matching Distillation (DMD) and its variants stan...<br/>Upvotes: 13<br/>GitHub Stars: 3813<br/>Authors: Dongyang Liu, Peng Gao, David Liu<br/>ðŸ”— <a href=\"https://github.com/Tongyi-MAI/Z-Image/tree/main\">GitHub</a><br/>ðŸ”— <a href=\"https://tongyi-mai.github.io/Z-Image-blog/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2511.22677\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2511.22677",
    "external_url": "https://github.com/Tongyi-MAI/Z-Image/tree/main",
    "published_date": "2025-11-27T13:24:28.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2511.22677",
      "upvotes": 13,
      "github_stars": 3813,
      "github_url": "https://github.com/Tongyi-MAI/Z-Image/tree/main",
      "project_url": "https://tongyi-mai.github.io/Z-Image-blog/",
      "authors": [
        "Dongyang Liu",
        "Peng Gao",
        "David Liu",
        "Ruoyi Du",
        "Zhen Li",
        "Qilong Wu",
        "Xin Jin",
        "Sihan Cao",
        "Shifeng Zhang",
        "Hongsheng Li",
        "Steven Hoi"
      ],
      "summary": "Diffusion model distillation has emerged as a powerful technique for creating efficient few-step and single-step generators. Among these, Distribution Matching Distillation (DMD) and its variants stand out for their impressive performance, which is widely attributed to their core mechanism of matching the student's output distribution to that of a pre-trained teacher model. In this work, we challenge this conventional understanding. Through a rigorous decomposition of the DMD training objective, we reveal that in complex tasks like text-to-image generation, where CFG is typically required for desirable few-step performance, the primary driver of few-step distillation is not distribution matching, but a previously overlooked component we identify as CFG Augmentation (CA). We demonstrate that this term acts as the core ``engine'' of distillation, while the Distribution Matching (DM) term functions as a ``regularizer'' that ensures training stability and mitigates artifacts. We further validate this decoupling by demonstrating that while the DM term is a highly effective regularizer, it is not unique; simpler non-parametric constraints or GAN-based objectives can serve the same stabilizing function, albeit with different trade-offs. This decoupling of labor motivates a more principled analysis of the properties of both terms, leading to a more systematic and in-depth understanding. This new understanding further enables us to propose principled modifications to the distillation process, such as decoupling the noise schedules for the engine and the regularizer, leading to further performance gains. Notably, our method has been adopted by the Z-Image ( https://github.com/Tongyi-MAI/Z-Image ) project to develop a top-tier 8-step image generation model, empirically validating the generalization and robustness of our findings.",
      "fetch_date": "2025-12-01",
      "num_comments": 2,
      "ai_summary": "The study reveals that in text-to-image generation, CFG Augmentation is the primary driver of few-step distillation in Distribution Matching Distillation (DMD), while the distribution matching term acts as a regularizer.",
      "ai_keywords": [
        "diffusion model distillation",
        "Distribution Matching Distillation (DMD)",
        "CFG Augmentation (CA)",
        "text-to-image generation",
        "configuration (CFG)",
        "distribution matching",
        "noise schedules",
        "Z-Image"
      ]
    }
  }
]