[
  {
    "id": "7a9ae43171e964d5b9a58c9a76f8c373",
    "source": "huggingface",
    "type": "paper",
    "title": "The Dragon Hatchling: The Missing Link between the Transformer and\n  Models of the Brain",
    "description": "The relationship between computing systems and the brain has served as\nmotivation for pioneering theoreticians since John von Neumann and Alan Turing.\nUniform, scale-free biological networks, such as ...<br/>Upvotes: 496<br/>GitHub Stars: 3126<br/>Authors: Adrian Kosowski, PrzemysÅ‚aw UznaÅ„ski, Jan Chorowski<br/>ðŸ”— <a href=\"https://github.com/pathwaycom/bdh\">GitHub</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2509.26507\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2509.26507",
    "external_url": "https://github.com/pathwaycom/bdh",
    "published_date": "2025-09-30T12:49:01.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2509.26507",
      "upvotes": 496,
      "github_stars": 3126,
      "github_url": "https://github.com/pathwaycom/bdh",
      "project_url": "",
      "authors": [
        "Adrian Kosowski",
        "PrzemysÅ‚aw UznaÅ„ski",
        "Jan Chorowski",
        "Zuzanna Stamirowska",
        "MichaÅ‚ Bartoszkiewicz"
      ],
      "summary": "The relationship between computing systems and the brain has served as\nmotivation for pioneering theoreticians since John von Neumann and Alan Turing.\nUniform, scale-free biological networks, such as the brain, have powerful\nproperties, including generalizing over time, which is the main barrier for\nMachine Learning on the path to Universal Reasoning Models.\n  We introduce `Dragon Hatchling' (BDH), a new Large Language Model\narchitecture based on a scale-free biologically inspired network of \\n\nlocally-interacting neuron particles. BDH couples strong theoretical\nfoundations and inherent interpretability without sacrificing Transformer-like\nperformance.\n  BDH is a practical, performant state-of-the-art attention-based state space\nsequence learning architecture. In addition to being a graph model, BDH admits\na GPU-friendly formulation. It exhibits Transformer-like scaling laws:\nempirically BDH rivals GPT2 performance on language and translation tasks, at\nthe same number of parameters (10M to 1B), for the same training data.\n  BDH can be represented as a brain model. The working memory of BDH during\ninference entirely relies on synaptic plasticity with Hebbian learning using\nspiking neurons. We confirm empirically that specific, individual synapses\nstrengthen connection whenever BDH hears or reasons about a specific concept\nwhile processing language inputs. The neuron interaction network of BDH is a\ngraph of high modularity with heavy-tailed degree distribution. The BDH model\nis biologically plausible, explaining one possible mechanism which human\nneurons could use to achieve speech.\n  BDH is designed for interpretability. Activation vectors of BDH are sparse\nand positive. We demonstrate monosemanticity in BDH on language tasks.\nInterpretability of state, which goes beyond interpretability of neurons and\nmodel parameters, is an inherent feature of the BDH architecture.",
      "fetch_date": "2025-10-01",
      "num_comments": 20,
      "ai_summary": "BDH, a biologically inspired Large Language Model, combines scale-free network architecture with Hebbian learning to achieve Transformer-like performance while maintaining interpretability.",
      "ai_keywords": [
        "Large Language Model",
        "scale-free network",
        "Hebbian learning",
        "synaptic plasticity",
        "spiking neurons",
        "graph model",
        "GPU-friendly",
        "Transformer-like scaling laws",
        "monosemanticity",
        "interpretability"
      ]
    }
  },
  {
    "id": "3d55a02b14bdc7a04256ce32475eb075",
    "source": "huggingface",
    "type": "paper",
    "title": "Less is More: Recursive Reasoning with Tiny Networks",
    "description": "Hierarchical Reasoning Model (HRM) is a novel approach using two small neural\nnetworks recursing at different frequencies. This biologically inspired method\nbeats Large Language models (LLMs) on hard ...<br/>Upvotes: 423<br/>GitHub Stars: 5016<br/>Authors: Alexia Jolicoeur-Martineau<br/>ðŸ”— <a href=\"https://github.com/SamsungSAILMontreal/TinyRecursiveModels\">GitHub</a><br/>ðŸ”— <a href=\"https://alexiajm.github.io/2025/09/29/tiny_recursive_models.html#\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2510.04871\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2510.04871",
    "external_url": "https://github.com/SamsungSAILMontreal/TinyRecursiveModels",
    "published_date": "2025-10-06T10:58:08.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2510.04871",
      "upvotes": 423,
      "github_stars": 5016,
      "github_url": "https://github.com/SamsungSAILMontreal/TinyRecursiveModels",
      "project_url": "https://alexiajm.github.io/2025/09/29/tiny_recursive_models.html#",
      "authors": [
        "Alexia Jolicoeur-Martineau"
      ],
      "summary": "Hierarchical Reasoning Model (HRM) is a novel approach using two small neural\nnetworks recursing at different frequencies. This biologically inspired method\nbeats Large Language models (LLMs) on hard puzzle tasks such as Sudoku, Maze,\nand ARC-AGI while trained with small models (27M parameters) on small data\n(around 1000 examples). HRM holds great promise for solving hard problems with\nsmall networks, but it is not yet well understood and may be suboptimal. We\npropose Tiny Recursive Model (TRM), a much simpler recursive reasoning approach\nthat achieves significantly higher generalization than HRM, while using a\nsingle tiny network with only 2 layers. With only 7M parameters, TRM obtains\n45% test-accuracy on ARC-AGI-1 and 8% on ARC-AGI-2, higher than most LLMs\n(e.g., Deepseek R1, o3-mini, Gemini 2.5 Pro) with less than 0.01% of the\nparameters.",
      "fetch_date": "2025-10-08",
      "num_comments": 32,
      "ai_summary": "Tiny Recursive Model (TRM) achieves high generalization on complex puzzle tasks using a small, two-layer network with minimal parameters, outperforming larger language models.",
      "ai_keywords": [
        "Hierarchical Reasoning Model",
        "HRM",
        "Tiny Recursive Model",
        "TRM",
        "recursive reasoning",
        "neural networks",
        "ARC-AGI",
        "Deepseek R1",
        "o3-mini",
        "Gemini 2.5 Pro"
      ]
    }
  },
  {
    "id": "b5eadc2228eda785fe3e8da98ea25174",
    "source": "huggingface",
    "type": "paper",
    "title": "Agent Learning via Early Experience",
    "description": "A long-term goal of language agents is to learn and improve through their own\nexperience, ultimately outperforming humans in complex, real-world tasks.\nHowever, training agents from experience data wi...<br/>Upvotes: 233<br/>Authors: Kai Zhang, Xiangchao Chen, Bo Liu<br/>ðŸ”— <a href=\"https://huggingface.co/papers/2510.08558\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2510.08558",
    "external_url": "",
    "published_date": "2025-10-09T13:59:17.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2510.08558",
      "upvotes": 233,
      "github_stars": 0,
      "github_url": "",
      "project_url": "",
      "authors": [
        "Kai Zhang",
        "Xiangchao Chen",
        "Bo Liu",
        "Tianci Xue",
        "Zeyi Liao",
        "Zhihan Liu",
        "Xiyao Wang",
        "Yuting Ning",
        "Zhaorun Chen",
        "Xiaohan Fu",
        "Jian Xie",
        "Yuxuan Sun",
        "Boyu Gou",
        "Qi Qi",
        "Zihang Meng",
        "Jianwei Yang",
        "Ning Zhang",
        "Xian Li",
        "Ashish Shah",
        "Dat Huynh",
        "Hengduo Li",
        "Zi Yang",
        "Sara Cao",
        "Lawrence Jang",
        "Shuyan Zhou",
        "Jiacheng Zhu",
        "Huan Sun",
        "Jason Weston",
        "Yu Su",
        "Yifan Wu"
      ],
      "summary": "A long-term goal of language agents is to learn and improve through their own\nexperience, ultimately outperforming humans in complex, real-world tasks.\nHowever, training agents from experience data with reinforcement learning\nremains difficult in many environments, which either lack verifiable rewards\n(e.g., websites) or require inefficient long-horizon rollouts (e.g., multi-turn\ntool use). As a result, most current agents rely on supervised fine-tuning on\nexpert data, which is challenging to scale and generalizes poorly. This\nlimitation stems from the nature of expert demonstrations: they capture only a\nnarrow range of scenarios and expose the agent to limited environment\ndiversity. We address this limitation with a middle-ground paradigm we call\nearly experience: interaction data generated by the agent's own actions, where\nthe resulting future states serve as supervision without reward signals. Within\nthis paradigm we study two strategies of using such data: (1) Implicit world\nmodeling, which uses collected states to ground the policy in environment\ndynamics; and (2) Self-reflection, where the agent learns from its suboptimal\nactions to improve reasoning and decision-making. We evaluate across eight\ndiverse environments and multiple model families. Our approaches consistently\nimprove effectiveness and out-of-domain generalization, highlighting the value\nof early experience. Moreover, in environments with verifiable rewards, our\nresults provide promising signals that early experience offers a strong\nfoundation for subsequent reinforcement learning, positioning it as a practical\nbridge between imitation learning and fully experience-driven agents.",
      "fetch_date": "2025-10-10",
      "num_comments": 9,
      "ai_summary": "Early experience, using agent-generated interaction data without reward signals, improves policy effectiveness and generalization, serving as a bridge between imitation learning and reinforcement learning.",
      "ai_keywords": [
        "reinforcement learning",
        "early experience",
        "implicit world modeling",
        "self-reflection",
        "out-of-domain generalization"
      ]
    }
  },
  {
    "id": "4618188e5babf65ab96739c065ddfd61",
    "source": "huggingface",
    "type": "paper",
    "title": "LongLive: Real-time Interactive Long Video Generation",
    "description": "We present LongLive, a frame-level autoregressive (AR) framework for\nreal-time and interactive long video generation. Long video generation presents\nchallenges in both efficiency and quality. Diffusio...<br/>Upvotes: 176<br/>GitHub Stars: 722<br/>Authors: Shuai Yang, Wei Huang, Ruihang Chu<br/>ðŸ”— <a href=\"https://github.com/NVlabs/LongLive\">GitHub</a><br/>ðŸ”— <a href=\"https://nvlabs.github.io/LongLive/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2509.22622\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2509.22622",
    "external_url": "https://github.com/NVlabs/LongLive",
    "published_date": "2025-09-26T13:48:24.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2509.22622",
      "upvotes": 176,
      "github_stars": 722,
      "github_url": "https://github.com/NVlabs/LongLive",
      "project_url": "https://nvlabs.github.io/LongLive/",
      "authors": [
        "Shuai Yang",
        "Wei Huang",
        "Ruihang Chu",
        "Yicheng Xiao",
        "Yuyang Zhao",
        "Xianbang Wang",
        "Muyang Li",
        "Enze Xie",
        "Yingcong Chen",
        "Yao Lu",
        "Song Han",
        "Yukang Chen"
      ],
      "summary": "We present LongLive, a frame-level autoregressive (AR) framework for\nreal-time and interactive long video generation. Long video generation presents\nchallenges in both efficiency and quality. Diffusion and Diffusion-Forcing\nmodels can produce high-quality videos but suffer from low efficiency due to\nbidirectional attention. Causal attention AR models support KV caching for\nfaster inference, but often degrade in quality on long videos due to memory\nchallenges during long-video training. In addition, beyond static prompt-based\ngeneration, interactive capabilities, such as streaming prompt inputs, are\ncritical for dynamic content creation, enabling users to guide narratives in\nreal time. This interactive requirement significantly increases complexity,\nespecially in ensuring visual consistency and semantic coherence during prompt\ntransitions. To address these challenges, LongLive adopts a causal, frame-level\nAR design that integrates a KV-recache mechanism that refreshes cached states\nwith new prompts for smooth, adherent switches; streaming long tuning to enable\nlong video training and to align training and inference (train-long-test-long);\nand short window attention paired with a frame-level attention sink, shorten as\nframe sink, preserving long-range consistency while enabling faster generation.\nWith these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model\nto minute-long generation in just 32 GPU-days. At inference, LongLive sustains\n20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both\nshort and long videos. LongLive supports up to 240-second videos on a single\nH100 GPU. LongLive further supports INT8-quantized inference with only marginal\nquality loss.",
      "fetch_date": "2025-09-29",
      "num_comments": 2,
      "ai_summary": "LongLive is a frame-level autoregressive framework for real-time and interactive long video generation, addressing efficiency and quality challenges through causal attention, KV-recache, streaming long tuning, and short window attention.",
      "ai_keywords": [
        "frame-level autoregressive",
        "diffusion models",
        "diffusion-forcing models",
        "bidirectional attention",
        "causal attention",
        "KV caching",
        "interactive capabilities",
        "streaming prompt inputs",
        "KV-recache mechanism",
        "streaming long tuning",
        "short window attention",
        "frame-level attention sink",
        "frame sink",
        "long-range consistency",
        "VBench",
        "INT8-quantized inference"
      ]
    }
  },
  {
    "id": "652276de74e8a80281a665d98165228d",
    "source": "huggingface",
    "type": "paper",
    "title": "MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP\n  Use",
    "description": "MCP standardizes how LLMs interact with external systems, forming the\nfoundation for general agents. However, existing MCP benchmarks remain narrow\nin scope: they focus on read-heavy tasks or tasks wi...<br/>Upvotes: 165<br/>GitHub Stars: 285<br/>Authors: Zijian Wu, Xiangyan Liu, Xinyuan Zhang<br/>ðŸ”— <a href=\"https://github.com/eval-sys/mcpmark\">GitHub</a><br/>ðŸ”— <a href=\"https://mcpmark.ai/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2509.24002\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2509.24002",
    "external_url": "https://github.com/eval-sys/mcpmark",
    "published_date": "2025-09-28T13:53:27.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2509.24002",
      "upvotes": 165,
      "github_stars": 285,
      "github_url": "https://github.com/eval-sys/mcpmark",
      "project_url": "https://mcpmark.ai/",
      "authors": [
        "Zijian Wu",
        "Xiangyan Liu",
        "Xinyuan Zhang",
        "Lingjun Chen",
        "Fanqing Meng",
        "Lingxiao Du",
        "Yiran Zhao",
        "Fanshi Zhang",
        "Yaoqi Ye",
        "Jiawei Wang",
        "Zirui Wang",
        "Jinjie Ni",
        "Yufan Yang",
        "Arvin Xu",
        "Michael Qizhe Shieh"
      ],
      "summary": "MCP standardizes how LLMs interact with external systems, forming the\nfoundation for general agents. However, existing MCP benchmarks remain narrow\nin scope: they focus on read-heavy tasks or tasks with limited interaction\ndepth, and fail to capture the complexity and realism of real-world workflows.\nTo address this gap, we propose MCPMark, a benchmark designed to evaluate MCP\nuse in a more realistic and comprehensive manner. It consists of 127\nhigh-quality tasks collaboratively created by domain experts and AI agents.\nEach task begins with a curated initial state and includes a programmatic\nscript for automatic verification. These tasks demand richer and more diverse\ninteractions with the environment, involving a broad range of create, read,\nupdate, and delete (CRUD) operations. We conduct a comprehensive evaluation of\ncutting-edge LLMs using a minimal agent framework that operates in a\ntool-calling loop. Empirical results show that the best-performing model,\ngpt-5-medium, reaches only 52.56\\% pass@1 and 33.86\\% pass^4, while other\nwidely regarded strong models, including claude-sonnet-4 and o3, fall below\n30\\% pass@1 and 15\\% pass^4. On average, LLMs require 16.2 execution\nturns and 17.4 tool calls per task, significantly surpassing those in\nprevious MCP benchmarks and highlighting the stress-testing nature of MCPMark.",
      "fetch_date": "2025-10-01",
      "num_comments": 8,
      "ai_summary": "MCPMark is a comprehensive benchmark for evaluating MCP use in real-world workflows, featuring diverse tasks that require richer interactions with the environment, and reveals that current LLMs perform poorly on these tasks.",
      "ai_keywords": [
        "MCP",
        "LLMs",
        "general agents",
        "MCP benchmarks",
        "MCPMark",
        "high-quality tasks",
        "domain experts",
        "AI agents",
        "initial state",
        "programmatic script",
        "automatic verification",
        "CRUD operations",
        "minimal agent framework",
        "tool-calling loop",
        "gpt-5-medium",
        "claude-sonnet-4",
        "o3",
        "pass@1",
        "pass^4",
        "execution turns",
        "tool calls"
      ]
    }
  },
  {
    "id": "f5ecd82d2d6a90e1b4eb7c0d2afe9c3e",
    "source": "huggingface",
    "type": "paper",
    "title": "QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning\n  for LLMs",
    "description": "We propose QeRL, a Quantization-enhanced Reinforcement Learning framework for\nlarge language models (LLMs). While RL is essential for LLMs' reasoning\ncapabilities, it is resource-intensive, requiring ...<br/>Upvotes: 164<br/>GitHub Stars: 350<br/>Authors: Wei Huang, Yi Ge, Shuai Yang<br/>ðŸ”— <a href=\"https://github.com/NVlabs/QeRL\">GitHub</a><br/>ðŸ”— <a href=\"https://github.com/NVlabs/QeRL\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2510.11696\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2510.11696",
    "external_url": "https://github.com/NVlabs/QeRL",
    "published_date": "2025-10-13T13:55:09.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2510.11696",
      "upvotes": 164,
      "github_stars": 350,
      "github_url": "https://github.com/NVlabs/QeRL",
      "project_url": "https://github.com/NVlabs/QeRL",
      "authors": [
        "Wei Huang",
        "Yi Ge",
        "Shuai Yang",
        "Yicheng Xiao",
        "Huizi Mao",
        "Yujun Lin",
        "Hanrong Ye",
        "Sifei Liu",
        "Ka Chun Cheung",
        "Hongxu Yin",
        "Yao Lu",
        "Xiaojuan Qi",
        "Song Han",
        "Yukang Chen"
      ],
      "summary": "We propose QeRL, a Quantization-enhanced Reinforcement Learning framework for\nlarge language models (LLMs). While RL is essential for LLMs' reasoning\ncapabilities, it is resource-intensive, requiring substantial GPU memory and\nlong rollout durations. QeRL addresses these issues by combining NVFP4\nquantization with Low-Rank Adaptation (LoRA), accelerating rollout phase of RL\nwhile reducing memory overhead. Beyond efficiency, our findings show that\nquantization noise increases policy entropy, enhancing exploration, and\nenabling the discovery of better strategies during RL. To further optimize\nexploration, QeRL introduces an Adaptive Quantization Noise (AQN) mechanism,\nwhich dynamically adjusts noise during training. Experiments demonstrate that\nQeRL delivers over 1.5 times speedup in the rollout phase. Moreover, this is\nthe first framework to enable RL training of a 32B LLM on a single H100 80GB\nGPU, while delivering overall speedups for RL training. It also achieves faster\nreward growth and higher final accuracy than 16-bit LoRA and QLoRA, while\nmatching the performance of full-parameter fine-tuning on mathematical\nbenchmarks such as GSM8K (90.8%) and MATH 500 (77.4%) in the 7B model. These\nresults establish QeRL as an efficient and effective framework for RL training\nin LLMs.",
      "fetch_date": "2025-10-14",
      "num_comments": 3,
      "ai_summary": "QeRL, a quantization-enhanced reinforcement learning framework, accelerates RL training for large language models by combining NVFP4 quantization with Low-Rank Adaptation and an Adaptive Quantization Noise mechanism, achieving significant speedups and improved performance.",
      "ai_keywords": [
        "NVFP4 quantization",
        "Low-Rank Adaptation (LoRA)",
        "Adaptive Quantization Noise (AQN)",
        "reinforcement learning",
        "large language models (LLMs)",
        "rollout phase",
        "policy entropy",
        "exploration",
        "reward growth",
        "GSM8K",
        "MATH 500"
      ]
    }
  },
  {
    "id": "d98bf11f4930fd1159bb5f90e3f76afd",
    "source": "huggingface",
    "type": "paper",
    "title": "PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B\n  Ultra-Compact Vision-Language Model",
    "description": "In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model\ntailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a\ncompact yet powerful vision-language model (VLM) ...<br/>Upvotes: 51<br/>GitHub Stars: 60005<br/>Authors: Cheng Cui, Ting Sun, Suyin Liang<br/>ðŸ”— <a href=\"https://github.com/PaddlePaddle/PaddleOCR\">GitHub</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2510.14528\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2510.14528",
    "external_url": "https://github.com/PaddlePaddle/PaddleOCR",
    "published_date": "2025-10-16T06:18:48.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2510.14528",
      "upvotes": 51,
      "github_stars": 60005,
      "github_url": "https://github.com/PaddlePaddle/PaddleOCR",
      "project_url": "",
      "authors": [
        "Cheng Cui",
        "Ting Sun",
        "Suyin Liang",
        "Tingquan Gao",
        "Zelun Zhang",
        "Jiaxuan Liu",
        "Xueqing Wang",
        "Changda Zhou",
        "Hongen Liu",
        "Manhui Lin",
        "Yue Zhang",
        "Yubo Zhang",
        "Handong Zheng",
        "Jing Zhang",
        "Jun Zhang",
        "Yi Liu",
        "Dianhai Yu",
        "Yanjun Ma"
      ],
      "summary": "In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model\ntailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a\ncompact yet powerful vision-language model (VLM) that integrates a NaViT-style\ndynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to\nenable accurate element recognition. This innovative model efficiently supports\n109 languages and excels in recognizing complex elements (e.g., text, tables,\nformulas, and charts), while maintaining minimal resource consumption. Through\ncomprehensive evaluations on widely used public benchmarks and in-house\nbenchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document\nparsing and element-level recognition. It significantly outperforms existing\nsolutions, exhibits strong competitiveness against top-tier VLMs, and delivers\nfast inference speeds. These strengths make it highly suitable for practical\ndeployment in real-world scenarios.",
      "fetch_date": "2025-10-17",
      "num_comments": 5,
      "ai_summary": "PaddleOCR-VL, a vision-language model combining NaViT-style visual encoder and ERNIE-4.5 language model, achieves state-of-the-art performance in document parsing with minimal resource consumption.",
      "ai_keywords": [
        "vision-language model",
        "NaViT-style",
        "dynamic resolution visual encoder",
        "ERNIE-4.5",
        "element recognition",
        "page-level document parsing",
        "element-level recognition",
        "inference speeds"
      ]
    }
  },
  {
    "id": "b1fe8132db01bc9650eda3e26023bc30",
    "source": "huggingface",
    "type": "paper",
    "title": "MinerU2.5: A Decoupled Vision-Language Model for Efficient\n  High-Resolution Document Parsing",
    "description": "We introduce MinerU2.5, a 1.2B-parameter document parsing vision-language\nmodel that achieves state-of-the-art recognition accuracy while maintaining\nexceptional computational efficiency. Our approach...<br/>Upvotes: 120<br/>GitHub Stars: 47161<br/>Authors: Junbo Niu, Zheng Liu, Zhuangcheng Gu<br/>ðŸ”— <a href=\"https://github.com/opendatalab/MinerU\">GitHub</a><br/>ðŸ”— <a href=\"https://opendatalab.github.io/MinerU/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2509.22186\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2509.22186",
    "external_url": "https://github.com/opendatalab/MinerU",
    "published_date": "2025-09-26T06:45:48.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2509.22186",
      "upvotes": 120,
      "github_stars": 47161,
      "github_url": "https://github.com/opendatalab/MinerU",
      "project_url": "https://opendatalab.github.io/MinerU/",
      "authors": [
        "Junbo Niu",
        "Zheng Liu",
        "Zhuangcheng Gu",
        "Bin Wang",
        "Linke Ouyang",
        "Zhiyuan Zhao",
        "Tao Chu",
        "Tianyao He",
        "Fan Wu",
        "Qintong Zhang",
        "Zhenjiang Jin",
        "Guang Liang",
        "Rui Zhang",
        "Wenzheng Zhang",
        "Yuan Qu",
        "Zhifei Ren",
        "Yuefeng Sun",
        "Yuanhong Zheng",
        "Dongsheng Ma",
        "Zirui Tang",
        "Boyu Niu",
        "Ziyang Miao",
        "Hejun Dong",
        "Siyi Qian",
        "Junyuan Zhang",
        "Jingzhou Chen",
        "Fangdong Wang",
        "Xiaomeng Zhao",
        "Liqun Wei",
        "Wei Li",
        "Shasha Wang",
        "Ruiliang Xu",
        "Yuanyuan Cao",
        "Lu Chen",
        "Qianqian Wu",
        "Huaiyu Gu",
        "Lindong Lu",
        "Keming Wang",
        "Dechen Lin",
        "Guanlin Shen",
        "Xuanhe Zhou",
        "Linfeng Zhang",
        "Yuhang Zang",
        "Xiaoyi Dong",
        "Jiaqi Wang",
        "Bo Zhang",
        "Lei Bai",
        "Pei Chu",
        "Weijia Li",
        "Jiang Wu",
        "Lijun Wu",
        "Zhenxiang Li",
        "Guangyu Wang",
        "Zhongying Tu",
        "Chao Xu",
        "Kai Chen",
        "Yu Qiao",
        "Bowen Zhou",
        "Dahua Lin",
        "Wentao Zhang",
        "Conghui He"
      ],
      "summary": "We introduce MinerU2.5, a 1.2B-parameter document parsing vision-language\nmodel that achieves state-of-the-art recognition accuracy while maintaining\nexceptional computational efficiency. Our approach employs a coarse-to-fine,\ntwo-stage parsing strategy that decouples global layout analysis from local\ncontent recognition. In the first stage, the model performs efficient layout\nanalysis on downsampled images to identify structural elements, circumventing\nthe computational overhead of processing high-resolution inputs. In the second\nstage, guided by the global layout, it performs targeted content recognition on\nnative-resolution crops extracted from the original image, preserving\nfine-grained details in dense text, complex formulas, and tables. To support\nthis strategy, we developed a comprehensive data engine that generates diverse,\nlarge-scale training corpora for both pretraining and fine-tuning. Ultimately,\nMinerU2.5 demonstrates strong document parsing ability, achieving\nstate-of-the-art performance on multiple benchmarks, surpassing both\ngeneral-purpose and domain-specific models across various recognition tasks,\nwhile maintaining significantly lower computational overhead.",
      "fetch_date": "2025-09-29",
      "num_comments": 2,
      "ai_summary": "MinerU2.5, a 1.2B-parameter document parsing vision-language model, achieves state-of-the-art recognition accuracy with computational efficiency through a coarse-to-fine parsing strategy.",
      "ai_keywords": [
        "document parsing",
        "vision-language model",
        "coarse-to-fine",
        "two-stage parsing",
        "layout analysis",
        "content recognition",
        "downsampled images",
        "native-resolution crops",
        "data engine",
        "pretraining",
        "fine-tuning",
        "state-of-the-art performance",
        "computational overhead"
      ]
    }
  },
  {
    "id": "4477d6ceb0f6af4c535f87fb4c3e55f5",
    "source": "huggingface",
    "type": "paper",
    "title": "BitNet Distillation",
    "description": "In this paper, we present BitNet Distillation (BitDistill), a lightweight\npipeline that fine-tunes off-the-shelf full-precision LLMs (e.g., Qwen) into\n1.58-bit precision (i.e., ternary weights {-1, 0,...<br/>Upvotes: 44<br/>GitHub Stars: 24269<br/>Authors: Xun Wu, Shaohan Huang, Wenhui Wang<br/>ðŸ”— <a href=\"https://github.com/microsoft/BitNet\">GitHub</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2510.13998\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2510.13998",
    "external_url": "https://github.com/microsoft/BitNet",
    "published_date": "2025-10-15T14:28:12.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2510.13998",
      "upvotes": 44,
      "github_stars": 24269,
      "github_url": "https://github.com/microsoft/BitNet",
      "project_url": "",
      "authors": [
        "Xun Wu",
        "Shaohan Huang",
        "Wenhui Wang",
        "Ting Song",
        "Li Dong",
        "Yan Xia",
        "Furu Wei"
      ],
      "summary": "In this paper, we present BitNet Distillation (BitDistill), a lightweight\npipeline that fine-tunes off-the-shelf full-precision LLMs (e.g., Qwen) into\n1.58-bit precision (i.e., ternary weights {-1, 0, 1}) for specific downstream\ntasks, achieving strong task-specific performance with minimal computational\ncost. Specifically, BitDistill incorporates three key techniques: the SubLN\nmodule, as introduced in BitNet; multi-head attention distillation, based on\nMiniLM; and continual pre-training, which serves as a crucial warm-up step to\nmitigate the scalability issue of the performance gap between finetuned\nfull-precision and 1.58-bit LLMs on specific tasks. Experimental results show\nthat BitDistill achieves performance comparable to the full-precision\ncounterpart models across model size, while enabling up to 10x memory savings\nand 2.65x faster inference on CPUs. Code is available at\nhttps://github.com/microsoft/BitNet.",
      "fetch_date": "2025-10-17",
      "num_comments": 3,
      "ai_summary": "BitNet Distillation fine-tunes large language models to 1.58-bit precision using SubLN, multi-head attention distillation, and continual pre-training, achieving comparable performance with significant memory and inference speed improvements.",
      "ai_keywords": [
        "BitNet Distillation",
        "BitDistill",
        "SubLN",
        "multi-head attention distillation",
        "continual pre-training",
        "LLMs",
        "Qwen",
        "ternary weights",
        "memory savings",
        "inference speed"
      ]
    }
  },
  {
    "id": "59f26db9f2ae0d3d43973c1ffbd40568",
    "source": "huggingface",
    "type": "paper",
    "title": "MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and\n  Training Recipe",
    "description": "Multimodal Large Language Models (MLLMs) are undergoing rapid progress and\nrepresent the frontier of AI development. However, their training and inference\nefficiency have emerged as a core bottleneck ...<br/>Upvotes: 48<br/>GitHub Stars: 22101<br/>Authors: Tianyu Yu, Zefan Wang, Chongyi Wang<br/>ðŸ”— <a href=\"https://github.com/OpenBMB/MiniCPM-V\">GitHub</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2509.18154\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2509.18154",
    "external_url": "https://github.com/OpenBMB/MiniCPM-V",
    "published_date": "2025-09-16T15:41:48.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2509.18154",
      "upvotes": 48,
      "github_stars": 22101,
      "github_url": "https://github.com/OpenBMB/MiniCPM-V",
      "project_url": "",
      "authors": [
        "Tianyu Yu",
        "Zefan Wang",
        "Chongyi Wang",
        "Fuwei Huang",
        "Wenshuo Ma",
        "Zhihui He",
        "Tianchi Cai",
        "Weize Chen",
        "Yuxiang Huang",
        "Yuanqian Zhao",
        "Bokai Xu",
        "Junbo Cui",
        "Yingjing Xu",
        "Liqing Ruan",
        "Luoyuan Zhang",
        "Hanyu Liu",
        "Jingkun Tang",
        "Hongyuan Liu",
        "Qining Guo",
        "Wenhao Hu",
        "Bingxiang He",
        "Jie Zhou",
        "Jie Cai",
        "Ji Qi",
        "Zonghao Guo",
        "Chi Chen",
        "Guoyang Zeng",
        "Yuxuan Li",
        "Ganqu Cui",
        "Ning Ding",
        "Xu Han",
        "Yuan Yao",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "summary": "Multimodal Large Language Models (MLLMs) are undergoing rapid progress and\nrepresent the frontier of AI development. However, their training and inference\nefficiency have emerged as a core bottleneck in making MLLMs more accessible\nand scalable. To address the challenges, we present MiniCPM-V 4.5, an 8B\nparameter model designed for high efficiency and strong performance. We\nintroduce three core improvements in model architecture, data strategy and\ntraining method: a unified 3D-Resampler model architecture for highly compact\nencoding over images and videos, a unified learning paradigm for document\nknowledge and text recognition without heavy data engineering, and a hybrid\nreinforcement learning strategy for proficiency in both short and long\nreasoning modes. Comprehensive experimental results in OpenCompass evaluation\nshow that MiniCPM-V 4.5 surpasses widely used proprietary models such as\nGPT-4o-latest, and significantly larger open-source models such as Qwen2.5-VL\n72B. Notably, the strong performance is achieved with remarkable efficiency.\nFor example, on the widely adopted VideoMME benchmark, MiniCPM-V 4.5 achieves\nstate-of-the-art performance among models under 30B size, using just 46.7\\% GPU\nmemory cost and 8.7\\% inference time of Qwen2.5-VL 7B.",
      "fetch_date": "2025-09-24",
      "num_comments": 4,
      "ai_summary": "MiniCPM-V 4.5, a 8B parameter multimodal large language model, achieves high performance and efficiency through a unified 3D-Resampler architecture, a unified learning paradigm, and a hybrid reinforcement learning strategy.",
      "ai_keywords": [
        "3D-Resampler",
        "unified learning paradigm",
        "hybrid reinforcement learning strategy",
        "multimodal large language models",
        "OpenCompass evaluation",
        "VideoMME benchmark"
      ]
    }
  },
  {
    "id": "2fbd109d587f03077296ef20d0d7459e",
    "source": "huggingface",
    "type": "paper",
    "title": "JoyAgent-JDGenie: Technical Report on the GAIA",
    "description": "Large Language Models are increasingly deployed as autonomous agents for\ncomplex real-world tasks, yet existing systems often focus on isolated\nimprovements without a unifying design for robustness an...<br/>Upvotes: 3<br/>GitHub Stars: 10780<br/>Authors: Jiarun Liu, Shiyue Xu, Shangkun Liu<br/>ðŸ”— <a href=\"https://github.com/jd-opensource/joyagent-jdgenie\">GitHub</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2510.00510\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2510.00510",
    "external_url": "https://github.com/jd-opensource/joyagent-jdgenie",
    "published_date": "2025-10-01T00:41:58.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2510.00510",
      "upvotes": 3,
      "github_stars": 10780,
      "github_url": "https://github.com/jd-opensource/joyagent-jdgenie",
      "project_url": "",
      "authors": [
        "Jiarun Liu",
        "Shiyue Xu",
        "Shangkun Liu",
        "Yang Li",
        "Wen Liu",
        "Min Liu",
        "Xiaoqing Zhou",
        "Hanmin Wang",
        "Shilin Jia",
        "zhen Wang",
        "Shaohua Tian",
        "Hanhao Li",
        "Junbo Zhang",
        "Yongli Yu",
        "Peng Cao",
        "Haofen Wang"
      ],
      "summary": "Large Language Models are increasingly deployed as autonomous agents for\ncomplex real-world tasks, yet existing systems often focus on isolated\nimprovements without a unifying design for robustness and adaptability. We\npropose a generalist agent architecture that integrates three core components:\na collective multi-agent framework combining planning and execution agents with\ncritic model voting, a hierarchical memory system spanning working, semantic,\nand procedural layers, and a refined tool suite for search, code execution, and\nmultimodal parsing. Evaluated on a comprehensive benchmark, our framework\nconsistently outperforms open-source baselines and approaches the performance\nof proprietary systems. These results demonstrate the importance of\nsystem-level integration and highlight a path toward scalable, resilient, and\nadaptive AI assistants capable of operating across diverse domains and tasks.",
      "fetch_date": "2025-10-02",
      "num_comments": 2,
      "ai_summary": "A generalist agent architecture combining multi-agent planning, hierarchical memory, and a refined tool suite outperforms existing systems in diverse tasks.",
      "ai_keywords": [
        "multi-agent framework",
        "planning agents",
        "execution agents",
        "critic model voting",
        "hierarchical memory system",
        "working memory",
        "semantic memory",
        "procedural memory",
        "tool suite",
        "search",
        "code execution",
        "multimodal parsing"
      ]
    }
  },
  {
    "id": "2431e39c5ab567b865fe3f25b69b4d35",
    "source": "huggingface",
    "type": "paper",
    "title": "RAG-Anything: All-in-One RAG Framework",
    "description": "Retrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm\nfor expanding Large Language Models beyond their static training limitations.\nHowever, a critical misalignment exists between...<br/>Upvotes: 32<br/>GitHub Stars: 9128<br/>Authors: Zirui Guo, Xubin Ren, Lingrui Xu<br/>ðŸ”— <a href=\"https://github.com/HKUDS/RAG-Anything\">GitHub</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2510.12323\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2510.12323",
    "external_url": "https://github.com/HKUDS/RAG-Anything",
    "published_date": "2025-10-14T05:25:35.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2510.12323",
      "upvotes": 32,
      "github_stars": 9128,
      "github_url": "https://github.com/HKUDS/RAG-Anything",
      "project_url": "",
      "authors": [
        "Zirui Guo",
        "Xubin Ren",
        "Lingrui Xu",
        "Jiahao Zhang",
        "Chao Huang"
      ],
      "summary": "Retrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm\nfor expanding Large Language Models beyond their static training limitations.\nHowever, a critical misalignment exists between current RAG capabilities and\nreal-world information environments. Modern knowledge repositories are\ninherently multimodal, containing rich combinations of textual content, visual\nelements, structured tables, and mathematical expressions. Yet existing RAG\nframeworks are limited to textual content, creating fundamental gaps when\nprocessing multimodal documents. We present RAG-Anything, a unified framework\nthat enables comprehensive knowledge retrieval across all modalities. Our\napproach reconceptualizes multimodal content as interconnected knowledge\nentities rather than isolated data types. The framework introduces dual-graph\nconstruction to capture both cross-modal relationships and textual semantics\nwithin a unified representation. We develop cross-modal hybrid retrieval that\ncombines structural knowledge navigation with semantic matching. This enables\neffective reasoning over heterogeneous content where relevant evidence spans\nmultiple modalities. RAG-Anything demonstrates superior performance on\nchallenging multimodal benchmarks, achieving significant improvements over\nstate-of-the-art methods. Performance gains become particularly pronounced on\nlong documents where traditional approaches fail. Our framework establishes a\nnew paradigm for multimodal knowledge access, eliminating the architectural\nfragmentation that constrains current systems. Our framework is open-sourced\nat: https://github.com/HKUDS/RAG-Anything.",
      "fetch_date": "2025-10-15",
      "num_comments": 4,
      "ai_summary": "RAG-Anything is a unified framework that enhances multimodal knowledge retrieval by integrating cross-modal relationships and semantic matching, outperforming existing methods on complex benchmarks.",
      "ai_keywords": [
        "Retrieval-Augmented Generation",
        "RAG",
        "Large Language Models",
        "multimodal",
        "textual content",
        "visual elements",
        "structured tables",
        "mathematical expressions",
        "dual-graph construction",
        "cross-modal hybrid retrieval",
        "structural knowledge navigation",
        "semantic matching",
        "multimodal benchmarks",
        "long documents"
      ]
    }
  }
]