[
  {
    "id": "2442f667afe5130cc9a066a427c61d55",
    "source": "huggingface",
    "type": "paper",
    "title": "OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-training in Every Iteration",
    "description": "As high-quality public text approaches exhaustion, a phenomenon known as the Data Wall, pre-training is shifting from more tokens to better tokens. However, existing methods either rely on heuristic s...<br/>Upvotes: 294<br/>Authors: Shaobo Wang, Xuan Ouyang, Tianyi Xu<br/>ðŸ”— <a href=\"https://huggingface.co/papers/2602.05400\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2602.05400",
    "external_url": "",
    "published_date": "2026-02-05T02:34:23.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2602.05400",
      "upvotes": 294,
      "github_stars": 0,
      "github_url": "",
      "project_url": "",
      "authors": [
        "Shaobo Wang",
        "Xuan Ouyang",
        "Tianyi Xu",
        "Yuzheng Hu",
        "Jialin Liu",
        "Guo Chen",
        "Tianyu Zhang",
        "Junhao Zheng",
        "Kexin Yang",
        "Xingzhang Ren",
        "Dayiheng Liu",
        "Linfeng Zhang"
      ],
      "summary": "As high-quality public text approaches exhaustion, a phenomenon known as the Data Wall, pre-training is shifting from more tokens to better tokens. However, existing methods either rely on heuristic static filters that ignore training dynamics, or use dynamic yet optimizer-agnostic criteria based on raw gradients. We propose OPUS (Optimizer-induced Projected Utility Selection), a dynamic data selection framework that defines utility in the optimizer-induced update space. OPUS scores candidates by projecting their effective updates, shaped by modern optimizers, onto a target direction derived from a stable, in-distribution proxy. To ensure scalability, we employ Ghost technique with CountSketch for computational efficiency, and Boltzmann sampling for data diversity, incurring only 4.7\\% additional compute overhead. OPUS achieves remarkable results across diverse corpora, quality tiers, optimizers, and model scales. In pre-training of GPT-2 Large/XL on FineWeb and FineWeb-Edu with 30B tokens, OPUS outperforms industrial-level baselines and even full 200B-token training. Moreover, when combined with industrial-level static filters, OPUS further improves pre-training efficiency, even with lower-quality data. Furthermore, in continued pre-training of Qwen3-8B-Base on SciencePedia, OPUS achieves superior performance using only 0.5B tokens compared to full training with 3B tokens, demonstrating significant data efficiency gains in specialized domains.",
      "fetch_date": "2026-02-11",
      "num_comments": 3,
      "ai_summary": "OPUS is a dynamic data selection framework that improves pre-training efficiency by scoring data candidates based on optimizer-induced update projections in a stable proxy-derived target space, achieving superior performance with reduced computational overhead.",
      "ai_keywords": [
        "data selection",
        "optimizer-induced update space",
        "effective updates",
        "stable in-distribution proxy",
        "Ghost technique",
        "CountSketch",
        "Boltzmann sampling",
        "pre-training",
        "GPT-2",
        "Qwen3-8B-Base",
        "FineWeb",
        "FineWeb-Edu",
        "SciencePedia"
      ]
    }
  },
  {
    "id": "60666ed538be4ebda8058b1f954547e6",
    "source": "huggingface",
    "type": "paper",
    "title": "Green-VLA: Staged Vision-Language-Action Model for Generalist Robots",
    "description": "We introduce Green-VLA, a staged Vision-Language-Action (VLA) framework for real-world deployment on the Green humanoid robot while maintaining generalization across diverse embodiments. Green-VLA fol...<br/>Upvotes: 276<br/>GitHub Stars: 34<br/>Authors: I. Apanasevich, M. Artemyev, R. Babakyan<br/>ðŸ”— <a href=\"https://github.com/greenvla/GreenVLA\">GitHub</a><br/>ðŸ”— <a href=\"https://greenvla.github.io\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2602.00919\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2602.00919",
    "external_url": "https://github.com/greenvla/GreenVLA",
    "published_date": "2026-01-31T17:13:23.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2602.00919",
      "upvotes": 276,
      "github_stars": 34,
      "github_url": "https://github.com/greenvla/GreenVLA",
      "project_url": "https://greenvla.github.io",
      "authors": [
        "I. Apanasevich",
        "M. Artemyev",
        "R. Babakyan",
        "P. Fedotova",
        "D. Grankin",
        "E. Kupryashin",
        "A. Misailidi",
        "D. Nerus",
        "A. Nutalapati",
        "G. Sidorov",
        "I. Efremov",
        "M. Gerasyov",
        "D. Pikurov",
        "Y. Senchenko",
        "S. Davidenko",
        "D. Kulikov",
        "M. Sultankin",
        "K. Askarbek",
        "O. Shamanin",
        "D. Statovoy",
        "E. Zalyaev",
        "I. Zorin",
        "A. Letkin",
        "E. Rusakov",
        "A. Silchenko",
        "V. Vorobyov",
        "S. Sobolnikov",
        "A. Postnikov"
      ],
      "summary": "We introduce Green-VLA, a staged Vision-Language-Action (VLA) framework for real-world deployment on the Green humanoid robot while maintaining generalization across diverse embodiments. Green-VLA follows a five stage curriculum: (L0) foundational VLMs, (L1) multimodal grounding, (R0) multi-embodiment pretraining, (R1) embodiment-specific adaptation, and (R2) reinforcement-learning (RL) policy alignment. We couple a scalable data-processing pipeline (3,000 hours of demonstrations) with temporal alignment and quality filtering, and use a unified, embodiment-aware action interface enabling a single policy to control humanoids, mobile manipulators, and fixed-base arms. At inference, the VLA controller is enhanced with episode-progress prediction, out-of-distribution detection, and joint-prediction-based guidance to improve safety and precise target selection. Experiments on Simpler BRIDGE WidowX and CALVIN ABC-D, as well as real-robot evaluations, show strong generalization and performance gains from RL alignment in success rate, robustness, and long-horizon efficiency.",
      "fetch_date": "2026-02-03",
      "num_comments": 7,
      "ai_summary": "Green-VLA is a five-stage vision-language-action framework for real-world robot deployment that achieves generalization across different robot embodiments through multimodal training and reinforcement learning.",
      "ai_keywords": [
        "Vision-Language-Action",
        "multimodal grounding",
        "multi-embodiment pretraining",
        "embodiment-specific adaptation",
        "reinforcement-learning",
        "episode-progress prediction",
        "out-of-distribution detection",
        "joint-prediction-based guidance"
      ]
    }
  },
  {
    "id": "b1cff772289aab941c862bdff282297c",
    "source": "huggingface",
    "type": "paper",
    "title": "ERNIE 5.0 Technical Report",
    "description": "In this report, we introduce ERNIE 5.0, a natively autoregressive foundation model desinged for unified multimodal understanding and generation across text, image, video, and audio. All modalities are...<br/>Upvotes: 249<br/>Authors: Haifeng Wang, Hua Wu, Tian Wu<br/>ðŸ”— <a href=\"https://huggingface.co/papers/2602.04705\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2602.04705",
    "external_url": "",
    "published_date": "2026-02-04T11:18:15.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2602.04705",
      "upvotes": 249,
      "github_stars": 0,
      "github_url": "",
      "project_url": "",
      "authors": [
        "Haifeng Wang",
        "Hua Wu",
        "Tian Wu",
        "Yu Sun",
        "Jing Liu",
        "Dianhai Yu",
        "Yanjun Ma",
        "Jingzhou He",
        "Zhongjun He",
        "Dou Hong",
        "Qiwen Liu",
        "Shuohuan Wang",
        "Junyuan Shang",
        "Zhenyu Zhang",
        "Yuchen Ding",
        "Jinle Zeng",
        "Jiabin Yang",
        "Liang Shen",
        "Ruibiao Chen",
        "Weichong Yin",
        "Siyu Ding",
        "Dai Dai",
        "Shikun Feng",
        "Siqi Bao",
        "Bolei He",
        "Yan Chen",
        "Zhenyu Jiao",
        "Ruiqing Zhang",
        "Zeyu Chen",
        "Qingqing Dang",
        "Kaipeng Deng",
        "Jiajun Jiang",
        "Enlei Gong",
        "Guoxia Wang",
        "Yanlin Sha",
        "Yi Liu",
        "Yehan Zheng",
        "Weijian Xu",
        "Jiaxiang Liu",
        "Zengfeng Zeng",
        "Yingqi Qu",
        "Zhongli Li",
        "Zhengkun Zhang",
        "Xiyang Wang",
        "Zixiang Xu",
        "Xinchao Xu",
        "Zhengjie Huang",
        "Dong Wang",
        "Bingjin Chen",
        "Yue Chang",
        "Xing Yuan",
        "Shiwei Huang",
        "Qiao Zhao",
        "Xinzhe Ding",
        "Shuangshuang Qiao",
        "Baoshan Yang",
        "Bihong Tang",
        "Bin Li",
        "Bingquan Wang",
        "Binhan Tang",
        "Binxiong Zheng",
        "Bo Cui",
        "Bo Ke",
        "Bo Zhang",
        "Bowen Zhang",
        "Boyan Zhang",
        "Boyang Liu",
        "Caiji Zhang",
        "Can Li",
        "Chang Xu",
        "Chao Pang",
        "Chao Zhang",
        "Chaoyi Yuan",
        "Chen Chen",
        "Cheng Cui",
        "Chenlin Yin",
        "Chun Gan",
        "Chunguang Chai",
        "Chuyu Fang",
        "Cuiyun Han",
        "Dan Zhang",
        "Danlei Feng",
        "Danxiang Zhu",
        "Dong Sun",
        "Dongbo Li",
        "Dongdong Li",
        "Dongdong Liu",
        "Dongxue Liu",
        "Fan Ding",
        "Fan Hu",
        "Fan Li",
        "Fan Mo",
        "Feisheng Wu",
        "Fengwei Liu",
        "Gangqiang Hu",
        "Gaofeng Lu",
        "Gaopeng Yong",
        "Gexiao Tian",
        "Guan Wang",
        "Guangchen Ni",
        "Guangshuo Wu",
        "Guanzhong Wang",
        "Guihua Liu",
        "Guishun Li",
        "Haibin Li",
        "Haijian Liang",
        "Haipeng Ming",
        "Haisu Wang",
        "Haiyang Lu",
        "Haiye Lin",
        "Han Zhou",
        "Hangting Lou",
        "Hanwen Du",
        "Hanzhi Zhang",
        "Hao Chen",
        "Hao Du",
        "Hao Liu",
        "Hao Zhou",
        "Haochen Jiang",
        "Haodong Tian",
        "Haoshuang Wang",
        "Haozhe Geng",
        "Heju Yin",
        "Hong Chen",
        "Hongchen Xue",
        "Hongen Liu",
        "Honggeng Zhang",
        "Hongji Xu",
        "Hongwei Chen",
        "Hongyang Zhang",
        "Hongyuan Zhang",
        "Hua Lu",
        "Huan Chen",
        "Huan Wang",
        "Huang He",
        "Hui Liu",
        "Hui Zhong",
        "Huibin Ruan",
        "Jiafeng Lu",
        "Jiage Liang",
        "Jiahao Hu",
        "Jiahao Hu",
        "Jiajie Yang",
        "Jialin Li",
        "Jian Chen",
        "Jian Wu",
        "Jianfeng Yang",
        "Jianguang Jiang",
        "Jianhua Wang",
        "Jianye Chen",
        "Jiaodi Liu",
        "Jiarui Zhou",
        "Jiawei Lv",
        "Jiaxin Zhou",
        "Jiaxuan Liu",
        "Jie Han",
        "Jie Sun",
        "Jiefan Fang",
        "Jihan Liu",
        "Jihua Liu",
        "Jing Hu",
        "Jing Qian",
        "Jing Yan",
        "Jingdong Du",
        "Jingdong Wang",
        "Jingjing Wu",
        "Jingyong Li",
        "Jinheng Wang",
        "Jinjin Li",
        "Jinliang Lu",
        "Jinlin Yu",
        "Jinnan Liu",
        "Jixiang Feng",
        "Jiyi Huang",
        "Jiyuan Zhang",
        "Jun Liang",
        "Jun Xia",
        "Jun Yu",
        "Junda Chen",
        "Junhao Feng",
        "Junhong Xiang",
        "Junliang Li",
        "Kai Liu",
        "Kailun Chen",
        "Kairan Su",
        "Kang Hu",
        "Kangkang Zhou",
        "Ke Chen",
        "Ke Wei",
        "Kui Huang",
        "Kun Wu",
        "Kunbin Chen",
        "Lei Han",
        "Lei Sun",
        "Lei Wen",
        "Linghui Meng",
        "Linhao Yu",
        "Liping Ouyang",
        "Liwen Zhang",
        "Longbin Ji",
        "Longzhi Wang",
        "Meng Sun",
        "Meng Tian",
        "Mengfei Li",
        "Mengqi Zeng",
        "Mengyu Zhang",
        "Ming Hong",
        "Mingcheng Zhou",
        "Mingming Huang",
        "Mingxin Chen",
        "Mingzhu Cai",
        "Naibin Gu",
        "Nemin Qiu",
        "Nian Wang",
        "Peng Qiu",
        "Peng Zhao",
        "Pengyu Zou",
        "Qi Wang",
        "Qi Xin",
        "Qian Wang",
        "Qiang Zhu",
        "Qianhui Luo",
        "Qianwei Yang",
        "Qianyue He",
        "Qifei Wu",
        "Qinrui Li",
        "Qiwen Bao",
        "Quan Zhang",
        "Quanxiang Liu",
        "Qunyi Xie",
        "Rongrui Zhan",
        "Rufeng Dai",
        "Rui Peng",
        "Ruian Liu",
        "Ruihao Xu",
        "Ruijie Wang",
        "Ruixi Zhang",
        "Ruixuan Liu",
        "Runsheng Shi",
        "Ruting Wang",
        "Senbo Kang",
        "Shan Lu",
        "Shaofei Yu",
        "Shaotian Gong",
        "Shenwei Hu",
        "Shifeng Zheng",
        "Shihao Guo",
        "Shilong Fan",
        "Shiqin Liu",
        "Shiwei Gu",
        "Shixi Zhang",
        "Shuai Yao",
        "Shuang Zhang",
        "Shuangqiao Liu",
        "Shuhao Liang",
        "Shuwei He",
        "Shuwen Yang",
        "Sijun He",
        "Siming Dai",
        "Siming Wu",
        "Siyi Long",
        "Songhe Deng",
        "Suhui Dong",
        "Suyin Liang",
        "Teng Hu",
        "Tianchan Xu",
        "Tianliang Lv",
        "Tianmeng Yang",
        "Tianyi Wei",
        "Tiezhu Gao",
        "Ting Sun",
        "Ting Zhang",
        "Tingdan Luo",
        "Wei He",
        "Wei Luan",
        "Wei Yin",
        "Wei Zhang",
        "Wei Zhou",
        "Weibao Gong",
        "Weibin Li",
        "Weicheng Huang",
        "Weichong Dang",
        "Weiguo Zhu",
        "Weilong Zhang",
        "Weiqi Tan",
        "Wen Huang",
        "Wenbin Chang",
        "Wenjing Du",
        "Wenlong Miao",
        "Wenpei Luo",
        "Wenquan Wu",
        "Xi Shi",
        "Xi Zhao",
        "Xiang Gao",
        "Xiangguo Zhang",
        "Xiangrui Yu",
        "Xiangsen Wang",
        "Xiangzhe Wang",
        "Xianlong Luo",
        "Xianying Ma",
        "Xiao Tan",
        "Xiaocong Lin",
        "Xiaofei Wang",
        "Xiaofeng Peng",
        "Xiaofeng Wu",
        "Xiaojian Xu",
        "Xiaolan Yuan",
        "Xiaopeng Cui",
        "Xiaotian Han",
        "Xiaoxiong Liu",
        "Xiaoxu Fei",
        "Xiaoxuan Wu",
        "Xiaoyu Wang",
        "Xiaoyu Zhang",
        "Xin Sun",
        "Xin Wang",
        "Xinhui Huang",
        "Xinming Zhu",
        "Xintong Yu",
        "Xinyi Xu",
        "Xinyu Wang",
        "Xiuxian Li",
        "XuanShi Zhu",
        "Xue Xu",
        "Xueying Lv",
        "Xuhong Li",
        "Xulong Wei",
        "Xuyi Chen",
        "Yabing Shi",
        "Yafeng Wang",
        "Yamei Li",
        "Yan Liu",
        "Yanfu Cheng",
        "Yang Gao",
        "Yang Liang",
        "Yang Wang",
        "Yang Wang",
        "Yang Yang",
        "Yanlong Liu",
        "Yannian Fu",
        "Yanpeng Wang",
        "Yanzheng Lin",
        "Yao Chen",
        "Yaozong Shen",
        "Yaqian Han",
        "Yehua Yang",
        "Yekun Chai",
        "Yesong Wang",
        "Yi Song",
        "Yichen Zhang",
        "Yifei Wang",
        "Yifeng Guo",
        "Yifeng Kou",
        "Yilong Chen",
        "Yilong Guo",
        "Yiming Wang",
        "Ying Chen",
        "Ying Wang",
        "Yingsheng Wu",
        "Yingzhan Lin",
        "Yinqi Yang",
        "Yiran Xing",
        "Yishu Lei",
        "Yixiang Tu",
        "Yiyan Chen",
        "Yong Zhang",
        "Yonghua Li",
        "Yongqiang Ma",
        "Yongxing Dai",
        "Yongyue Zhang",
        "Yu Ran",
        "Yu Sun",
        "Yu-Wen Michael Zhang",
        "Yuang Liu",
        "Yuanle Liu",
        "Yuanyuan Zhou",
        "Yubo Zhang",
        "Yuchen Han",
        "Yucheng Wang",
        "Yude Gao",
        "Yuedong Luo",
        "Yuehu Dong",
        "Yufeng Hu",
        "Yuhui Cao",
        "Yuhui Yun",
        "Yukun Chen",
        "Yukun Gao",
        "Yukun Li",
        "Yumeng Zhang",
        "Yun Fan",
        "Yun Ma",
        "Yunfei Zhang",
        "Yunshen Xie",
        "Yuping Xu",
        "Yuqin Zhang",
        "Yuqing Liu",
        "Yurui Li",
        "Yuwen Wang",
        "Yuxiang Lu",
        "Zefeng Cai",
        "Zelin Zhao",
        "Zelun Zhang",
        "Zenan Lin",
        "Zezhao Dong",
        "Zhaowu Pan",
        "Zhaoyu Liu",
        "Zhe Dong",
        "Zhe Zhang",
        "Zhen Zhang",
        "Zhengfan Wu",
        "Zhengrui Wei",
        "Zhengsheng Ning",
        "Zhenxing Li",
        "Zhenyu Li",
        "Zhenyu Qian",
        "Zhenyun Li",
        "Zhi Li",
        "Zhichao Chen",
        "Zhicheng Dong",
        "Zhida Feng",
        "Zhifan Feng",
        "Zhihao Deng",
        "Zhijin Yu",
        "Zhiyang Chen",
        "Zhonghui Zheng",
        "Zhuangzhuang Guo",
        "Zhujun Zhang",
        "Zhuo Sun",
        "Zichang Liu",
        "Zihan Lin",
        "Zihao Huang",
        "Zihe Zhu",
        "Ziheng Zhao",
        "Ziping Chen",
        "Zixuan Zhu",
        "Ziyang Xu",
        "Ziyi Liang",
        "Ziyuan Gao"
      ],
      "summary": "In this report, we introduce ERNIE 5.0, a natively autoregressive foundation model desinged for unified multimodal understanding and generation across text, image, video, and audio. All modalities are trained from scratch under a unified next-group-of-tokens prediction objective, based on an ultra-sparse mixture-of-experts (MoE) architecture with modality-agnostic expert routing. To address practical challenges in large-scale deployment under diverse resource constraints, ERNIE 5.0 adopts a novel elastic training paradigm. Within a single pre-training run, the model learns a family of sub-models with varying depths, expert capacities, and routing sparsity, enabling flexible trade-offs among performance, model size, and inference latency in memory- or time-constrained scenarios. Moreover, we systematically address the challenges of scaling reinforcement learning to unified foundation models, thereby guaranteeing efficient and stable post-training under ultra-sparse MoE architectures and diverse multimodal settings. Extensive experiments demonstrate that ERNIE 5.0 achieves strong and balanced performance across multiple modalities. To the best of our knowledge, among publicly disclosed models, ERNIE 5.0 represents the first production-scale realization of a trillion-parameter unified autoregressive model that supports both multimodal understanding and generation. To facilitate further research, we present detailed visualizations of modality-agnostic expert routing in the unified model, alongside comprehensive empirical analysis of elastic training, aiming to offer profound insights to the community.",
      "fetch_date": "2026-02-05",
      "num_comments": 4,
      "ai_summary": "ERNIE 5.0 is a production-scale trillion-parameter autoregressive model that unifies multimodal understanding and generation through sparse MoE architecture and elastic training.",
      "ai_keywords": [
        "autoregressive foundation model",
        "unified multimodal understanding",
        "unified next-group-of-tokens prediction objective",
        "mixture-of-experts",
        "modality-agnostic expert routing",
        "elastic training paradigm",
        "reinforcement learning",
        "sparse MoE architecture"
      ]
    }
  },
  {
    "id": "59257a8c7ec0c63e71909f450abc636c",
    "source": "huggingface",
    "type": "paper",
    "title": "Weak-Driven Learning: How Weak Agents make Strong Agents Stronger",
    "description": "As post-training optimization becomes central to improving large language models, we observe a persistent saturation bottleneck: once models grow highly confident, further training yields diminishing ...<br/>Upvotes: 236<br/>GitHub Stars: 74<br/>Authors: Zehao Chen, Gongxun Li, Tianxiang Ai<br/>ðŸ”— <a href=\"https://github.com/chenzehao82/Weak-Driven-Learning\">GitHub</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2602.08222\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2602.08222",
    "external_url": "https://github.com/chenzehao82/Weak-Driven-Learning",
    "published_date": "2026-02-08T21:50:40.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2602.08222",
      "upvotes": 236,
      "github_stars": 74,
      "github_url": "https://github.com/chenzehao82/Weak-Driven-Learning",
      "project_url": "",
      "authors": [
        "Zehao Chen",
        "Gongxun Li",
        "Tianxiang Ai",
        "Yifei Li",
        "Zixuan Huang",
        "Wang Zhou",
        "Fuzhen Zhuang",
        "Xianglong Liu",
        "Jianxin Li",
        "Deqing Wang",
        "Yikun Ban"
      ],
      "summary": "As post-training optimization becomes central to improving large language models, we observe a persistent saturation bottleneck: once models grow highly confident, further training yields diminishing returns. While existing methods continue to reinforce target predictions, we find that informative supervision signals remain latent in models' own historical weak states. Motivated by this observation, we propose WMSS (Weak Agents Can Make Strong Agents Stronger), a post-training paradigm that leverages weak checkpoints to guide continued optimization. By identifying recoverable learning gaps via entropy dynamics and reinforcing them through compensatory learning, WMSS enables strong agents to improve beyond conventional post-training saturation. Experiments on mathematical reasoning and code generation datasets show that agents trained with our approach achieve effective performance improvements, while incurring zero additional inference cost.",
      "fetch_date": "2026-02-10",
      "num_comments": 5,
      "ai_summary": "WMSS is a post-training paradigm that uses weak model checkpoints to identify and fill learning gaps, enabling continued improvement beyond conventional saturation points in large language models.",
      "ai_keywords": [
        "post-training optimization",
        "large language models",
        "saturation bottleneck",
        "weak checkpoints",
        "entropy dynamics",
        "compensatory learning",
        "learning gaps"
      ]
    }
  },
  {
    "id": "4f9739b510c1386a39502a9995b593ed",
    "source": "huggingface",
    "type": "paper",
    "title": "Kimi K2.5: Visual Agentic Intelligence",
    "description": "We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhan...<br/>Upvotes: 226<br/>GitHub Stars: 958<br/>Authors: Kimi Team, Tongtong Bai, Yifan Bai<br/>ðŸ”— <a href=\"https://github.com/MoonshotAI/Kimi-K2.5\">GitHub</a><br/>ðŸ”— <a href=\"https://www.kimi.com/blog/kimi-k2-5.html\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2602.02276\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2602.02276",
    "external_url": "https://github.com/MoonshotAI/Kimi-K2.5",
    "published_date": "2026-02-02T11:17:38.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2602.02276",
      "upvotes": 226,
      "github_stars": 958,
      "github_url": "https://github.com/MoonshotAI/Kimi-K2.5",
      "project_url": "https://www.kimi.com/blog/kimi-k2-5.html",
      "authors": [
        "Kimi Team",
        "Tongtong Bai",
        "Yifan Bai",
        "Yiping Bao",
        "S. H. Cai",
        "Yuan Cao",
        "Y. Charles",
        "H. S. Che",
        "Cheng Chen",
        "Guanduo Chen",
        "Huarong Chen",
        "Jia Chen",
        "Jiahao Chen",
        "Jianlong Chen",
        "Jun Chen",
        "Kefan Chen",
        "Liang Chen",
        "Ruijue Chen",
        "Xinhao Chen",
        "Yanru Chen",
        "Yanxu Chen",
        "Yicun Chen",
        "Yimin Chen",
        "Yingjiang Chen",
        "Yuankun Chen",
        "Yujie Chen",
        "Yutian Chen",
        "Zhirong Chen",
        "Ziwei Chen",
        "Dazhi Cheng",
        "Minghan Chu",
        "Jialei Cui",
        "Jiaqi Deng",
        "Muxi Diao",
        "Hao Ding",
        "Mengfan Dong",
        "Mengnan Dong",
        "Yuxin Dong",
        "Yuhao Dong",
        "Angang Du",
        "Chenzhuang Du",
        "Dikang Du",
        "Lingxiao Du",
        "Yulun Du",
        "Yu Fan",
        "Shengjun Fang",
        "Qiulin Feng",
        "Yichen Feng",
        "Garimugai Fu",
        "Kelin Fu",
        "Hongcheng Gao",
        "Tong Gao",
        "Yuyao Ge",
        "Shangyi Geng",
        "Chengyang Gong",
        "Xiaochen Gong",
        "Zhuoma Gongque",
        "Qizheng Gu",
        "Xinran Gu",
        "Yicheng Gu",
        "Longyu Guan",
        "Yuanying Guo",
        "Xiaoru Hao",
        "Weiran He",
        "Wenyang He",
        "Yunjia He",
        "Chao Hong",
        "Hao Hu",
        "Jiaxi Hu",
        "Yangyang Hu",
        "Zhenxing Hu",
        "Ke Huang",
        "Ruiyuan Huang",
        "Weixiao Huang",
        "Zhiqi Huang",
        "Tao Jiang",
        "Zhejun Jiang",
        "Xinyi Jin",
        "Yu Jing",
        "Guokun Lai",
        "Aidi Li",
        "C. Li",
        "Cheng Li",
        "Fang Li",
        "Guanghe Li",
        "Guanyu Li",
        "Haitao Li",
        "Haoyang Li",
        "Jia Li",
        "Jingwei Li",
        "Junxiong Li",
        "Lincan Li",
        "Mo Li",
        "Weihong Li",
        "Wentao Li",
        "Xinhang Li",
        "Xinhao Li",
        "Yang Li",
        "Yanhao Li",
        "Yiwei Li",
        "Yuxiao Li",
        "Zhaowei Li",
        "Zheming Li",
        "Weilong Liao",
        "Jiawei Lin",
        "Xiaohan Lin",
        "Zhishan Lin",
        "Zichao Lin",
        "Cheng Liu",
        "Chenyu Liu",
        "Hongzhang Liu",
        "Liang Liu",
        "Shaowei Liu",
        "Shudong Liu",
        "Shuran Liu",
        "Tianwei Liu",
        "Tianyu Liu",
        "Weizhou Liu",
        "Xiangyan Liu",
        "Yangyang Liu",
        "Yanming Liu",
        "Yibo Liu",
        "Yuanxin Liu",
        "Yue Liu",
        "Zhengying Liu",
        "Zhongnuo Liu",
        "Enzhe Lu",
        "Haoyu Lu",
        "Zhiyuan Lu",
        "Junyu Luo",
        "Tongxu Luo",
        "Yashuo Luo",
        "Long Ma",
        "Yingwei Ma",
        "Shaoguang Mao",
        "Yuan Mei",
        "Xin Men",
        "Fanqing Meng",
        "Zhiyong Meng",
        "Yibo Miao",
        "Minqing Ni",
        "Kun Ouyang",
        "Siyuan Pan",
        "Bo Pang",
        "Yuchao Qian",
        "Ruoyu Qin",
        "Zeyu Qin",
        "Jiezhong Qiu",
        "Bowen Qu",
        "Zeyu Shang",
        "Youbo Shao",
        "Tianxiao Shen",
        "Zhennan Shen",
        "Juanfeng Shi",
        "Lidong Shi",
        "Shengyuan Shi",
        "Feifan Song",
        "Pengwei Song",
        "Tianhui Song",
        "Xiaoxi Song",
        "Hongjin Su",
        "Jianlin Su",
        "Zhaochen Su",
        "Lin Sui",
        "Jinsong Sun",
        "Junyao Sun",
        "Tongyu Sun",
        "Flood Sung",
        "Yunpeng Tai",
        "Chuning Tang",
        "Heyi Tang",
        "Xiaojuan Tang",
        "Zhengyang Tang",
        "Jiawen Tao",
        "Shiyuan Teng",
        "Chaoran Tian",
        "Pengfei Tian",
        "Ao Wang",
        "Bowen Wang",
        "Chensi Wang",
        "Chuang Wang",
        "Congcong Wang",
        "Dingkun Wang",
        "Dinglu Wang",
        "Dongliang Wang",
        "Feng Wang",
        "Hailong Wang",
        "Haiming Wang",
        "Hengzhi Wang",
        "Huaqing Wang",
        "Hui Wang",
        "Jiahao Wang",
        "Jinhong Wang",
        "Jiuzheng Wang",
        "Kaixin Wang",
        "Linian Wang",
        "Qibin Wang",
        "Shengjie Wang",
        "Shuyi Wang",
        "Si Wang",
        "Wei Wang",
        "Xiaochen Wang",
        "Xinyuan Wang",
        "Yao Wang",
        "Yejie Wang",
        "Yipu Wang",
        "Yiqin Wang",
        "Yucheng Wang",
        "Yuzhi Wang",
        "Zhaoji Wang",
        "Zhaowei Wang",
        "Zhengtao Wang",
        "Zhexu Wang",
        "Zihan Wang",
        "Zizhe Wang",
        "Chu Wei",
        "Ming Wei",
        "Chuan Wen",
        "Zichen Wen",
        "Chengjie Wu",
        "Haoning Wu",
        "Junyan Wu",
        "Rucong Wu",
        "Wenhao Wu",
        "Yuefeng Wu",
        "Yuhao Wu",
        "Yuxin Wu",
        "Zijian Wu",
        "Chenjun Xiao",
        "Jin Xie",
        "Xiaotong Xie",
        "Yuchong Xie",
        "Yifei Xin",
        "Bowei Xing",
        "Boyu Xu",
        "Jianfan Xu",
        "Jing Xu",
        "Jinjing Xu",
        "L. H. Xu",
        "Lin Xu",
        "Suting Xu",
        "Weixin Xu",
        "Xinbo Xu",
        "Xinran Xu",
        "Yangchuan Xu",
        "Yichang Xu",
        "Yuemeng Xu",
        "Zelai Xu",
        "Ziyao Xu",
        "Junjie Yan",
        "Yuzi Yan",
        "Guangyao Yang",
        "Hao Yang",
        "Junwei Yang",
        "Kai Yang",
        "Ningyuan Yang",
        "Ruihan Yang",
        "Xiaofei Yang",
        "Xinlong Yang",
        "Ying Yang",
        "Yi Yang",
        "Yi Yang",
        "Zhen Yang",
        "Zhilin Yang",
        "Zonghan Yang",
        "Haotian Yao",
        "Dan Ye",
        "Wenjie Ye",
        "Zhuorui Ye",
        "Bohong Yin",
        "Chengzhen Yu",
        "Longhui Yu",
        "Tao Yu",
        "Tianxiang Yu",
        "Enming Yuan",
        "Mengjie Yuan",
        "Xiaokun Yuan",
        "Yang Yue",
        "Weihao Zeng",
        "Dunyuan Zha",
        "Haobing Zhan",
        "Dehao Zhang",
        "Hao Zhang",
        "Jin Zhang",
        "Puqi Zhang",
        "Qiao Zhang",
        "Rui Zhang",
        "Xiaobin Zhang",
        "Y. Zhang",
        "Yadong Zhang",
        "Yangkun Zhang",
        "Yichi Zhang",
        "Yizhi Zhang",
        "Yongting Zhang",
        "Yu Zhang",
        "Yushun Zhang",
        "Yutao Zhang",
        "Yutong Zhang",
        "Zheng Zhang",
        "Chenguang Zhao",
        "Feifan Zhao",
        "Jinxiang Zhao",
        "Shuai Zhao",
        "Xiangyu Zhao",
        "Yikai Zhao",
        "Zijia Zhao",
        "Huabin Zheng",
        "Ruihan Zheng",
        "Shaojie Zheng",
        "Tengyang Zheng",
        "Junfeng Zhong",
        "Longguang Zhong",
        "Weiming Zhong",
        "M. Zhou",
        "Runjie Zhou",
        "Xinyu Zhou",
        "Zaida Zhou",
        "Jinguo Zhu",
        "Liya Zhu",
        "Xinhao Zhu",
        "Yuxuan Zhu",
        "Zhen Zhu",
        "Jingze Zhuang",
        "Weiyu Zhuang",
        "Ying Zou",
        "Xinxing Zu"
      ],
      "summary": "We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhance each other. This includes a series of techniques such as joint text-vision pre-training, zero-vision SFT, and joint text-vision reinforcement learning. Building on this multimodal foundation, K2.5 introduces Agent Swarm, a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. Extensive evaluations show that Kimi K2.5 achieves state-of-the-art results across various domains including coding, vision, reasoning, and agentic tasks. Agent Swarm also reduces latency by up to 4.5times over single-agent baselines. We release the post-trained Kimi K2.5 model checkpoint to facilitate future research and real-world applications of agentic intelligence.",
      "fetch_date": "2026-02-03",
      "num_comments": 3,
      "ai_summary": "Kimi K2.5 is an open-source multimodal agentic model that enhances text and vision processing through joint optimization techniques and introduces Agent Swarm for parallel task execution.",
      "ai_keywords": [
        "multimodal agentic model",
        "joint text-vision pre-training",
        "zero-vision SFT",
        "joint text-vision reinforcement learning",
        "Agent Swarm",
        "self-directed parallel agent orchestration framework",
        "heterogeneous sub-problems"
      ]
    }
  },
  {
    "id": "01505ff128b7cc4069a838f067f22852",
    "source": "huggingface",
    "type": "paper",
    "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning",
    "description": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perfor...<br/>Upvotes: 211<br/>GitHub Stars: 142<br/>Authors: Chengwen Liu, Xiaomin Yu, Zhuoyue Chang<br/>ðŸ”— <a href=\"https://github.com/QuantaAlpha/VideoDR-Benchmark\">GitHub</a><br/>ðŸ”— <a href=\"https://videodr-benchmark.github.io/#/home\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2601.06943\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2601.06943",
    "external_url": "https://github.com/QuantaAlpha/VideoDR-Benchmark",
    "published_date": "2026-01-11T10:07:37.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2601.06943",
      "upvotes": 211,
      "github_stars": 142,
      "github_url": "https://github.com/QuantaAlpha/VideoDR-Benchmark",
      "project_url": "https://videodr-benchmark.github.io/#/home",
      "authors": [
        "Chengwen Liu",
        "Xiaomin Yu",
        "Zhuoyue Chang",
        "Zhe Huang",
        "Shuo Zhang",
        "Heng Lian",
        "Kunyi Wang",
        "Rui Xu",
        "Sen Hu",
        "Jianheng Hou",
        "Hao Peng",
        "Chengwei Qin",
        "Xiaobin Hu",
        "Hong Peng",
        "Ronghao Chen",
        "Huacan Wang"
      ],
      "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.",
      "fetch_date": "2026-01-13",
      "num_comments": 7,
      "ai_summary": "VideoDR benchmark enables video question answering by combining cross-frame visual extraction, web retrieval, and multi-hop reasoning in open-domain settings.",
      "ai_keywords": [
        "video question answering",
        "cross-frame visual anchor extraction",
        "interactive web retrieval",
        "multi-hop reasoning",
        "multimodal large language models",
        "Workflow paradigm",
        "Agentic paradigm",
        "goal drift",
        "long-horizon consistency"
      ]
    }
  },
  {
    "id": "e6cd663cc8eaf53e2ed564e037a90af3",
    "source": "huggingface",
    "type": "paper",
    "title": "Qwen3-TTS Technical Report",
    "description": "In this report, we present the Qwen3-TTS series, a family of advanced multilingual, controllable, robust, and streaming text-to-speech models. Qwen3-TTS supports state-of-the-art 3-second voice clonin...<br/>Upvotes: 63<br/>GitHub Stars: 7527<br/>Authors: Hangrui Hu, Xinfa Zhu, Ting He<br/>ðŸ”— <a href=\"https://github.com/QwenLM/Qwen3-TTS\">GitHub</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2601.15621\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2601.15621",
    "external_url": "https://github.com/QwenLM/Qwen3-TTS",
    "published_date": "2026-01-21T22:51:43.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2601.15621",
      "upvotes": 63,
      "github_stars": 7527,
      "github_url": "https://github.com/QwenLM/Qwen3-TTS",
      "project_url": "",
      "authors": [
        "Hangrui Hu",
        "Xinfa Zhu",
        "Ting He",
        "Dake Guo",
        "Bin Zhang",
        "Xiong Wang",
        "Zhifang Guo",
        "Ziyue Jiang",
        "Hongkun Hao",
        "Zishan Guo",
        "Xinyu Zhang",
        "Pei Zhang",
        "Baosong Yang",
        "Jin Xu",
        "Jingren Zhou",
        "Junyang Lin"
      ],
      "summary": "In this report, we present the Qwen3-TTS series, a family of advanced multilingual, controllable, robust, and streaming text-to-speech models. Qwen3-TTS supports state-of-the-art 3-second voice cloning and description-based control, allowing both the creation of entirely novel voices and fine-grained manipulation over the output speech. Trained on over 5 million hours of speech data spanning 10 languages, Qwen3-TTS adopts a dual-track LM architecture for real-time synthesis, coupled with two speech tokenizers: 1) Qwen-TTS-Tokenizer-25Hz is a single-codebook codec emphasizing semantic content, which offers seamlessly integration with Qwen-Audio and enables streaming waveform reconstruction via a block-wise DiT. 2) Qwen-TTS-Tokenizer-12Hz achieves extreme bitrate reduction and ultra-low-latency streaming, enabling immediate first-packet emission (97,ms) through its 12.5 Hz, 16-layer multi-codebook design and a lightweight causal ConvNet. Extensive experiments indicate state-of-the-art performance across diverse objective and subjective benchmark (e.g., TTS multilingual test set, InstructTTSEval, and our long speech test set). To facilitate community research and development, we release both tokenizers and models under the Apache 2.0 license.",
      "fetch_date": "2026-01-23",
      "num_comments": 1,
      "ai_summary": "The Qwen3-TTS series presents advanced multilingual text-to-speech models with voice cloning and controllable speech generation capabilities, utilizing dual-track LM architecture and specialized speech tokenizers for efficient streaming synthesis.",
      "ai_keywords": [
        "text-to-speech",
        "voice cloning",
        "dual-track LM architecture",
        "speech tokenizers",
        "Qwen-TTS-Tokenizer-25Hz",
        "Qwen-TTS-Tokenizer-12Hz",
        "DiT",
        "ConvNet",
        "streaming waveform reconstruction",
        "multilingual",
        "controllable speech generation"
      ]
    }
  },
  {
    "id": "e4456066efdcdf46fe4b108089570f93",
    "source": "huggingface",
    "type": "paper",
    "title": "HeartMuLa: A Family of Open Sourced Music Foundation Models",
    "description": "We present a family of open-source Music Foundation Models designed to advance large-scale music understanding and generation across diverse tasks and modalities. Our framework consists of four major ...<br/>Upvotes: 42<br/>GitHub Stars: 3488<br/>Authors: Dongchao Yang, Yuxin Xie, Yuguo Yin<br/>ðŸ”— <a href=\"https://github.com/HeartMuLa/heartlib\">GitHub</a><br/>ðŸ”— <a href=\"https://heartmula.github.io/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2601.10547\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2601.10547",
    "external_url": "https://github.com/HeartMuLa/heartlib",
    "published_date": "2026-01-15T11:14:25.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2601.10547",
      "upvotes": 42,
      "github_stars": 3488,
      "github_url": "https://github.com/HeartMuLa/heartlib",
      "project_url": "https://heartmula.github.io/",
      "authors": [
        "Dongchao Yang",
        "Yuxin Xie",
        "Yuguo Yin",
        "Zheyu Wang",
        "Xiaoyu Yi",
        "Gongxi Zhu",
        "Xiaolong Weng",
        "Zihan Xiong",
        "Yingzhe Ma",
        "Dading Cong",
        "Jingliang Liu",
        "Zihang Huang",
        "Jinghan Ru",
        "Rongjie Huang",
        "Haoran Wan",
        "Peixu Wang",
        "Kuoxi Yu",
        "Helin Wang",
        "Liming Liang",
        "Xianwei Zhuang",
        "Yuanyuan Wang",
        "Haohan Guo",
        "Junjie Cao",
        "Zeqian Ju",
        "Songxiang Liu",
        "Yuewen Cao",
        "Heming Weng",
        "Yuexian Zou"
      ],
      "summary": "We present a family of open-source Music Foundation Models designed to advance large-scale music understanding and generation across diverse tasks and modalities. Our framework consists of four major components: (1) HeartCLAP, an audio-text alignment model; (2) HeartTranscriptor, a robust lyric recognition model optimized for real-world music scenarios; and (3) HeartCodec, a low-frame-rate (12.5 Hz) yet high-fidelity music codec tokenizer that captures long-range musical structure while preserving fine-grained acoustic details and enabling efficient autoregressive modeling; (4) HeartMuLa, an LLM-based song generation model capable of synthesizing high-fidelity music under rich, user-controllable conditions (e.g., textual style descriptions, lyrics, and reference audio). In addition, it provides two specialized modes: (i) fine-grained musical attribute control, which allows users to specify the style of different song sections (e.g., intro, verse, chorus) using natural language prompts; and (ii) short, engaging music generation, which is suitable as background music for short videos. Lastly, HeartMuLa improves significantly when scaled to 7B parameters. For the first time, we show that a Suno-level, commercial-grade system can be reproduced using academic-scale data and GPU resources. We expect these foundation models to serve as strong baselines for future research and to facilitate practical applications in multimodal content production.",
      "fetch_date": "2026-01-16",
      "num_comments": 4,
      "ai_summary": "A suite of open-source music foundation models is introduced, featuring components for audio-text alignment, lyric recognition, music coding, and large language model-based song generation with controllable attributes and scalable parameterization.",
      "ai_keywords": [
        "Music Foundation Models",
        "audio-text alignment model",
        "lyric recognition model",
        "music codec tokenizer",
        "LLM-based song generation model",
        "autoregressive modeling",
        "musical attribute control",
        "short video background music generation",
        "parameter scaling"
      ]
    }
  },
  {
    "id": "3ca1c9448a1392dc62dcb1e40f0a07ae",
    "source": "huggingface",
    "type": "paper",
    "title": "PaperBanana: Automating Academic Illustration for AI Scientists",
    "description": "Despite rapid advances in autonomous AI scientists powered by language models, generating publication-ready illustrations remains a labor-intensive bottleneck in the research workflow. To lift this bu...<br/>Upvotes: 176<br/>GitHub Stars: 3389<br/>Authors: Dawei Zhu, Rui Meng, Yale Song<br/>ðŸ”— <a href=\"https://github.com/dwzhu-pku/PaperBanana\">GitHub</a><br/>ðŸ”— <a href=\"https://dwzhu-pku.github.io/PaperBanana/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2601.23265\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2601.23265",
    "external_url": "https://github.com/dwzhu-pku/PaperBanana",
    "published_date": "2026-01-30T13:33:37.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2601.23265",
      "upvotes": 176,
      "github_stars": 3389,
      "github_url": "https://github.com/dwzhu-pku/PaperBanana",
      "project_url": "https://dwzhu-pku.github.io/PaperBanana/",
      "authors": [
        "Dawei Zhu",
        "Rui Meng",
        "Yale Song",
        "Xiyu Wei",
        "Sujian Li",
        "Tomas Pfister",
        "Jinsung Yoon"
      ],
      "summary": "Despite rapid advances in autonomous AI scientists powered by language models, generating publication-ready illustrations remains a labor-intensive bottleneck in the research workflow. To lift this burden, we introduce PaperBanana, an agentic framework for automated generation of publication-ready academic illustrations. Powered by state-of-the-art VLMs and image generation models, PaperBanana orchestrates specialized agents to retrieve references, plan content and style, render images, and iteratively refine via self-critique. To rigorously evaluate our framework, we introduce PaperBananaBench, comprising 292 test cases for methodology diagrams curated from NeurIPS 2025 publications, covering diverse research domains and illustration styles. Comprehensive experiments demonstrate that PaperBanana consistently outperforms leading baselines in faithfulness, conciseness, readability, and aesthetics. We further show that our method effectively extends to the generation of high-quality statistical plots. Collectively, PaperBanana paves the way for the automated generation of publication-ready illustrations.",
      "fetch_date": "2026-02-02",
      "num_comments": 12,
      "ai_summary": "_paperbanana is an agentic framework that automates the creation of publication-ready academic illustrations using advanced vision-language models and image generation techniques.",
      "ai_keywords": [
        "VLMs",
        "image generation models",
        "agentic framework",
        "publication-ready illustrations",
        "methodology diagrams",
        "PaperBananaBench",
        "self-critique",
        "statistical plots"
      ]
    }
  },
  {
    "id": "de0ab4deaf169a5f7442c17a4444ef3f",
    "source": "huggingface",
    "type": "paper",
    "title": "Advancing Open-source World Models",
    "description": "We present LingBot-World, an open-sourced world simulator stemming from video generation. Positioned as a top-tier world model, LingBot-World offers the following features. (1) It maintains high fidel...<br/>Upvotes: 126<br/>GitHub Stars: 2821<br/>Authors: Robbyant Team, Zelin Gao, Qiuyu Wang<br/>ðŸ”— <a href=\"https://github.com/Robbyant/lingbot-world/\">GitHub</a><br/>ðŸ”— <a href=\"https://technology.robbyant.com/lingbot-world\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2601.20540\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2601.20540",
    "external_url": "https://github.com/Robbyant/lingbot-world/",
    "published_date": "2026-01-28T07:37:01.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2601.20540",
      "upvotes": 126,
      "github_stars": 2821,
      "github_url": "https://github.com/Robbyant/lingbot-world/",
      "project_url": "https://technology.robbyant.com/lingbot-world",
      "authors": [
        "Robbyant Team",
        "Zelin Gao",
        "Qiuyu Wang",
        "Yanhong Zeng",
        "Jiapeng Zhu",
        "Ka Leong Cheng",
        "Yixuan Li",
        "Hanlin Wang",
        "Yinghao Xu",
        "Shuailei Ma",
        "Yihang Chen",
        "Jie Liu",
        "Yansong Cheng",
        "Yao Yao",
        "Jiayi Zhu",
        "Yihao Meng",
        "Kecheng Zheng",
        "Qingyan Bai",
        "Jingye Chen",
        "Zehong Shen",
        "Yue Yu",
        "Xing Zhu",
        "Yujun Shen",
        "Hao Ouyang"
      ],
      "summary": "We present LingBot-World, an open-sourced world simulator stemming from video generation. Positioned as a top-tier world model, LingBot-World offers the following features. (1) It maintains high fidelity and robust dynamics in a broad spectrum of environments, including realism, scientific contexts, cartoon styles, and beyond. (2) It enables a minute-level horizon while preserving contextual consistency over time, which is also known as \"long-term memory\". (3) It supports real-time interactivity, achieving a latency of under 1 second when producing 16 frames per second. We provide public access to the code and model in an effort to narrow the divide between open-source and closed-source technologies. We believe our release will empower the community with practical applications across areas like content creation, gaming, and robot learning.",
      "fetch_date": "2026-01-29",
      "num_comments": 2,
      "ai_summary": "LingBot-World is an open-source world simulator with high-fidelity dynamics, long-term memory capabilities, and real-time interactivity for diverse environments.",
      "ai_keywords": [
        "world simulator",
        "video generation",
        "world model",
        "long-term memory",
        "real-time interactivity"
      ]
    }
  },
  {
    "id": "9cef780d0ecdc7bc89377cb0a062dde8",
    "source": "huggingface",
    "type": "paper",
    "title": "Why Steering Works: Toward a Unified View of Language Model Parameter Dynamics",
    "description": "Methods for controlling large language models (LLMs), including local weight fine-tuning, LoRA-based adaptation, and activation-based interventions, are often studied in isolation, obscuring their con...<br/>Upvotes: 13<br/>GitHub Stars: 2714<br/>Authors: Ziwen Xu, Chenyan Wu, Hengyu Sun<br/>ðŸ”— <a href=\"https://github.com/zjunlp/EasyEdit/blob/main/examples/SPLIT.md\">GitHub</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2602.02343\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2602.02343",
    "external_url": "https://github.com/zjunlp/EasyEdit/blob/main/examples/SPLIT.md",
    "published_date": "2026-02-02T12:04:36.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2602.02343",
      "upvotes": 13,
      "github_stars": 2714,
      "github_url": "https://github.com/zjunlp/EasyEdit/blob/main/examples/SPLIT.md",
      "project_url": "",
      "authors": [
        "Ziwen Xu",
        "Chenyan Wu",
        "Hengyu Sun",
        "Haiwen Hong",
        "Mengru Wang",
        "Yunzhi Yao",
        "Longtao Huang",
        "Hui Xue",
        "Shumin Deng",
        "Zhixuan Chu",
        "Huajun Chen",
        "Ningyu Zhang"
      ],
      "summary": "Methods for controlling large language models (LLMs), including local weight fine-tuning, LoRA-based adaptation, and activation-based interventions, are often studied in isolation, obscuring their connections and making comparison difficult. In this work, we present a unified view that frames these interventions as dynamic weight updates induced by a control signal, placing them within a single conceptual framework. Building on this view, we propose a unified preference-utility analysis that separates control effects into preference, defined as the tendency toward a target concept, and utility, defined as coherent and task-valid generation, and measures both on a shared log-odds scale using polarity-paired contrastive examples. Across methods, we observe a consistent trade-off between preference and utility: stronger control increases preference while predictably reducing utility. We further explain this behavior through an activation manifold perspective, in which control shifts representations along target-concept directions to enhance preference, while utility declines primarily when interventions push representations off the model's valid-generation manifold. Finally, we introduce a new steering approach SPLIT guided by this analysis that improves preference while better preserving utility. Code is available at https://github.com/zjunlp/EasyEdit/blob/main/examples/SPLIT.md.",
      "fetch_date": "2026-02-03",
      "num_comments": 3,
      "ai_summary": "Large language model control methods are unified under a dynamic weight update framework, revealing a preference-utility trade-off and enabling improved steering through SPLIT approach.",
      "ai_keywords": [
        "local weight fine-tuning",
        "LoRA-based adaptation",
        "activation-based interventions",
        "dynamic weight updates",
        "preference-utility analysis",
        "control signal",
        "polarity-paired contrastive examples",
        "activation manifold",
        "valid-generation manifold",
        "SPLIT"
      ]
    }
  },
  {
    "id": "99cad11185f3c1e5d4dd41eca2b2005a",
    "source": "huggingface",
    "type": "paper",
    "title": "RLinf-USER: A Unified and Extensible System for Real-World Online Policy Learning in Embodied AI",
    "description": "Online policy learning directly in the physical world is a promising yet challenging direction for embodied intelligence. Unlike simulation, real-world systems cannot be arbitrarily accelerated, cheap...<br/>Upvotes: 50<br/>GitHub Stars: 2476<br/>Authors: Hongzhi Zang, Shu'ang Yu, Hao Lin<br/>ðŸ”— <a href=\"https://github.com/RLinf/RLinf/blob/main/examples/embodiment/run_realworld_async.sh\">GitHub</a><br/>ðŸ”— <a href=\"https://rlinf.readthedocs.io/en/latest/rst_source/publications/rlinf_user.html\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2602.07837\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2602.07837",
    "external_url": "https://github.com/RLinf/RLinf/blob/main/examples/embodiment/run_realworld_async.sh",
    "published_date": "2026-02-08T01:23:43.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2602.07837",
      "upvotes": 50,
      "github_stars": 2476,
      "github_url": "https://github.com/RLinf/RLinf/blob/main/examples/embodiment/run_realworld_async.sh",
      "project_url": "https://rlinf.readthedocs.io/en/latest/rst_source/publications/rlinf_user.html",
      "authors": [
        "Hongzhi Zang",
        "Shu'ang Yu",
        "Hao Lin",
        "Tianxing Zhou",
        "Zefang Huang",
        "Zhen Guo",
        "Xin Xu",
        "Jiakai Zhou",
        "Yuze Sheng",
        "Shizhe Zhang",
        "Feng Gao",
        "Wenhao Tang",
        "Yufeng Yue",
        "Quanlu Zhang",
        "Xinlei Chen",
        "Chao Yu",
        "Yu Wang"
      ],
      "summary": "Online policy learning directly in the physical world is a promising yet challenging direction for embodied intelligence. Unlike simulation, real-world systems cannot be arbitrarily accelerated, cheaply reset, or massively replicated, which makes scalable data collection, heterogeneous deployment, and long-horizon effective training difficult. These challenges suggest that real-world policy learning is not only an algorithmic issue but fundamentally a systems problem. We present USER, a Unified and extensible SystEm for Real-world online policy learning. USER treats physical robots as first-class hardware resources alongside GPUs through a unified hardware abstraction layer, enabling automatic discovery, management, and scheduling of heterogeneous robots. To address cloud-edge communication, USER introduces an adaptive communication plane with tunneling-based networking, distributed data channels for traffic localization, and streaming-multiprocessor-aware weight synchronization to regulate GPU-side overhead. On top of this infrastructure, USER organizes learning as a fully asynchronous framework with a persistent, cache-aware buffer, enabling efficient long-horizon experiments with robust crash recovery and reuse of historical data. In addition, USER provides extensible abstractions for rewards, algorithms, and policies, supporting online imitation or reinforcement learning of CNN/MLP, generative policies, and large vision-language-action (VLA) models within a unified pipeline. Results in both simulation and the real world show that USER enables multi-robot coordination, heterogeneous manipulators, edge-cloud collaboration with large models, and long-running asynchronous training, offering a unified and extensible systems foundation for real-world online policy learning.",
      "fetch_date": "2026-02-10",
      "num_comments": 2,
      "ai_summary": "USER is a unified systems framework that enables scalable, asynchronous online policy learning in physical robots by treating them as first-class hardware resources and supporting diverse learning paradigms including VLA models.",
      "ai_keywords": [
        "online policy learning",
        "embodied intelligence",
        "real-world systems",
        "heterogeneous robots",
        "hardware abstraction layer",
        "adaptive communication plane",
        "tunneling-based networking",
        "distributed data channels",
        "streaming-multiprocessor-aware weight synchronization",
        "asynchronous framework",
        "cache-aware buffer",
        "crash recovery",
        "reinforcement learning",
        "imitation learning",
        "vision-language-action models",
        "multi-robot coordination",
        "edge-cloud collaboration"
      ]
    }
  }
]