[
  {
    "id": "9f022f923749f1889fd7797edcd67136",
    "source": "huggingface",
    "type": "paper",
    "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
    "description": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To ach...<br/>Upvotes: 218<br/>GitHub Stars: 365<br/>Authors: Shih-Yang Liu, Xin Dong, Ximing Lu<br/>ðŸ”— <a href=\"https://github.com/NVlabs/GDPO\">GitHub</a><br/>ðŸ”— <a href=\"https://nvlabs.github.io/GDPO/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2601.05242\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2601.05242",
    "external_url": "https://github.com/NVlabs/GDPO",
    "published_date": "2026-01-08T13:59:24.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2601.05242",
      "upvotes": 218,
      "github_stars": 365,
      "github_url": "https://github.com/NVlabs/GDPO",
      "project_url": "https://nvlabs.github.io/GDPO/",
      "authors": [
        "Shih-Yang Liu",
        "Xin Dong",
        "Ximing Lu",
        "Shizhe Diao",
        "Peter Belcak",
        "Mingjie Liu",
        "Min-Hung Chen",
        "Hongxu Yin",
        "Yu-Chiang Frank Wang",
        "Kwang-Ting Cheng",
        "Yejin Choi",
        "Jan Kautz",
        "Pavlo Molchanov"
      ],
      "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.",
      "fetch_date": "2026-01-09",
      "num_comments": 9,
      "ai_summary": "Multi-reward reinforcement learning suffers from reward normalization collapse in GRPO, which GDPO addresses by decoupling reward normalization for improved training stability and performance across reasoning tasks.",
      "ai_keywords": [
        "Reinforcement learning",
        "Group Relative Policy Optimization",
        "multi-reward setting",
        "policy optimization",
        "Group reward-Decoupled Normalization Policy Optimization",
        "reward normalization",
        "advantage values",
        "training stability",
        "multi-reward reinforcement learning"
      ]
    }
  },
  {
    "id": "01505ff128b7cc4069a838f067f22852",
    "source": "huggingface",
    "type": "paper",
    "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning",
    "description": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perfor...<br/>Upvotes: 210<br/>GitHub Stars: 139<br/>Authors: Chengwen Liu, Xiaomin Yu, Zhuoyue Chang<br/>ðŸ”— <a href=\"https://github.com/QuantaAlpha/VideoDR-Benchmark\">GitHub</a><br/>ðŸ”— <a href=\"https://videodr-benchmark.github.io/#/home\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2601.06943\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2601.06943",
    "external_url": "https://github.com/QuantaAlpha/VideoDR-Benchmark",
    "published_date": "2026-01-11T10:07:37.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2601.06943",
      "upvotes": 210,
      "github_stars": 139,
      "github_url": "https://github.com/QuantaAlpha/VideoDR-Benchmark",
      "project_url": "https://videodr-benchmark.github.io/#/home",
      "authors": [
        "Chengwen Liu",
        "Xiaomin Yu",
        "Zhuoyue Chang",
        "Zhe Huang",
        "Shuo Zhang",
        "Heng Lian",
        "Kunyi Wang",
        "Rui Xu",
        "Sen Hu",
        "Jianheng Hou",
        "Hao Peng",
        "Chengwei Qin",
        "Xiaobin Hu",
        "Hong Peng",
        "Ronghao Chen",
        "Huacan Wang"
      ],
      "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.",
      "fetch_date": "2026-01-13",
      "num_comments": 7,
      "ai_summary": "VideoDR benchmark enables video question answering by combining cross-frame visual extraction, web retrieval, and multi-hop reasoning in open-domain settings.",
      "ai_keywords": [
        "video question answering",
        "cross-frame visual anchor extraction",
        "interactive web retrieval",
        "multi-hop reasoning",
        "multimodal large language models",
        "Workflow paradigm",
        "Agentic paradigm",
        "goal drift",
        "long-horizon consistency"
      ]
    }
  },
  {
    "id": "7d8f529e24394b1b5750aeba9852eed4",
    "source": "huggingface",
    "type": "paper",
    "title": "BabyVision: Visual Reasoning Beyond Language",
    "description": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understandin...<br/>Upvotes: 195<br/>GitHub Stars: 173<br/>Authors: Liang Chen, Weichu Xie, Yiyan Liang<br/>ðŸ”— <a href=\"https://github.com/UniPat-AI/BabyVision\">GitHub</a><br/>ðŸ”— <a href=\"https://unipat.ai/blog/BabyVision\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2601.06521\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2601.06521",
    "external_url": "https://github.com/UniPat-AI/BabyVision",
    "published_date": "2026-01-10T05:42:44.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2601.06521",
      "upvotes": 195,
      "github_stars": 173,
      "github_url": "https://github.com/UniPat-AI/BabyVision",
      "project_url": "https://unipat.ai/blog/BabyVision",
      "authors": [
        "Liang Chen",
        "Weichu Xie",
        "Yiyan Liang",
        "Hongfeng He",
        "Hans Zhao",
        "Zhibo Yang",
        "Zhiqi Huang",
        "Haoning Wu",
        "Haoyu Lu",
        "Y. charles",
        "Yiping Bao",
        "Yuantao Fan",
        "Guopeng Li",
        "Haiyang Shen",
        "Xuanzhong Chen",
        "Wendong Xu",
        "Shuzheng Si",
        "Zefan Cai",
        "Wenhao Chai",
        "Ziqi Huang",
        "Fangfu Liu",
        "Tianyu Liu",
        "Baobao Chang",
        "Xiaobo Hu",
        "Kaiyuan Chen",
        "Yixin Ren",
        "Yang Liu",
        "Yuan Gong",
        "Kuan Li"
      ],
      "summary": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.",
      "fetch_date": "2026-01-13",
      "num_comments": 6,
      "ai_summary": "Current multimodal large language models exhibit significant gaps in fundamental visual understanding compared to human children, as demonstrated by the BabyVision benchmark.",
      "ai_keywords": [
        "Multimodal LLMs",
        "visual reasoning",
        "core visual skills",
        "BabyVision benchmark",
        "visual perception",
        "visual primitives"
      ]
    }
  },
  {
    "id": "ed6630b4b50db2ee02e2a5ddb5f32ac1",
    "source": "huggingface",
    "type": "paper",
    "title": "STEP3-VL-10B Technical Report",
    "description": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized t...<br/>Upvotes: 193<br/>GitHub Stars: 389<br/>Authors: Ailin Huang, Chengyuan Yao, Chunrui Han<br/>ðŸ”— <a href=\"https://github.com/stepfun-ai/Step3-VL-10B\">GitHub</a><br/>ðŸ”— <a href=\"https://stepfun-ai.github.io/Step3-VL-10B\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2601.09668\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2601.09668",
    "external_url": "https://github.com/stepfun-ai/Step3-VL-10B",
    "published_date": "2026-01-14T12:58:24.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2601.09668",
      "upvotes": 193,
      "github_stars": 389,
      "github_url": "https://github.com/stepfun-ai/Step3-VL-10B",
      "project_url": "https://stepfun-ai.github.io/Step3-VL-10B",
      "authors": [
        "Ailin Huang",
        "Chengyuan Yao",
        "Chunrui Han",
        "Fanqi Wan",
        "Hangyu Guo",
        "Haoran Lv",
        "Hongyu Zhou",
        "Jia Wang",
        "Jian Zhou",
        "Jianjian Sun",
        "Jingcheng Hu",
        "Kangheng Lin",
        "Liang Zhao",
        "Mitt Huang",
        "Song Yuan",
        "Wenwen Qu",
        "Xiangfeng Wang",
        "Yanlin Lai",
        "Yingxiu Zhao",
        "Yinmin Zhang",
        "Yukang Shi",
        "Yuyang Chen",
        "Zejia Weng",
        "Ziyang Meng",
        "Ang Li",
        "Aobo Kong",
        "Bo Dong",
        "Changyi Wan",
        "David Wang",
        "Di Qi",
        "Dingming Li",
        "En Yu",
        "Guopeng Li",
        "Haiquan Yin",
        "Han Zhou",
        "Hanshan Zhang",
        "Haolong Yan",
        "Hebin Zhou",
        "Hongbo Peng",
        "Jiaran Zhang",
        "Jiashu Lv",
        "Jiayi Fu",
        "Jie Cheng",
        "Jie Zhou",
        "Jisheng Yin",
        "Jingjing Xie",
        "Jingwei Wu",
        "Jun Zhang",
        "Junfeng Liu",
        "Kaijun Tan",
        "Kaiwen Yan",
        "Liangyu Chen",
        "Lina Chen",
        "Mingliang Li",
        "Qian Zhao",
        "Quan Sun",
        "Shaoliang Pang",
        "Shengjie Fan",
        "Shijie Shang",
        "Siyuan Zhang",
        "Tianhao You",
        "Wei Ji",
        "Wuxun Xie",
        "Xiaobo Yang",
        "Xiaojie Hou",
        "Xiaoran Jiao",
        "Xiaoxiao Ren",
        "Xiangwen Kong",
        "Xin Huang",
        "Xin Wu",
        "Xing Chen",
        "Xinran Wang",
        "Xuelin Zhang",
        "Yana Wei",
        "Yang Li",
        "Yanming Xu",
        "Yeqing Shen",
        "Yuang Peng",
        "Yue Peng",
        "Yu Zhou",
        "Yusheng Li",
        "Yuxiang Yang",
        "Yuyang Zhang",
        "Zhe Xie",
        "Zhewei Huang",
        "Zhenyi Lu",
        "Zhimin Fan",
        "Zihui Cheng",
        "Daxin Jiang",
        "Qi Han",
        "Xiangyu Zhang",
        "Yibo Zhu",
        "Zheng Ge"
      ],
      "summary": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.",
      "fetch_date": "2026-01-16",
      "num_comments": 6,
      "ai_summary": "STEP3-VL-10B achieves superior multimodal performance through unified pre-training with a language-aligned Perception Encoder and Qwen3-8B decoder, combined with scaled post-training and Parallel Coordinated Reasoning for efficient large-scale visual reasoning.",
      "ai_keywords": [
        "multimodal tokens",
        "Perception Encoder",
        "Qwen3-8B decoder",
        "vision-language synergy",
        "reinforcement learning",
        "Parallel Coordinated Reasoning",
        "test-time compute",
        "visual hypotheses",
        "MMBench",
        "MMMU",
        "AIME2025",
        "MathVision"
      ]
    }
  },
  {
    "id": "60666ed538be4ebda8058b1f954547e6",
    "source": "huggingface",
    "type": "paper",
    "title": "Green-VLA: Staged Vision-Language-Action Model for Generalist Robots",
    "description": "We introduce Green-VLA, a staged Vision-Language-Action (VLA) framework for real-world deployment on the Green humanoid robot while maintaining generalization across diverse embodiments. Green-VLA fol...<br/>Upvotes: 192<br/>GitHub Stars: 29<br/>Authors: I. Apanasevich, M. Artemyev, R. Babakyan<br/>ðŸ”— <a href=\"https://github.com/greenvla/GreenVLA\">GitHub</a><br/>ðŸ”— <a href=\"https://greenvla.github.io\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2602.00919\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2602.00919",
    "external_url": "https://github.com/greenvla/GreenVLA",
    "published_date": "2026-01-31T17:13:23.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2602.00919",
      "upvotes": 192,
      "github_stars": 29,
      "github_url": "https://github.com/greenvla/GreenVLA",
      "project_url": "https://greenvla.github.io",
      "authors": [
        "I. Apanasevich",
        "M. Artemyev",
        "R. Babakyan",
        "P. Fedotova",
        "D. Grankin",
        "E. Kupryashin",
        "A. Misailidi",
        "D. Nerus",
        "A. Nutalapati",
        "G. Sidorov",
        "I. Efremov",
        "M. Gerasyov",
        "D. Pikurov",
        "Y. Senchenko",
        "S. Davidenko",
        "D. Kulikov",
        "M. Sultankin",
        "K. Askarbek",
        "O. Shamanin",
        "D. Statovoy",
        "E. Zalyaev",
        "I. Zorin",
        "A. Letkin",
        "E. Rusakov",
        "A. Silchenko",
        "V. Vorobyov",
        "S. Sobolnikov",
        "A. Postnikov"
      ],
      "summary": "We introduce Green-VLA, a staged Vision-Language-Action (VLA) framework for real-world deployment on the Green humanoid robot while maintaining generalization across diverse embodiments. Green-VLA follows a five stage curriculum: (L0) foundational VLMs, (L1) multimodal grounding, (R0) multi-embodiment pretraining, (R1) embodiment-specific adaptation, and (R2) reinforcement-learning (RL) policy alignment. We couple a scalable data-processing pipeline (3,000 hours of demonstrations) with temporal alignment and quality filtering, and use a unified, embodiment-aware action interface enabling a single policy to control humanoids, mobile manipulators, and fixed-base arms. At inference, the VLA controller is enhanced with episode-progress prediction, out-of-distribution detection, and joint-prediction-based guidance to improve safety and precise target selection. Experiments on Simpler BRIDGE WidowX and CALVIN ABC-D, as well as real-robot evaluations, show strong generalization and performance gains from RL alignment in success rate, robustness, and long-horizon efficiency.",
      "fetch_date": "2026-02-03",
      "num_comments": 3,
      "ai_summary": "Green-VLA is a five-stage vision-language-action framework for real-world robot deployment that achieves generalization across different robot embodiments through multimodal training and reinforcement learning.",
      "ai_keywords": [
        "Vision-Language-Action",
        "multimodal grounding",
        "multi-embodiment pretraining",
        "embodiment-specific adaptation",
        "reinforcement-learning",
        "episode-progress prediction",
        "out-of-distribution detection",
        "joint-prediction-based guidance"
      ]
    }
  },
  {
    "id": "c030a09f9828ad54ac0c86051210dce0",
    "source": "huggingface",
    "type": "paper",
    "title": "Agentic Reasoning for Large Language Models",
    "description": "Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world se...<br/>Upvotes: 190<br/>GitHub Stars: 879<br/>Authors: Tianxin Wei, Ting-Wei Li, Zhining Liu<br/>ðŸ”— <a href=\"https://github.com/weitianxin/Awesome-Agentic-Reasoning\">GitHub</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2601.12538\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2601.12538",
    "external_url": "https://github.com/weitianxin/Awesome-Agentic-Reasoning",
    "published_date": "2026-01-18T13:58:23.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2601.12538",
      "upvotes": 190,
      "github_stars": 879,
      "github_url": "https://github.com/weitianxin/Awesome-Agentic-Reasoning",
      "project_url": "",
      "authors": [
        "Tianxin Wei",
        "Ting-Wei Li",
        "Zhining Liu",
        "Xuying Ning",
        "Ze Yang",
        "Jiaru Zou",
        "Zhichen Zeng",
        "Ruizhong Qiu",
        "Xiao Lin",
        "Dongqi Fu",
        "Zihao Li",
        "Mengting Ai",
        "Duo Zhou",
        "Wenxuan Bao",
        "Yunzhe Li",
        "Gaotang Li",
        "Cheng Qian",
        "Yu Wang",
        "Xiangru Tang",
        "Yin Xiao",
        "Liri Fang",
        "Hui Liu",
        "Xianfeng Tang",
        "Yuji Zhang",
        "Chi Wang",
        "Jiaxuan You",
        "Heng Ji",
        "Hanghang Tong",
        "Jingrui He"
      ],
      "summary": "Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.",
      "fetch_date": "2026-01-22",
      "num_comments": 6,
      "ai_summary": "Agentic reasoning redefines large language models as autonomous agents capable of planning, acting, and learning through continuous interaction in dynamic environments across single-agent and multi-agent frameworks.",
      "ai_keywords": [
        "large language models",
        "agentic reasoning",
        "autonomous agents",
        "planning",
        "tool use",
        "search",
        "feedback",
        "memory",
        "adaptation",
        "collaborative settings",
        "coordination",
        "knowledge sharing",
        "reinforcement learning",
        "supervised fine-tuning",
        "in-context reasoning",
        "post-training reasoning",
        "real-world applications",
        "benchmarks",
        "thought and action",
        "world modeling",
        "scalable multi-agent training",
        "governance"
      ]
    }
  },
  {
    "id": "e6cd663cc8eaf53e2ed564e037a90af3",
    "source": "huggingface",
    "type": "paper",
    "title": "Qwen3-TTS Technical Report",
    "description": "In this report, we present the Qwen3-TTS series, a family of advanced multilingual, controllable, robust, and streaming text-to-speech models. Qwen3-TTS supports state-of-the-art 3-second voice clonin...<br/>Upvotes: 57<br/>GitHub Stars: 6804<br/>Authors: Hangrui Hu, Xinfa Zhu, Ting He<br/>ðŸ”— <a href=\"https://github.com/QwenLM/Qwen3-TTS\">GitHub</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2601.15621\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2601.15621",
    "external_url": "https://github.com/QwenLM/Qwen3-TTS",
    "published_date": "2026-01-21T22:51:43.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2601.15621",
      "upvotes": 57,
      "github_stars": 6804,
      "github_url": "https://github.com/QwenLM/Qwen3-TTS",
      "project_url": "",
      "authors": [
        "Hangrui Hu",
        "Xinfa Zhu",
        "Ting He",
        "Dake Guo",
        "Bin Zhang",
        "Xiong Wang",
        "Zhifang Guo",
        "Ziyue Jiang",
        "Hongkun Hao",
        "Zishan Guo",
        "Xinyu Zhang",
        "Pei Zhang",
        "Baosong Yang",
        "Jin Xu",
        "Jingren Zhou",
        "Junyang Lin"
      ],
      "summary": "In this report, we present the Qwen3-TTS series, a family of advanced multilingual, controllable, robust, and streaming text-to-speech models. Qwen3-TTS supports state-of-the-art 3-second voice cloning and description-based control, allowing both the creation of entirely novel voices and fine-grained manipulation over the output speech. Trained on over 5 million hours of speech data spanning 10 languages, Qwen3-TTS adopts a dual-track LM architecture for real-time synthesis, coupled with two speech tokenizers: 1) Qwen-TTS-Tokenizer-25Hz is a single-codebook codec emphasizing semantic content, which offers seamlessly integration with Qwen-Audio and enables streaming waveform reconstruction via a block-wise DiT. 2) Qwen-TTS-Tokenizer-12Hz achieves extreme bitrate reduction and ultra-low-latency streaming, enabling immediate first-packet emission (97,ms) through its 12.5 Hz, 16-layer multi-codebook design and a lightweight causal ConvNet. Extensive experiments indicate state-of-the-art performance across diverse objective and subjective benchmark (e.g., TTS multilingual test set, InstructTTSEval, and our long speech test set). To facilitate community research and development, we release both tokenizers and models under the Apache 2.0 license.",
      "fetch_date": "2026-01-23",
      "num_comments": 1,
      "ai_summary": "The Qwen3-TTS series presents advanced multilingual text-to-speech models with voice cloning and controllable speech generation capabilities, utilizing dual-track LM architecture and specialized speech tokenizers for efficient streaming synthesis.",
      "ai_keywords": [
        "text-to-speech",
        "voice cloning",
        "dual-track LM architecture",
        "speech tokenizers",
        "Qwen-TTS-Tokenizer-25Hz",
        "Qwen-TTS-Tokenizer-12Hz",
        "DiT",
        "ConvNet",
        "streaming waveform reconstruction",
        "multilingual",
        "controllable speech generation"
      ]
    }
  },
  {
    "id": "4abe591dec20d02c68570faea169dfcb",
    "source": "huggingface",
    "type": "paper",
    "title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization",
    "description": "Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual ef...<br/>Upvotes: 119<br/>GitHub Stars: 4365<br/>Authors: Yuchen Shi, Yuzheng Cai, Siqi Cai<br/>ðŸ”— <a href=\"https://github.com/TencentCloudADP/youtu-agent\">GitHub</a><br/>ðŸ”— <a href=\"https://tencentcloudadp.github.io/youtu-agent/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2512.24615\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2512.24615",
    "external_url": "https://github.com/TencentCloudADP/youtu-agent",
    "published_date": "2025-12-30T23:17:36.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2512.24615",
      "upvotes": 119,
      "github_stars": 4365,
      "github_url": "https://github.com/TencentCloudADP/youtu-agent",
      "project_url": "https://tencentcloudadp.github.io/youtu-agent/",
      "authors": [
        "Yuchen Shi",
        "Yuzheng Cai",
        "Siqi Cai",
        "Zihan Xu",
        "Lichao Chen",
        "Yulei Qin",
        "Zhijian Zhou",
        "Xiang Fei",
        "Chaofan Qiu",
        "Xiaoyu Tan",
        "Gang Li",
        "Zongyi Li",
        "Haojia Lin",
        "Guocan Cai",
        "Yong Mao",
        "Yunsheng Wu",
        "Ke Li",
        "Xing Sun"
      ],
      "summary": "Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose Youtu-Agent, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a Workflow mode for standard tasks and a Meta-Agent mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an Agent Practice module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an Agent RL module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\\%) and GAIA (72.8\\%) using open-weight models. Our automated generation pipeline achieves over 81\\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\\% and +5.4\\% respectively. Moreover, our Agent RL training achieves 40\\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\\% and 21\\% on Maths and general/multi-hop QA benchmarks.",
      "fetch_date": "2026-01-05",
      "num_comments": 5,
      "ai_summary": "",
      "ai_keywords": []
    }
  },
  {
    "id": "7f5474609ca7dc9a4bb2a0fecae8667c",
    "source": "huggingface",
    "type": "paper",
    "title": "LTX-2: Efficient Joint Audio-Visual Foundation Model",
    "description": "Recent text-to-video diffusion models can generate compelling video sequences, yet they remain silent -- missing the semantic, emotional, and atmospheric cues that audio provides. We introduce LTX-2, ...<br/>Upvotes: 146<br/>GitHub Stars: 3495<br/>Authors: Yoav HaCohen, Benny Brazowski, Nisan Chiprut<br/>ðŸ”— <a href=\"https://github.com/Lightricks/LTX-2\">GitHub</a><br/>ðŸ”— <a href=\"https://app.ltx.studio/ltx-2-playground/i2v\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2601.03233\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2601.03233",
    "external_url": "https://github.com/Lightricks/LTX-2",
    "published_date": "2026-01-06T13:24:41.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2601.03233",
      "upvotes": 146,
      "github_stars": 3495,
      "github_url": "https://github.com/Lightricks/LTX-2",
      "project_url": "https://app.ltx.studio/ltx-2-playground/i2v",
      "authors": [
        "Yoav HaCohen",
        "Benny Brazowski",
        "Nisan Chiprut",
        "Yaki Bitterman",
        "Andrew Kvochko",
        "Avishai Berkowitz",
        "Daniel Shalem",
        "Daphna Lifschitz",
        "Dudu Moshe",
        "Eitan Porat",
        "Eitan Richardson",
        "Guy Shiran",
        "Itay Chachy",
        "Jonathan Chetboun",
        "Michael Finkelson",
        "Michael Kupchick",
        "Nir Zabari",
        "Nitzan Guetta",
        "Noa Kotler",
        "Ofir Bibi",
        "Ori Gordon",
        "Poriya Panet",
        "Roi Benita",
        "Shahar Armon",
        "Victor Kulikov",
        "Yaron Inger",
        "Yonatan Shiftan",
        "Zeev Melumian",
        "Zeev Farbman"
      ],
      "summary": "Recent text-to-video diffusion models can generate compelling video sequences, yet they remain silent -- missing the semantic, emotional, and atmospheric cues that audio provides. We introduce LTX-2, an open-source foundational model capable of generating high-quality, temporally synchronized audiovisual content in a unified manner. LTX-2 consists of an asymmetric dual-stream transformer with a 14B-parameter video stream and a 5B-parameter audio stream, coupled through bidirectional audio-video cross-attention layers with temporal positional embeddings and cross-modality AdaLN for shared timestep conditioning. This architecture enables efficient training and inference of a unified audiovisual model while allocating more capacity for video generation than audio generation. We employ a multilingual text encoder for broader prompt understanding and introduce a modality-aware classifier-free guidance (modality-CFG) mechanism for improved audiovisual alignment and controllability. Beyond generating speech, LTX-2 produces rich, coherent audio tracks that follow the characters, environment, style, and emotion of each scene -- complete with natural background and foley elements. In our evaluations, the model achieves state-of-the-art audiovisual quality and prompt adherence among open-source systems, while delivering results comparable to proprietary models at a fraction of their computational cost and inference time. All model weights and code are publicly released.",
      "fetch_date": "2026-01-07",
      "num_comments": 4,
      "ai_summary": "LTX-2 is an open-source audiovisual diffusion model that generates synchronized video and audio content using a dual-stream transformer architecture with cross-modal attention and classifier-free guidance.",
      "ai_keywords": [
        "text-to-video diffusion models",
        "audiovisual content",
        "dual-stream transformer",
        "cross-attention layers",
        "temporal positional embeddings",
        "AdaLN",
        "classifier-free guidance",
        "modality-aware classifier-free guidance",
        "multilingual text encoder",
        "diffusion models"
      ]
    }
  },
  {
    "id": "e4456066efdcdf46fe4b108089570f93",
    "source": "huggingface",
    "type": "paper",
    "title": "HeartMuLa: A Family of Open Sourced Music Foundation Models",
    "description": "We present a family of open-source Music Foundation Models designed to advance large-scale music understanding and generation across diverse tasks and modalities. Our framework consists of four major ...<br/>Upvotes: 41<br/>GitHub Stars: 2933<br/>Authors: Dongchao Yang, Yuxin Xie, Yuguo Yin<br/>ðŸ”— <a href=\"https://github.com/HeartMuLa/heartlib\">GitHub</a><br/>ðŸ”— <a href=\"https://heartmula.github.io/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2601.10547\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2601.10547",
    "external_url": "https://github.com/HeartMuLa/heartlib",
    "published_date": "2026-01-15T11:14:25.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2601.10547",
      "upvotes": 41,
      "github_stars": 2933,
      "github_url": "https://github.com/HeartMuLa/heartlib",
      "project_url": "https://heartmula.github.io/",
      "authors": [
        "Dongchao Yang",
        "Yuxin Xie",
        "Yuguo Yin",
        "Zheyu Wang",
        "Xiaoyu Yi",
        "Gongxi Zhu",
        "Xiaolong Weng",
        "Zihan Xiong",
        "Yingzhe Ma",
        "Dading Cong",
        "Jingliang Liu",
        "Zihang Huang",
        "Jinghan Ru",
        "Rongjie Huang",
        "Haoran Wan",
        "Peixu Wang",
        "Kuoxi Yu",
        "Helin Wang",
        "Liming Liang",
        "Xianwei Zhuang",
        "Yuanyuan Wang",
        "Haohan Guo",
        "Junjie Cao",
        "Zeqian Ju",
        "Songxiang Liu",
        "Yuewen Cao",
        "Heming Weng",
        "Yuexian Zou"
      ],
      "summary": "We present a family of open-source Music Foundation Models designed to advance large-scale music understanding and generation across diverse tasks and modalities. Our framework consists of four major components: (1) HeartCLAP, an audio-text alignment model; (2) HeartTranscriptor, a robust lyric recognition model optimized for real-world music scenarios; and (3) HeartCodec, a low-frame-rate (12.5 Hz) yet high-fidelity music codec tokenizer that captures long-range musical structure while preserving fine-grained acoustic details and enabling efficient autoregressive modeling; (4) HeartMuLa, an LLM-based song generation model capable of synthesizing high-fidelity music under rich, user-controllable conditions (e.g., textual style descriptions, lyrics, and reference audio). In addition, it provides two specialized modes: (i) fine-grained musical attribute control, which allows users to specify the style of different song sections (e.g., intro, verse, chorus) using natural language prompts; and (ii) short, engaging music generation, which is suitable as background music for short videos. Lastly, HeartMuLa improves significantly when scaled to 7B parameters. For the first time, we show that a Suno-level, commercial-grade system can be reproduced using academic-scale data and GPU resources. We expect these foundation models to serve as strong baselines for future research and to facilitate practical applications in multimodal content production.",
      "fetch_date": "2026-01-16",
      "num_comments": 4,
      "ai_summary": "A suite of open-source music foundation models is introduced, featuring components for audio-text alignment, lyric recognition, music coding, and large language model-based song generation with controllable attributes and scalable parameterization.",
      "ai_keywords": [
        "Music Foundation Models",
        "audio-text alignment model",
        "lyric recognition model",
        "music codec tokenizer",
        "LLM-based song generation model",
        "autoregressive modeling",
        "musical attribute control",
        "short video background music generation",
        "parameter scaling"
      ]
    }
  },
  {
    "id": "9cef780d0ecdc7bc89377cb0a062dde8",
    "source": "huggingface",
    "type": "paper",
    "title": "Why Steering Works: Toward a Unified View of Language Model Parameter Dynamics",
    "description": "Methods for controlling large language models (LLMs), including local weight fine-tuning, LoRA-based adaptation, and activation-based interventions, are often studied in isolation, obscuring their con...<br/>Upvotes: 11<br/>GitHub Stars: 2710<br/>Authors: Ziwen Xu, Chenyan Wu, Hengyu Sun<br/>ðŸ”— <a href=\"https://github.com/zjunlp/EasyEdit/blob/main/examples/SPLIT.md\">GitHub</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2602.02343\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2602.02343",
    "external_url": "https://github.com/zjunlp/EasyEdit/blob/main/examples/SPLIT.md",
    "published_date": "2026-02-02T12:04:36.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2602.02343",
      "upvotes": 11,
      "github_stars": 2710,
      "github_url": "https://github.com/zjunlp/EasyEdit/blob/main/examples/SPLIT.md",
      "project_url": "",
      "authors": [
        "Ziwen Xu",
        "Chenyan Wu",
        "Hengyu Sun",
        "Haiwen Hong",
        "Mengru Wang",
        "Yunzhi Yao",
        "Longtao Huang",
        "Hui Xue",
        "Shumin Deng",
        "Zhixuan Chu",
        "Huajun Chen",
        "Ningyu Zhang"
      ],
      "summary": "Methods for controlling large language models (LLMs), including local weight fine-tuning, LoRA-based adaptation, and activation-based interventions, are often studied in isolation, obscuring their connections and making comparison difficult. In this work, we present a unified view that frames these interventions as dynamic weight updates induced by a control signal, placing them within a single conceptual framework. Building on this view, we propose a unified preference-utility analysis that separates control effects into preference, defined as the tendency toward a target concept, and utility, defined as coherent and task-valid generation, and measures both on a shared log-odds scale using polarity-paired contrastive examples. Across methods, we observe a consistent trade-off between preference and utility: stronger control increases preference while predictably reducing utility. We further explain this behavior through an activation manifold perspective, in which control shifts representations along target-concept directions to enhance preference, while utility declines primarily when interventions push representations off the model's valid-generation manifold. Finally, we introduce a new steering approach SPLIT guided by this analysis that improves preference while better preserving utility. Code is available at https://github.com/zjunlp/EasyEdit/blob/main/examples/SPLIT.md.",
      "fetch_date": "2026-02-03",
      "num_comments": 3,
      "ai_summary": "Large language model control methods are unified under a dynamic weight update framework, revealing a preference-utility trade-off and enabling improved steering through SPLIT approach.",
      "ai_keywords": [
        "local weight fine-tuning",
        "LoRA-based adaptation",
        "activation-based interventions",
        "dynamic weight updates",
        "preference-utility analysis",
        "control signal",
        "polarity-paired contrastive examples",
        "activation manifold",
        "valid-generation manifold",
        "SPLIT"
      ]
    }
  },
  {
    "id": "42acbf549db8c79c9ce60d3d38eb8c52",
    "source": "huggingface",
    "type": "paper",
    "title": "SimpleMem: Efficient Lifelong Memory for LLM Agents",
    "description": "To support reliable long-term interaction in complex environments, LLM agents require memory systems that efficiently manage historical experiences. Existing approaches either retain full interaction ...<br/>Upvotes: 36<br/>GitHub Stars: 2692<br/>Authors: Jiaqi Liu, Yaofeng Su, Peng Xia<br/>ðŸ”— <a href=\"https://github.com/aiming-lab/SimpleMem\">GitHub</a><br/>ðŸ”— <a href=\"https://aiming-lab.github.io/SimpleMem-Page/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2601.02553\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2601.02553",
    "external_url": "https://github.com/aiming-lab/SimpleMem",
    "published_date": "2026-01-05T16:02:49.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2601.02553",
      "upvotes": 36,
      "github_stars": 2692,
      "github_url": "https://github.com/aiming-lab/SimpleMem",
      "project_url": "https://aiming-lab.github.io/SimpleMem-Page/",
      "authors": [
        "Jiaqi Liu",
        "Yaofeng Su",
        "Peng Xia",
        "Siwei Han",
        "Zeyu Zheng",
        "Cihang Xie",
        "Mingyu Ding",
        "Huaxiu Yao"
      ],
      "summary": "To support reliable long-term interaction in complex environments, LLM agents require memory systems that efficiently manage historical experiences. Existing approaches either retain full interaction histories via passive context extension, leading to substantial redundancy, or rely on iterative reasoning to filter noise, incurring high token costs. To address this challenge, we introduce SimpleMem, an efficient memory framework based on semantic lossless compression. We propose a three-stage pipeline designed to maximize information density and token utilization: (1) Semantic Structured Compression, which applies entropy-aware filtering to distill unstructured interactions into compact, multi-view indexed memory units; (2) Recursive Memory Consolidation, an asynchronous process that integrates related units into higher-level abstract representations to reduce redundancy; and (3) Adaptive Query-Aware Retrieval, which dynamically adjusts retrieval scope based on query complexity to construct precise context efficiently. Experiments on benchmark datasets show that our method consistently outperforms baseline approaches in accuracy, retrieval efficiency, and inference cost, achieving an average F1 improvement of 26.4% while reducing inference-time token consumption by up to 30-fold, demonstrating a superior balance between performance and efficiency. Code is available at https://github.com/aiming-lab/SimpleMem.",
      "fetch_date": "2026-01-06",
      "num_comments": 3,
      "ai_summary": "",
      "ai_keywords": []
    }
  }
]