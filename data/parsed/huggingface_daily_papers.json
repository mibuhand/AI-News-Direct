[
  {
    "id": "b1cff772289aab941c862bdff282297c",
    "source": "huggingface",
    "type": "paper",
    "title": "ERNIE 5.0 Technical Report",
    "description": "In this report, we introduce ERNIE 5.0, a natively autoregressive foundation model desinged for unified multimodal understanding and generation across text, image, video, and audio. All modalities are...<br/>Upvotes: 236<br/>Authors: Haifeng Wang, Hua Wu, Tian Wu<br/>ðŸ”— <a href=\"https://huggingface.co/papers/2602.04705\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2602.04705",
    "external_url": "",
    "published_date": "2026-02-04T11:18:15.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2602.04705",
      "upvotes": 236,
      "github_stars": 0,
      "github_url": "",
      "project_url": "",
      "authors": [
        "Haifeng Wang",
        "Hua Wu",
        "Tian Wu",
        "Yu Sun",
        "Jing Liu",
        "Dianhai Yu",
        "Yanjun Ma",
        "Jingzhou He",
        "Zhongjun He",
        "Dou Hong",
        "Qiwen Liu",
        "Shuohuan Wang",
        "Junyuan Shang",
        "Zhenyu Zhang",
        "Yuchen Ding",
        "Jinle Zeng",
        "Jiabin Yang",
        "Liang Shen",
        "Ruibiao Chen",
        "Weichong Yin",
        "Siyu Ding",
        "Dai Dai",
        "Shikun Feng",
        "Siqi Bao",
        "Bolei He",
        "Yan Chen",
        "Zhenyu Jiao",
        "Ruiqing Zhang",
        "Zeyu Chen",
        "Qingqing Dang",
        "Kaipeng Deng",
        "Jiajun Jiang",
        "Enlei Gong",
        "Guoxia Wang",
        "Yanlin Sha",
        "Yi Liu",
        "Yehan Zheng",
        "Weijian Xu",
        "Jiaxiang Liu",
        "Zengfeng Zeng",
        "Yingqi Qu",
        "Zhongli Li",
        "Zhengkun Zhang",
        "Xiyang Wang",
        "Zixiang Xu",
        "Xinchao Xu",
        "Zhengjie Huang",
        "Dong Wang",
        "Bingjin Chen",
        "Yue Chang",
        "Xing Yuan",
        "Shiwei Huang",
        "Qiao Zhao",
        "Xinzhe Ding",
        "Shuangshuang Qiao",
        "Baoshan Yang",
        "Bihong Tang",
        "Bin Li",
        "Bingquan Wang",
        "Binhan Tang",
        "Binxiong Zheng",
        "Bo Cui",
        "Bo Ke",
        "Bo Zhang",
        "Bowen Zhang",
        "Boyan Zhang",
        "Boyang Liu",
        "Caiji Zhang",
        "Can Li",
        "Chang Xu",
        "Chao Pang",
        "Chao Zhang",
        "Chaoyi Yuan",
        "Chen Chen",
        "Cheng Cui",
        "Chenlin Yin",
        "Chun Gan",
        "Chunguang Chai",
        "Chuyu Fang",
        "Cuiyun Han",
        "Dan Zhang",
        "Danlei Feng",
        "Danxiang Zhu",
        "Dong Sun",
        "Dongbo Li",
        "Dongdong Li",
        "Dongdong Liu",
        "Dongxue Liu",
        "Fan Ding",
        "Fan Hu",
        "Fan Li",
        "Fan Mo",
        "Feisheng Wu",
        "Fengwei Liu",
        "Gangqiang Hu",
        "Gaofeng Lu",
        "Gaopeng Yong",
        "Gexiao Tian",
        "Guan Wang",
        "Guangchen Ni",
        "Guangshuo Wu",
        "Guanzhong Wang",
        "Guihua Liu",
        "Guishun Li",
        "Haibin Li",
        "Haijian Liang",
        "Haipeng Ming",
        "Haisu Wang",
        "Haiyang Lu",
        "Haiye Lin",
        "Han Zhou",
        "Hangting Lou",
        "Hanwen Du",
        "Hanzhi Zhang",
        "Hao Chen",
        "Hao Du",
        "Hao Liu",
        "Hao Zhou",
        "Haochen Jiang",
        "Haodong Tian",
        "Haoshuang Wang",
        "Haozhe Geng",
        "Heju Yin",
        "Hong Chen",
        "Hongchen Xue",
        "Hongen Liu",
        "Honggeng Zhang",
        "Hongji Xu",
        "Hongwei Chen",
        "Hongyang Zhang",
        "Hongyuan Zhang",
        "Hua Lu",
        "Huan Chen",
        "Huan Wang",
        "Huang He",
        "Hui Liu",
        "Hui Zhong",
        "Huibin Ruan",
        "Jiafeng Lu",
        "Jiage Liang",
        "Jiahao Hu",
        "Jiahao Hu",
        "Jiajie Yang",
        "Jialin Li",
        "Jian Chen",
        "Jian Wu",
        "Jianfeng Yang",
        "Jianguang Jiang",
        "Jianhua Wang",
        "Jianye Chen",
        "Jiaodi Liu",
        "Jiarui Zhou",
        "Jiawei Lv",
        "Jiaxin Zhou",
        "Jiaxuan Liu",
        "Jie Han",
        "Jie Sun",
        "Jiefan Fang",
        "Jihan Liu",
        "Jihua Liu",
        "Jing Hu",
        "Jing Qian",
        "Jing Yan",
        "Jingdong Du",
        "Jingdong Wang",
        "Jingjing Wu",
        "Jingyong Li",
        "Jinheng Wang",
        "Jinjin Li",
        "Jinliang Lu",
        "Jinlin Yu",
        "Jinnan Liu",
        "Jixiang Feng",
        "Jiyi Huang",
        "Jiyuan Zhang",
        "Jun Liang",
        "Jun Xia",
        "Jun Yu",
        "Junda Chen",
        "Junhao Feng",
        "Junhong Xiang",
        "Junliang Li",
        "Kai Liu",
        "Kailun Chen",
        "Kairan Su",
        "Kang Hu",
        "Kangkang Zhou",
        "Ke Chen",
        "Ke Wei",
        "Kui Huang",
        "Kun Wu",
        "Kunbin Chen",
        "Lei Han",
        "Lei Sun",
        "Lei Wen",
        "Linghui Meng",
        "Linhao Yu",
        "Liping Ouyang",
        "Liwen Zhang",
        "Longbin Ji",
        "Longzhi Wang",
        "Meng Sun",
        "Meng Tian",
        "Mengfei Li",
        "Mengqi Zeng",
        "Mengyu Zhang",
        "Ming Hong",
        "Mingcheng Zhou",
        "Mingming Huang",
        "Mingxin Chen",
        "Mingzhu Cai",
        "Naibin Gu",
        "Nemin Qiu",
        "Nian Wang",
        "Peng Qiu",
        "Peng Zhao",
        "Pengyu Zou",
        "Qi Wang",
        "Qi Xin",
        "Qian Wang",
        "Qiang Zhu",
        "Qianhui Luo",
        "Qianwei Yang",
        "Qianyue He",
        "Qifei Wu",
        "Qinrui Li",
        "Qiwen Bao",
        "Quan Zhang",
        "Quanxiang Liu",
        "Qunyi Xie",
        "Rongrui Zhan",
        "Rufeng Dai",
        "Rui Peng",
        "Ruian Liu",
        "Ruihao Xu",
        "Ruijie Wang",
        "Ruixi Zhang",
        "Ruixuan Liu",
        "Runsheng Shi",
        "Ruting Wang",
        "Senbo Kang",
        "Shan Lu",
        "Shaofei Yu",
        "Shaotian Gong",
        "Shenwei Hu",
        "Shifeng Zheng",
        "Shihao Guo",
        "Shilong Fan",
        "Shiqin Liu",
        "Shiwei Gu",
        "Shixi Zhang",
        "Shuai Yao",
        "Shuang Zhang",
        "Shuangqiao Liu",
        "Shuhao Liang",
        "Shuwei He",
        "Shuwen Yang",
        "Sijun He",
        "Siming Dai",
        "Siming Wu",
        "Siyi Long",
        "Songhe Deng",
        "Suhui Dong",
        "Suyin Liang",
        "Teng Hu",
        "Tianchan Xu",
        "Tianliang Lv",
        "Tianmeng Yang",
        "Tianyi Wei",
        "Tiezhu Gao",
        "Ting Sun",
        "Ting Zhang",
        "Tingdan Luo",
        "Wei He",
        "Wei Luan",
        "Wei Yin",
        "Wei Zhang",
        "Wei Zhou",
        "Weibao Gong",
        "Weibin Li",
        "Weicheng Huang",
        "Weichong Dang",
        "Weiguo Zhu",
        "Weilong Zhang",
        "Weiqi Tan",
        "Wen Huang",
        "Wenbin Chang",
        "Wenjing Du",
        "Wenlong Miao",
        "Wenpei Luo",
        "Wenquan Wu",
        "Xi Shi",
        "Xi Zhao",
        "Xiang Gao",
        "Xiangguo Zhang",
        "Xiangrui Yu",
        "Xiangsen Wang",
        "Xiangzhe Wang",
        "Xianlong Luo",
        "Xianying Ma",
        "Xiao Tan",
        "Xiaocong Lin",
        "Xiaofei Wang",
        "Xiaofeng Peng",
        "Xiaofeng Wu",
        "Xiaojian Xu",
        "Xiaolan Yuan",
        "Xiaopeng Cui",
        "Xiaotian Han",
        "Xiaoxiong Liu",
        "Xiaoxu Fei",
        "Xiaoxuan Wu",
        "Xiaoyu Wang",
        "Xiaoyu Zhang",
        "Xin Sun",
        "Xin Wang",
        "Xinhui Huang",
        "Xinming Zhu",
        "Xintong Yu",
        "Xinyi Xu",
        "Xinyu Wang",
        "Xiuxian Li",
        "XuanShi Zhu",
        "Xue Xu",
        "Xueying Lv",
        "Xuhong Li",
        "Xulong Wei",
        "Xuyi Chen",
        "Yabing Shi",
        "Yafeng Wang",
        "Yamei Li",
        "Yan Liu",
        "Yanfu Cheng",
        "Yang Gao",
        "Yang Liang",
        "Yang Wang",
        "Yang Wang",
        "Yang Yang",
        "Yanlong Liu",
        "Yannian Fu",
        "Yanpeng Wang",
        "Yanzheng Lin",
        "Yao Chen",
        "Yaozong Shen",
        "Yaqian Han",
        "Yehua Yang",
        "Yekun Chai",
        "Yesong Wang",
        "Yi Song",
        "Yichen Zhang",
        "Yifei Wang",
        "Yifeng Guo",
        "Yifeng Kou",
        "Yilong Chen",
        "Yilong Guo",
        "Yiming Wang",
        "Ying Chen",
        "Ying Wang",
        "Yingsheng Wu",
        "Yingzhan Lin",
        "Yinqi Yang",
        "Yiran Xing",
        "Yishu Lei",
        "Yixiang Tu",
        "Yiyan Chen",
        "Yong Zhang",
        "Yonghua Li",
        "Yongqiang Ma",
        "Yongxing Dai",
        "Yongyue Zhang",
        "Yu Ran",
        "Yu Sun",
        "Yu-Wen Michael Zhang",
        "Yuang Liu",
        "Yuanle Liu",
        "Yuanyuan Zhou",
        "Yubo Zhang",
        "Yuchen Han",
        "Yucheng Wang",
        "Yude Gao",
        "Yuedong Luo",
        "Yuehu Dong",
        "Yufeng Hu",
        "Yuhui Cao",
        "Yuhui Yun",
        "Yukun Chen",
        "Yukun Gao",
        "Yukun Li",
        "Yumeng Zhang",
        "Yun Fan",
        "Yun Ma",
        "Yunfei Zhang",
        "Yunshen Xie",
        "Yuping Xu",
        "Yuqin Zhang",
        "Yuqing Liu",
        "Yurui Li",
        "Yuwen Wang",
        "Yuxiang Lu",
        "Zefeng Cai",
        "Zelin Zhao",
        "Zelun Zhang",
        "Zenan Lin",
        "Zezhao Dong",
        "Zhaowu Pan",
        "Zhaoyu Liu",
        "Zhe Dong",
        "Zhe Zhang",
        "Zhen Zhang",
        "Zhengfan Wu",
        "Zhengrui Wei",
        "Zhengsheng Ning",
        "Zhenxing Li",
        "Zhenyu Li",
        "Zhenyu Qian",
        "Zhenyun Li",
        "Zhi Li",
        "Zhichao Chen",
        "Zhicheng Dong",
        "Zhida Feng",
        "Zhifan Feng",
        "Zhihao Deng",
        "Zhijin Yu",
        "Zhiyang Chen",
        "Zhonghui Zheng",
        "Zhuangzhuang Guo",
        "Zhujun Zhang",
        "Zhuo Sun",
        "Zichang Liu",
        "Zihan Lin",
        "Zihao Huang",
        "Zihe Zhu",
        "Ziheng Zhao",
        "Ziping Chen",
        "Zixuan Zhu",
        "Ziyang Xu",
        "Ziyi Liang",
        "Ziyuan Gao"
      ],
      "summary": "In this report, we introduce ERNIE 5.0, a natively autoregressive foundation model desinged for unified multimodal understanding and generation across text, image, video, and audio. All modalities are trained from scratch under a unified next-group-of-tokens prediction objective, based on an ultra-sparse mixture-of-experts (MoE) architecture with modality-agnostic expert routing. To address practical challenges in large-scale deployment under diverse resource constraints, ERNIE 5.0 adopts a novel elastic training paradigm. Within a single pre-training run, the model learns a family of sub-models with varying depths, expert capacities, and routing sparsity, enabling flexible trade-offs among performance, model size, and inference latency in memory- or time-constrained scenarios. Moreover, we systematically address the challenges of scaling reinforcement learning to unified foundation models, thereby guaranteeing efficient and stable post-training under ultra-sparse MoE architectures and diverse multimodal settings. Extensive experiments demonstrate that ERNIE 5.0 achieves strong and balanced performance across multiple modalities. To the best of our knowledge, among publicly disclosed models, ERNIE 5.0 represents the first production-scale realization of a trillion-parameter unified autoregressive model that supports both multimodal understanding and generation. To facilitate further research, we present detailed visualizations of modality-agnostic expert routing in the unified model, alongside comprehensive empirical analysis of elastic training, aiming to offer profound insights to the community.",
      "fetch_date": "2026-02-05",
      "num_comments": 4,
      "ai_summary": "ERNIE 5.0 is a production-scale trillion-parameter autoregressive model that unifies multimodal understanding and generation through sparse MoE architecture and elastic training.",
      "ai_keywords": [
        "autoregressive foundation model",
        "unified multimodal understanding",
        "unified next-group-of-tokens prediction objective",
        "mixture-of-experts",
        "modality-agnostic expert routing",
        "elastic training paradigm",
        "reinforcement learning",
        "sparse MoE architecture"
      ]
    }
  },
  {
    "id": "60666ed538be4ebda8058b1f954547e6",
    "source": "huggingface",
    "type": "paper",
    "title": "Green-VLA: Staged Vision-Language-Action Model for Generalist Robots",
    "description": "We introduce Green-VLA, a staged Vision-Language-Action (VLA) framework for real-world deployment on the Green humanoid robot while maintaining generalization across diverse embodiments. Green-VLA fol...<br/>Upvotes: 236<br/>GitHub Stars: 31<br/>Authors: I. Apanasevich, M. Artemyev, R. Babakyan<br/>ðŸ”— <a href=\"https://github.com/greenvla/GreenVLA\">GitHub</a><br/>ðŸ”— <a href=\"https://greenvla.github.io\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2602.00919\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2602.00919",
    "external_url": "https://github.com/greenvla/GreenVLA",
    "published_date": "2026-01-31T17:13:23.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2602.00919",
      "upvotes": 236,
      "github_stars": 31,
      "github_url": "https://github.com/greenvla/GreenVLA",
      "project_url": "https://greenvla.github.io",
      "authors": [
        "I. Apanasevich",
        "M. Artemyev",
        "R. Babakyan",
        "P. Fedotova",
        "D. Grankin",
        "E. Kupryashin",
        "A. Misailidi",
        "D. Nerus",
        "A. Nutalapati",
        "G. Sidorov",
        "I. Efremov",
        "M. Gerasyov",
        "D. Pikurov",
        "Y. Senchenko",
        "S. Davidenko",
        "D. Kulikov",
        "M. Sultankin",
        "K. Askarbek",
        "O. Shamanin",
        "D. Statovoy",
        "E. Zalyaev",
        "I. Zorin",
        "A. Letkin",
        "E. Rusakov",
        "A. Silchenko",
        "V. Vorobyov",
        "S. Sobolnikov",
        "A. Postnikov"
      ],
      "summary": "We introduce Green-VLA, a staged Vision-Language-Action (VLA) framework for real-world deployment on the Green humanoid robot while maintaining generalization across diverse embodiments. Green-VLA follows a five stage curriculum: (L0) foundational VLMs, (L1) multimodal grounding, (R0) multi-embodiment pretraining, (R1) embodiment-specific adaptation, and (R2) reinforcement-learning (RL) policy alignment. We couple a scalable data-processing pipeline (3,000 hours of demonstrations) with temporal alignment and quality filtering, and use a unified, embodiment-aware action interface enabling a single policy to control humanoids, mobile manipulators, and fixed-base arms. At inference, the VLA controller is enhanced with episode-progress prediction, out-of-distribution detection, and joint-prediction-based guidance to improve safety and precise target selection. Experiments on Simpler BRIDGE WidowX and CALVIN ABC-D, as well as real-robot evaluations, show strong generalization and performance gains from RL alignment in success rate, robustness, and long-horizon efficiency.",
      "fetch_date": "2026-02-03",
      "num_comments": 6,
      "ai_summary": "Green-VLA is a five-stage vision-language-action framework for real-world robot deployment that achieves generalization across different robot embodiments through multimodal training and reinforcement learning.",
      "ai_keywords": [
        "Vision-Language-Action",
        "multimodal grounding",
        "multi-embodiment pretraining",
        "embodiment-specific adaptation",
        "reinforcement-learning",
        "episode-progress prediction",
        "out-of-distribution detection",
        "joint-prediction-based guidance"
      ]
    }
  },
  {
    "id": "9f022f923749f1889fd7797edcd67136",
    "source": "huggingface",
    "type": "paper",
    "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
    "description": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To ach...<br/>Upvotes: 222<br/>GitHub Stars: 377<br/>Authors: Shih-Yang Liu, Xin Dong, Ximing Lu<br/>ðŸ”— <a href=\"https://github.com/NVlabs/GDPO\">GitHub</a><br/>ðŸ”— <a href=\"https://nvlabs.github.io/GDPO/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2601.05242\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2601.05242",
    "external_url": "https://github.com/NVlabs/GDPO",
    "published_date": "2026-01-08T13:59:24.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2601.05242",
      "upvotes": 222,
      "github_stars": 377,
      "github_url": "https://github.com/NVlabs/GDPO",
      "project_url": "https://nvlabs.github.io/GDPO/",
      "authors": [
        "Shih-Yang Liu",
        "Xin Dong",
        "Ximing Lu",
        "Shizhe Diao",
        "Peter Belcak",
        "Mingjie Liu",
        "Min-Hung Chen",
        "Hongxu Yin",
        "Yu-Chiang Frank Wang",
        "Kwang-Ting Cheng",
        "Yejin Choi",
        "Jan Kautz",
        "Pavlo Molchanov"
      ],
      "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.",
      "fetch_date": "2026-01-09",
      "num_comments": 9,
      "ai_summary": "Multi-reward reinforcement learning suffers from reward normalization collapse in GRPO, which GDPO addresses by decoupling reward normalization for improved training stability and performance across reasoning tasks.",
      "ai_keywords": [
        "Reinforcement learning",
        "Group Relative Policy Optimization",
        "multi-reward setting",
        "policy optimization",
        "Group reward-Decoupled Normalization Policy Optimization",
        "reward normalization",
        "advantage values",
        "training stability",
        "multi-reward reinforcement learning"
      ]
    }
  },
  {
    "id": "01505ff128b7cc4069a838f067f22852",
    "source": "huggingface",
    "type": "paper",
    "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning",
    "description": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perfor...<br/>Upvotes: 210<br/>GitHub Stars: 140<br/>Authors: Chengwen Liu, Xiaomin Yu, Zhuoyue Chang<br/>ðŸ”— <a href=\"https://github.com/QuantaAlpha/VideoDR-Benchmark\">GitHub</a><br/>ðŸ”— <a href=\"https://videodr-benchmark.github.io/#/home\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2601.06943\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2601.06943",
    "external_url": "https://github.com/QuantaAlpha/VideoDR-Benchmark",
    "published_date": "2026-01-11T10:07:37.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2601.06943",
      "upvotes": 210,
      "github_stars": 140,
      "github_url": "https://github.com/QuantaAlpha/VideoDR-Benchmark",
      "project_url": "https://videodr-benchmark.github.io/#/home",
      "authors": [
        "Chengwen Liu",
        "Xiaomin Yu",
        "Zhuoyue Chang",
        "Zhe Huang",
        "Shuo Zhang",
        "Heng Lian",
        "Kunyi Wang",
        "Rui Xu",
        "Sen Hu",
        "Jianheng Hou",
        "Hao Peng",
        "Chengwei Qin",
        "Xiaobin Hu",
        "Hong Peng",
        "Ronghao Chen",
        "Huacan Wang"
      ],
      "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.",
      "fetch_date": "2026-01-13",
      "num_comments": 7,
      "ai_summary": "VideoDR benchmark enables video question answering by combining cross-frame visual extraction, web retrieval, and multi-hop reasoning in open-domain settings.",
      "ai_keywords": [
        "video question answering",
        "cross-frame visual anchor extraction",
        "interactive web retrieval",
        "multi-hop reasoning",
        "multimodal large language models",
        "Workflow paradigm",
        "Agentic paradigm",
        "goal drift",
        "long-horizon consistency"
      ]
    }
  },
  {
    "id": "4f9739b510c1386a39502a9995b593ed",
    "source": "huggingface",
    "type": "paper",
    "title": "Kimi K2.5: Visual Agentic Intelligence",
    "description": "We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhan...<br/>Upvotes: 206<br/>GitHub Stars: 814<br/>Authors: Kimi Team, Tongtong Bai, Yifan Bai<br/>ðŸ”— <a href=\"https://github.com/MoonshotAI/Kimi-K2.5\">GitHub</a><br/>ðŸ”— <a href=\"https://www.kimi.com/blog/kimi-k2-5.html\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2602.02276\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2602.02276",
    "external_url": "https://github.com/MoonshotAI/Kimi-K2.5",
    "published_date": "2026-02-02T11:17:38.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2602.02276",
      "upvotes": 206,
      "github_stars": 814,
      "github_url": "https://github.com/MoonshotAI/Kimi-K2.5",
      "project_url": "https://www.kimi.com/blog/kimi-k2-5.html",
      "authors": [
        "Kimi Team",
        "Tongtong Bai",
        "Yifan Bai",
        "Yiping Bao",
        "S. H. Cai",
        "Yuan Cao",
        "Y. Charles",
        "H. S. Che",
        "Cheng Chen",
        "Guanduo Chen",
        "Huarong Chen",
        "Jia Chen",
        "Jiahao Chen",
        "Jianlong Chen",
        "Jun Chen",
        "Kefan Chen",
        "Liang Chen",
        "Ruijue Chen",
        "Xinhao Chen",
        "Yanru Chen",
        "Yanxu Chen",
        "Yicun Chen",
        "Yimin Chen",
        "Yingjiang Chen",
        "Yuankun Chen",
        "Yujie Chen",
        "Yutian Chen",
        "Zhirong Chen",
        "Ziwei Chen",
        "Dazhi Cheng",
        "Minghan Chu",
        "Jialei Cui",
        "Jiaqi Deng",
        "Muxi Diao",
        "Hao Ding",
        "Mengfan Dong",
        "Mengnan Dong",
        "Yuxin Dong",
        "Yuhao Dong",
        "Angang Du",
        "Chenzhuang Du",
        "Dikang Du",
        "Lingxiao Du",
        "Yulun Du",
        "Yu Fan",
        "Shengjun Fang",
        "Qiulin Feng",
        "Yichen Feng",
        "Garimugai Fu",
        "Kelin Fu",
        "Hongcheng Gao",
        "Tong Gao",
        "Yuyao Ge",
        "Shangyi Geng",
        "Chengyang Gong",
        "Xiaochen Gong",
        "Zhuoma Gongque",
        "Qizheng Gu",
        "Xinran Gu",
        "Yicheng Gu",
        "Longyu Guan",
        "Yuanying Guo",
        "Xiaoru Hao",
        "Weiran He",
        "Wenyang He",
        "Yunjia He",
        "Chao Hong",
        "Hao Hu",
        "Jiaxi Hu",
        "Yangyang Hu",
        "Zhenxing Hu",
        "Ke Huang",
        "Ruiyuan Huang",
        "Weixiao Huang",
        "Zhiqi Huang",
        "Tao Jiang",
        "Zhejun Jiang",
        "Xinyi Jin",
        "Yu Jing",
        "Guokun Lai",
        "Aidi Li",
        "C. Li",
        "Cheng Li",
        "Fang Li",
        "Guanghe Li",
        "Guanyu Li",
        "Haitao Li",
        "Haoyang Li",
        "Jia Li",
        "Jingwei Li",
        "Junxiong Li",
        "Lincan Li",
        "Mo Li",
        "Weihong Li",
        "Wentao Li",
        "Xinhang Li",
        "Xinhao Li",
        "Yang Li",
        "Yanhao Li",
        "Yiwei Li",
        "Yuxiao Li",
        "Zhaowei Li",
        "Zheming Li",
        "Weilong Liao",
        "Jiawei Lin",
        "Xiaohan Lin",
        "Zhishan Lin",
        "Zichao Lin",
        "Cheng Liu",
        "Chenyu Liu",
        "Hongzhang Liu",
        "Liang Liu",
        "Shaowei Liu",
        "Shudong Liu",
        "Shuran Liu",
        "Tianwei Liu",
        "Tianyu Liu",
        "Weizhou Liu",
        "Xiangyan Liu",
        "Yangyang Liu",
        "Yanming Liu",
        "Yibo Liu",
        "Yuanxin Liu",
        "Yue Liu",
        "Zhengying Liu",
        "Zhongnuo Liu",
        "Enzhe Lu",
        "Haoyu Lu",
        "Zhiyuan Lu",
        "Junyu Luo",
        "Tongxu Luo",
        "Yashuo Luo",
        "Long Ma",
        "Yingwei Ma",
        "Shaoguang Mao",
        "Yuan Mei",
        "Xin Men",
        "Fanqing Meng",
        "Zhiyong Meng",
        "Yibo Miao",
        "Minqing Ni",
        "Kun Ouyang",
        "Siyuan Pan",
        "Bo Pang",
        "Yuchao Qian",
        "Ruoyu Qin",
        "Zeyu Qin",
        "Jiezhong Qiu",
        "Bowen Qu",
        "Zeyu Shang",
        "Youbo Shao",
        "Tianxiao Shen",
        "Zhennan Shen",
        "Juanfeng Shi",
        "Lidong Shi",
        "Shengyuan Shi",
        "Feifan Song",
        "Pengwei Song",
        "Tianhui Song",
        "Xiaoxi Song",
        "Hongjin Su",
        "Jianlin Su",
        "Zhaochen Su",
        "Lin Sui",
        "Jinsong Sun",
        "Junyao Sun",
        "Tongyu Sun",
        "Flood Sung",
        "Yunpeng Tai",
        "Chuning Tang",
        "Heyi Tang",
        "Xiaojuan Tang",
        "Zhengyang Tang",
        "Jiawen Tao",
        "Shiyuan Teng",
        "Chaoran Tian",
        "Pengfei Tian",
        "Ao Wang",
        "Bowen Wang",
        "Chensi Wang",
        "Chuang Wang",
        "Congcong Wang",
        "Dingkun Wang",
        "Dinglu Wang",
        "Dongliang Wang",
        "Feng Wang",
        "Hailong Wang",
        "Haiming Wang",
        "Hengzhi Wang",
        "Huaqing Wang",
        "Hui Wang",
        "Jiahao Wang",
        "Jinhong Wang",
        "Jiuzheng Wang",
        "Kaixin Wang",
        "Linian Wang",
        "Qibin Wang",
        "Shengjie Wang",
        "Shuyi Wang",
        "Si Wang",
        "Wei Wang",
        "Xiaochen Wang",
        "Xinyuan Wang",
        "Yao Wang",
        "Yejie Wang",
        "Yipu Wang",
        "Yiqin Wang",
        "Yucheng Wang",
        "Yuzhi Wang",
        "Zhaoji Wang",
        "Zhaowei Wang",
        "Zhengtao Wang",
        "Zhexu Wang",
        "Zihan Wang",
        "Zizhe Wang",
        "Chu Wei",
        "Ming Wei",
        "Chuan Wen",
        "Zichen Wen",
        "Chengjie Wu",
        "Haoning Wu",
        "Junyan Wu",
        "Rucong Wu",
        "Wenhao Wu",
        "Yuefeng Wu",
        "Yuhao Wu",
        "Yuxin Wu",
        "Zijian Wu",
        "Chenjun Xiao",
        "Jin Xie",
        "Xiaotong Xie",
        "Yuchong Xie",
        "Yifei Xin",
        "Bowei Xing",
        "Boyu Xu",
        "Jianfan Xu",
        "Jing Xu",
        "Jinjing Xu",
        "L. H. Xu",
        "Lin Xu",
        "Suting Xu",
        "Weixin Xu",
        "Xinbo Xu",
        "Xinran Xu",
        "Yangchuan Xu",
        "Yichang Xu",
        "Yuemeng Xu",
        "Zelai Xu",
        "Ziyao Xu",
        "Junjie Yan",
        "Yuzi Yan",
        "Guangyao Yang",
        "Hao Yang",
        "Junwei Yang",
        "Kai Yang",
        "Ningyuan Yang",
        "Ruihan Yang",
        "Xiaofei Yang",
        "Xinlong Yang",
        "Ying Yang",
        "Yi Yang",
        "Yi Yang",
        "Zhen Yang",
        "Zhilin Yang",
        "Zonghan Yang",
        "Haotian Yao",
        "Dan Ye",
        "Wenjie Ye",
        "Zhuorui Ye",
        "Bohong Yin",
        "Chengzhen Yu",
        "Longhui Yu",
        "Tao Yu",
        "Tianxiang Yu",
        "Enming Yuan",
        "Mengjie Yuan",
        "Xiaokun Yuan",
        "Yang Yue",
        "Weihao Zeng",
        "Dunyuan Zha",
        "Haobing Zhan",
        "Dehao Zhang",
        "Hao Zhang",
        "Jin Zhang",
        "Puqi Zhang",
        "Qiao Zhang",
        "Rui Zhang",
        "Xiaobin Zhang",
        "Y. Zhang",
        "Yadong Zhang",
        "Yangkun Zhang",
        "Yichi Zhang",
        "Yizhi Zhang",
        "Yongting Zhang",
        "Yu Zhang",
        "Yushun Zhang",
        "Yutao Zhang",
        "Yutong Zhang",
        "Zheng Zhang",
        "Chenguang Zhao",
        "Feifan Zhao",
        "Jinxiang Zhao",
        "Shuai Zhao",
        "Xiangyu Zhao",
        "Yikai Zhao",
        "Zijia Zhao",
        "Huabin Zheng",
        "Ruihan Zheng",
        "Shaojie Zheng",
        "Tengyang Zheng",
        "Junfeng Zhong",
        "Longguang Zhong",
        "Weiming Zhong",
        "M. Zhou",
        "Runjie Zhou",
        "Xinyu Zhou",
        "Zaida Zhou",
        "Jinguo Zhu",
        "Liya Zhu",
        "Xinhao Zhu",
        "Yuxuan Zhu",
        "Zhen Zhu",
        "Jingze Zhuang",
        "Weiyu Zhuang",
        "Ying Zou",
        "Xinxing Zu"
      ],
      "summary": "We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhance each other. This includes a series of techniques such as joint text-vision pre-training, zero-vision SFT, and joint text-vision reinforcement learning. Building on this multimodal foundation, K2.5 introduces Agent Swarm, a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. Extensive evaluations show that Kimi K2.5 achieves state-of-the-art results across various domains including coding, vision, reasoning, and agentic tasks. Agent Swarm also reduces latency by up to 4.5times over single-agent baselines. We release the post-trained Kimi K2.5 model checkpoint to facilitate future research and real-world applications of agentic intelligence.",
      "fetch_date": "2026-02-03",
      "num_comments": 3,
      "ai_summary": "Kimi K2.5 is an open-source multimodal agentic model that enhances text and vision processing through joint optimization techniques and introduces Agent Swarm for parallel task execution.",
      "ai_keywords": [
        "multimodal agentic model",
        "joint text-vision pre-training",
        "zero-vision SFT",
        "joint text-vision reinforcement learning",
        "Agent Swarm",
        "self-directed parallel agent orchestration framework",
        "heterogeneous sub-problems"
      ]
    }
  },
  {
    "id": "7d8f529e24394b1b5750aeba9852eed4",
    "source": "huggingface",
    "type": "paper",
    "title": "BabyVision: Visual Reasoning Beyond Language",
    "description": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understandin...<br/>Upvotes: 195<br/>GitHub Stars: 175<br/>Authors: Liang Chen, Weichu Xie, Yiyan Liang<br/>ðŸ”— <a href=\"https://github.com/UniPat-AI/BabyVision\">GitHub</a><br/>ðŸ”— <a href=\"https://unipat.ai/blog/BabyVision\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2601.06521\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2601.06521",
    "external_url": "https://github.com/UniPat-AI/BabyVision",
    "published_date": "2026-01-10T05:42:44.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2601.06521",
      "upvotes": 195,
      "github_stars": 175,
      "github_url": "https://github.com/UniPat-AI/BabyVision",
      "project_url": "https://unipat.ai/blog/BabyVision",
      "authors": [
        "Liang Chen",
        "Weichu Xie",
        "Yiyan Liang",
        "Hongfeng He",
        "Hans Zhao",
        "Zhibo Yang",
        "Zhiqi Huang",
        "Haoning Wu",
        "Haoyu Lu",
        "Y. charles",
        "Yiping Bao",
        "Yuantao Fan",
        "Guopeng Li",
        "Haiyang Shen",
        "Xuanzhong Chen",
        "Wendong Xu",
        "Shuzheng Si",
        "Zefan Cai",
        "Wenhao Chai",
        "Ziqi Huang",
        "Fangfu Liu",
        "Tianyu Liu",
        "Baobao Chang",
        "Xiaobo Hu",
        "Kaiyuan Chen",
        "Yixin Ren",
        "Yang Liu",
        "Yuan Gong",
        "Kuan Li"
      ],
      "summary": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.",
      "fetch_date": "2026-01-13",
      "num_comments": 6,
      "ai_summary": "Current multimodal large language models exhibit significant gaps in fundamental visual understanding compared to human children, as demonstrated by the BabyVision benchmark.",
      "ai_keywords": [
        "Multimodal LLMs",
        "visual reasoning",
        "core visual skills",
        "BabyVision benchmark",
        "visual perception",
        "visual primitives"
      ]
    }
  },
  {
    "id": "e6cd663cc8eaf53e2ed564e037a90af3",
    "source": "huggingface",
    "type": "paper",
    "title": "Qwen3-TTS Technical Report",
    "description": "In this report, we present the Qwen3-TTS series, a family of advanced multilingual, controllable, robust, and streaming text-to-speech models. Qwen3-TTS supports state-of-the-art 3-second voice clonin...<br/>Upvotes: 60<br/>GitHub Stars: 7143<br/>Authors: Hangrui Hu, Xinfa Zhu, Ting He<br/>ðŸ”— <a href=\"https://github.com/QwenLM/Qwen3-TTS\">GitHub</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2601.15621\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2601.15621",
    "external_url": "https://github.com/QwenLM/Qwen3-TTS",
    "published_date": "2026-01-21T22:51:43.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2601.15621",
      "upvotes": 60,
      "github_stars": 7143,
      "github_url": "https://github.com/QwenLM/Qwen3-TTS",
      "project_url": "",
      "authors": [
        "Hangrui Hu",
        "Xinfa Zhu",
        "Ting He",
        "Dake Guo",
        "Bin Zhang",
        "Xiong Wang",
        "Zhifang Guo",
        "Ziyue Jiang",
        "Hongkun Hao",
        "Zishan Guo",
        "Xinyu Zhang",
        "Pei Zhang",
        "Baosong Yang",
        "Jin Xu",
        "Jingren Zhou",
        "Junyang Lin"
      ],
      "summary": "In this report, we present the Qwen3-TTS series, a family of advanced multilingual, controllable, robust, and streaming text-to-speech models. Qwen3-TTS supports state-of-the-art 3-second voice cloning and description-based control, allowing both the creation of entirely novel voices and fine-grained manipulation over the output speech. Trained on over 5 million hours of speech data spanning 10 languages, Qwen3-TTS adopts a dual-track LM architecture for real-time synthesis, coupled with two speech tokenizers: 1) Qwen-TTS-Tokenizer-25Hz is a single-codebook codec emphasizing semantic content, which offers seamlessly integration with Qwen-Audio and enables streaming waveform reconstruction via a block-wise DiT. 2) Qwen-TTS-Tokenizer-12Hz achieves extreme bitrate reduction and ultra-low-latency streaming, enabling immediate first-packet emission (97,ms) through its 12.5 Hz, 16-layer multi-codebook design and a lightweight causal ConvNet. Extensive experiments indicate state-of-the-art performance across diverse objective and subjective benchmark (e.g., TTS multilingual test set, InstructTTSEval, and our long speech test set). To facilitate community research and development, we release both tokenizers and models under the Apache 2.0 license.",
      "fetch_date": "2026-01-23",
      "num_comments": 1,
      "ai_summary": "The Qwen3-TTS series presents advanced multilingual text-to-speech models with voice cloning and controllable speech generation capabilities, utilizing dual-track LM architecture and specialized speech tokenizers for efficient streaming synthesis.",
      "ai_keywords": [
        "text-to-speech",
        "voice cloning",
        "dual-track LM architecture",
        "speech tokenizers",
        "Qwen-TTS-Tokenizer-25Hz",
        "Qwen-TTS-Tokenizer-12Hz",
        "DiT",
        "ConvNet",
        "streaming waveform reconstruction",
        "multilingual",
        "controllable speech generation"
      ]
    }
  },
  {
    "id": "e4456066efdcdf46fe4b108089570f93",
    "source": "huggingface",
    "type": "paper",
    "title": "HeartMuLa: A Family of Open Sourced Music Foundation Models",
    "description": "We present a family of open-source Music Foundation Models designed to advance large-scale music understanding and generation across diverse tasks and modalities. Our framework consists of four major ...<br/>Upvotes: 42<br/>GitHub Stars: 3196<br/>Authors: Dongchao Yang, Yuxin Xie, Yuguo Yin<br/>ðŸ”— <a href=\"https://github.com/HeartMuLa/heartlib\">GitHub</a><br/>ðŸ”— <a href=\"https://heartmula.github.io/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2601.10547\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2601.10547",
    "external_url": "https://github.com/HeartMuLa/heartlib",
    "published_date": "2026-01-15T11:14:25.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2601.10547",
      "upvotes": 42,
      "github_stars": 3196,
      "github_url": "https://github.com/HeartMuLa/heartlib",
      "project_url": "https://heartmula.github.io/",
      "authors": [
        "Dongchao Yang",
        "Yuxin Xie",
        "Yuguo Yin",
        "Zheyu Wang",
        "Xiaoyu Yi",
        "Gongxi Zhu",
        "Xiaolong Weng",
        "Zihan Xiong",
        "Yingzhe Ma",
        "Dading Cong",
        "Jingliang Liu",
        "Zihang Huang",
        "Jinghan Ru",
        "Rongjie Huang",
        "Haoran Wan",
        "Peixu Wang",
        "Kuoxi Yu",
        "Helin Wang",
        "Liming Liang",
        "Xianwei Zhuang",
        "Yuanyuan Wang",
        "Haohan Guo",
        "Junjie Cao",
        "Zeqian Ju",
        "Songxiang Liu",
        "Yuewen Cao",
        "Heming Weng",
        "Yuexian Zou"
      ],
      "summary": "We present a family of open-source Music Foundation Models designed to advance large-scale music understanding and generation across diverse tasks and modalities. Our framework consists of four major components: (1) HeartCLAP, an audio-text alignment model; (2) HeartTranscriptor, a robust lyric recognition model optimized for real-world music scenarios; and (3) HeartCodec, a low-frame-rate (12.5 Hz) yet high-fidelity music codec tokenizer that captures long-range musical structure while preserving fine-grained acoustic details and enabling efficient autoregressive modeling; (4) HeartMuLa, an LLM-based song generation model capable of synthesizing high-fidelity music under rich, user-controllable conditions (e.g., textual style descriptions, lyrics, and reference audio). In addition, it provides two specialized modes: (i) fine-grained musical attribute control, which allows users to specify the style of different song sections (e.g., intro, verse, chorus) using natural language prompts; and (ii) short, engaging music generation, which is suitable as background music for short videos. Lastly, HeartMuLa improves significantly when scaled to 7B parameters. For the first time, we show that a Suno-level, commercial-grade system can be reproduced using academic-scale data and GPU resources. We expect these foundation models to serve as strong baselines for future research and to facilitate practical applications in multimodal content production.",
      "fetch_date": "2026-01-16",
      "num_comments": 4,
      "ai_summary": "A suite of open-source music foundation models is introduced, featuring components for audio-text alignment, lyric recognition, music coding, and large language model-based song generation with controllable attributes and scalable parameterization.",
      "ai_keywords": [
        "Music Foundation Models",
        "audio-text alignment model",
        "lyric recognition model",
        "music codec tokenizer",
        "LLM-based song generation model",
        "autoregressive modeling",
        "musical attribute control",
        "short video background music generation",
        "parameter scaling"
      ]
    }
  },
  {
    "id": "9cef780d0ecdc7bc89377cb0a062dde8",
    "source": "huggingface",
    "type": "paper",
    "title": "Why Steering Works: Toward a Unified View of Language Model Parameter Dynamics",
    "description": "Methods for controlling large language models (LLMs), including local weight fine-tuning, LoRA-based adaptation, and activation-based interventions, are often studied in isolation, obscuring their con...<br/>Upvotes: 13<br/>GitHub Stars: 2711<br/>Authors: Ziwen Xu, Chenyan Wu, Hengyu Sun<br/>ðŸ”— <a href=\"https://github.com/zjunlp/EasyEdit/blob/main/examples/SPLIT.md\">GitHub</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2602.02343\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2602.02343",
    "external_url": "https://github.com/zjunlp/EasyEdit/blob/main/examples/SPLIT.md",
    "published_date": "2026-02-02T12:04:36.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2602.02343",
      "upvotes": 13,
      "github_stars": 2711,
      "github_url": "https://github.com/zjunlp/EasyEdit/blob/main/examples/SPLIT.md",
      "project_url": "",
      "authors": [
        "Ziwen Xu",
        "Chenyan Wu",
        "Hengyu Sun",
        "Haiwen Hong",
        "Mengru Wang",
        "Yunzhi Yao",
        "Longtao Huang",
        "Hui Xue",
        "Shumin Deng",
        "Zhixuan Chu",
        "Huajun Chen",
        "Ningyu Zhang"
      ],
      "summary": "Methods for controlling large language models (LLMs), including local weight fine-tuning, LoRA-based adaptation, and activation-based interventions, are often studied in isolation, obscuring their connections and making comparison difficult. In this work, we present a unified view that frames these interventions as dynamic weight updates induced by a control signal, placing them within a single conceptual framework. Building on this view, we propose a unified preference-utility analysis that separates control effects into preference, defined as the tendency toward a target concept, and utility, defined as coherent and task-valid generation, and measures both on a shared log-odds scale using polarity-paired contrastive examples. Across methods, we observe a consistent trade-off between preference and utility: stronger control increases preference while predictably reducing utility. We further explain this behavior through an activation manifold perspective, in which control shifts representations along target-concept directions to enhance preference, while utility declines primarily when interventions push representations off the model's valid-generation manifold. Finally, we introduce a new steering approach SPLIT guided by this analysis that improves preference while better preserving utility. Code is available at https://github.com/zjunlp/EasyEdit/blob/main/examples/SPLIT.md.",
      "fetch_date": "2026-02-03",
      "num_comments": 3,
      "ai_summary": "Large language model control methods are unified under a dynamic weight update framework, revealing a preference-utility trade-off and enabling improved steering through SPLIT approach.",
      "ai_keywords": [
        "local weight fine-tuning",
        "LoRA-based adaptation",
        "activation-based interventions",
        "dynamic weight updates",
        "preference-utility analysis",
        "control signal",
        "polarity-paired contrastive examples",
        "activation manifold",
        "valid-generation manifold",
        "SPLIT"
      ]
    }
  },
  {
    "id": "de0ab4deaf169a5f7442c17a4444ef3f",
    "source": "huggingface",
    "type": "paper",
    "title": "Advancing Open-source World Models",
    "description": "We present LingBot-World, an open-sourced world simulator stemming from video generation. Positioned as a top-tier world model, LingBot-World offers the following features. (1) It maintains high fidel...<br/>Upvotes: 119<br/>GitHub Stars: 2647<br/>Authors: Robbyant Team, Zelin Gao, Qiuyu Wang<br/>ðŸ”— <a href=\"https://github.com/Robbyant/lingbot-world/\">GitHub</a><br/>ðŸ”— <a href=\"https://technology.robbyant.com/lingbot-world\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2601.20540\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2601.20540",
    "external_url": "https://github.com/Robbyant/lingbot-world/",
    "published_date": "2026-01-28T07:37:01.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2601.20540",
      "upvotes": 119,
      "github_stars": 2647,
      "github_url": "https://github.com/Robbyant/lingbot-world/",
      "project_url": "https://technology.robbyant.com/lingbot-world",
      "authors": [
        "Robbyant Team",
        "Zelin Gao",
        "Qiuyu Wang",
        "Yanhong Zeng",
        "Jiapeng Zhu",
        "Ka Leong Cheng",
        "Yixuan Li",
        "Hanlin Wang",
        "Yinghao Xu",
        "Shuailei Ma",
        "Yihang Chen",
        "Jie Liu",
        "Yansong Cheng",
        "Yao Yao",
        "Jiayi Zhu",
        "Yihao Meng",
        "Kecheng Zheng",
        "Qingyan Bai",
        "Jingye Chen",
        "Zehong Shen",
        "Yue Yu",
        "Xing Zhu",
        "Yujun Shen",
        "Hao Ouyang"
      ],
      "summary": "We present LingBot-World, an open-sourced world simulator stemming from video generation. Positioned as a top-tier world model, LingBot-World offers the following features. (1) It maintains high fidelity and robust dynamics in a broad spectrum of environments, including realism, scientific contexts, cartoon styles, and beyond. (2) It enables a minute-level horizon while preserving contextual consistency over time, which is also known as \"long-term memory\". (3) It supports real-time interactivity, achieving a latency of under 1 second when producing 16 frames per second. We provide public access to the code and model in an effort to narrow the divide between open-source and closed-source technologies. We believe our release will empower the community with practical applications across areas like content creation, gaming, and robot learning.",
      "fetch_date": "2026-01-29",
      "num_comments": 2,
      "ai_summary": "LingBot-World is an open-source world simulator with high-fidelity dynamics, long-term memory capabilities, and real-time interactivity for diverse environments.",
      "ai_keywords": [
        "world simulator",
        "video generation",
        "world model",
        "long-term memory",
        "real-time interactivity"
      ]
    }
  },
  {
    "id": "3ca1c9448a1392dc62dcb1e40f0a07ae",
    "source": "huggingface",
    "type": "paper",
    "title": "PaperBanana: Automating Academic Illustration for AI Scientists",
    "description": "Despite rapid advances in autonomous AI scientists powered by language models, generating publication-ready illustrations remains a labor-intensive bottleneck in the research workflow. To lift this bu...<br/>Upvotes: 140<br/>GitHub Stars: 2416<br/>Authors: Dawei Zhu, Rui Meng, Yale Song<br/>ðŸ”— <a href=\"https://github.com/dwzhu-pku/PaperBanana\">GitHub</a><br/>ðŸ”— <a href=\"https://dwzhu-pku.github.io/PaperBanana/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2601.23265\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2601.23265",
    "external_url": "https://github.com/dwzhu-pku/PaperBanana",
    "published_date": "2026-01-30T13:33:37.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2601.23265",
      "upvotes": 140,
      "github_stars": 2416,
      "github_url": "https://github.com/dwzhu-pku/PaperBanana",
      "project_url": "https://dwzhu-pku.github.io/PaperBanana/",
      "authors": [
        "Dawei Zhu",
        "Rui Meng",
        "Yale Song",
        "Xiyu Wei",
        "Sujian Li",
        "Tomas Pfister",
        "Jinsung Yoon"
      ],
      "summary": "Despite rapid advances in autonomous AI scientists powered by language models, generating publication-ready illustrations remains a labor-intensive bottleneck in the research workflow. To lift this burden, we introduce PaperBanana, an agentic framework for automated generation of publication-ready academic illustrations. Powered by state-of-the-art VLMs and image generation models, PaperBanana orchestrates specialized agents to retrieve references, plan content and style, render images, and iteratively refine via self-critique. To rigorously evaluate our framework, we introduce PaperBananaBench, comprising 292 test cases for methodology diagrams curated from NeurIPS 2025 publications, covering diverse research domains and illustration styles. Comprehensive experiments demonstrate that PaperBanana consistently outperforms leading baselines in faithfulness, conciseness, readability, and aesthetics. We further show that our method effectively extends to the generation of high-quality statistical plots. Collectively, PaperBanana paves the way for the automated generation of publication-ready illustrations.",
      "fetch_date": "2026-02-02",
      "num_comments": 12,
      "ai_summary": "_paperbanana is an agentic framework that automates the creation of publication-ready academic illustrations using advanced vision-language models and image generation techniques.",
      "ai_keywords": [
        "VLMs",
        "image generation models",
        "agentic framework",
        "publication-ready illustrations",
        "methodology diagrams",
        "PaperBananaBench",
        "self-critique",
        "statistical plots"
      ]
    }
  },
  {
    "id": "e77f7ec067148dac222fa8934786f0dd",
    "source": "huggingface",
    "type": "paper",
    "title": "WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning",
    "description": "Recent advancements in Large Language Models (LLMs) have largely focused on depth scaling, where a single agent solves long-horizon problems with multi-turn reasoning and tool use. However, as tasks g...<br/>Upvotes: 85<br/>GitHub Stars: 2407<br/>Authors: Zelai Xu, Zhexuan Xu, Ruize Zhang<br/>ðŸ”— <a href=\"https://github.com/RLinf/RLinf/tree/main/examples/wideseek_r1\">GitHub</a><br/>ðŸ”— <a href=\"https://wideseek-r1.github.io/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2602.04634\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2602.04634",
    "external_url": "https://github.com/RLinf/RLinf/tree/main/examples/wideseek_r1",
    "published_date": "2026-02-04T10:05:12.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2602.04634",
      "upvotes": 85,
      "github_stars": 2407,
      "github_url": "https://github.com/RLinf/RLinf/tree/main/examples/wideseek_r1",
      "project_url": "https://wideseek-r1.github.io/",
      "authors": [
        "Zelai Xu",
        "Zhexuan Xu",
        "Ruize Zhang",
        "Chunyang Zhu",
        "Shi Yu",
        "Weilin Liu",
        "Quanlu Zhang",
        "Wenbo Ding",
        "Chao Yu",
        "Yu Wang"
      ],
      "summary": "Recent advancements in Large Language Models (LLMs) have largely focused on depth scaling, where a single agent solves long-horizon problems with multi-turn reasoning and tool use. However, as tasks grow broader, the key bottleneck shifts from individual competence to organizational capability. In this work, we explore a complementary dimension of width scaling with multi-agent systems to address broad information seeking. Existing multi-agent systems often rely on hand-crafted workflows and turn-taking interactions that fail to parallelize work effectively. To bridge this gap, we propose WideSeek-R1, a lead-agent-subagent framework trained via multi-agent reinforcement learning (MARL) to synergize scalable orchestration and parallel execution. By utilizing a shared LLM with isolated contexts and specialized tools, WideSeek-R1 jointly optimizes the lead agent and parallel subagents on a curated dataset of 20k broad information-seeking tasks. Extensive experiments show that WideSeek-R1-4B achieves an item F1 score of 40.0% on the WideSearch benchmark, which is comparable to the performance of single-agent DeepSeek-R1-671B. Furthermore, WideSeek-R1-4B exhibits consistent performance gains as the number of parallel subagents increases, highlighting the effectiveness of width scaling.",
      "fetch_date": "2026-02-05",
      "num_comments": 4,
      "ai_summary": "Multi-agent systems using reinforcement learning enable parallel information seeking with scalable orchestration, achieving performance comparable to larger single agents.",
      "ai_keywords": [
        "Large Language Models",
        "multi-agent systems",
        "multi-agent reinforcement learning",
        "lead-agent-subagent framework",
        "parallel execution",
        "information seeking",
        "WideSearch benchmark",
        "F1 score"
      ]
    }
  }
]