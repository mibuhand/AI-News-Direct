[
  {
    "id": "c360b805a86862ca56d4316aea6a684d",
    "source": "huggingface",
    "type": "paper",
    "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models",
    "description": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) De...<br/>Upvotes: 243<br/>Authors: DeepSeek-AI, Aixin Liu, Aoxue Mei<br/>ðŸ”— <a href=\"https://huggingface.co/papers/2512.02556\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2512.02556",
    "external_url": "",
    "published_date": "2025-12-02T04:25:14.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2512.02556",
      "upvotes": 243,
      "github_stars": 0,
      "github_url": "",
      "project_url": "",
      "authors": [
        "DeepSeek-AI",
        "Aixin Liu",
        "Aoxue Mei",
        "Bangcai Lin",
        "Bing Xue",
        "Bingxuan Wang",
        "Bingzheng Xu",
        "Bochao Wu",
        "Bowei Zhang",
        "Chaofan Lin",
        "Chen Dong",
        "Chengda Lu",
        "Chenggang Zhao",
        "Chengqi Deng",
        "Chenhao Xu",
        "Chong Ruan",
        "Damai Dai",
        "Daya Guo",
        "Dejian Yang",
        "Deli Chen",
        "Erhang Li",
        "Fangqi Zhou",
        "Fangyun Lin",
        "Fucong Dai",
        "Guangbo Hao",
        "Guanting Chen",
        "Guowei Li",
        "H. Zhang",
        "Hanwei Xu",
        "Hao Li",
        "Haofen Liang",
        "Haoran Wei",
        "Haowei Zhang",
        "Haowen Luo",
        "Haozhe Ji",
        "Honghui Ding",
        "Hongxuan Tang",
        "Huanqi Cao",
        "Huazuo Gao",
        "Hui Qu",
        "Hui Zeng",
        "Jialiang Huang",
        "Jiashi Li",
        "Jiaxin Xu",
        "Jiewen Hu",
        "Jingchang Chen",
        "Jingting Xiang",
        "Jingyang Yuan",
        "Jingyuan Cheng",
        "Jinhua Zhu",
        "Jun Ran",
        "Junguang Jiang",
        "Junjie Qiu",
        "Junlong Li",
        "Junxiao Song",
        "Kai Dong",
        "Kaige Gao",
        "Kang Guan",
        "Kexin Huang",
        "Kexing Zhou",
        "Kezhao Huang",
        "Kuai Yu",
        "Lean Wang",
        "Lecong Zhang",
        "Lei Wang",
        "Liang Zhao",
        "Liangsheng Yin",
        "Lihua Guo",
        "Lingxiao Luo",
        "Linwang Ma",
        "Litong Wang",
        "Liyue Zhang",
        "M. S. Di",
        "M. Y Xu",
        "Mingchuan Zhang",
        "Minghua Zhang",
        "Minghui Tang",
        "Mingxu Zhou",
        "Panpan Huang",
        "Peixin Cong",
        "Peiyi Wang",
        "Qiancheng Wang",
        "Qihao Zhu",
        "Qingyang Li",
        "Qinyu Chen",
        "Qiushi Du",
        "Ruiling Xu",
        "Ruiqi Ge",
        "Ruisong Zhang",
        "Ruizhe Pan",
        "Runji Wang",
        "Runqiu Yin",
        "Runxin Xu",
        "Ruomeng Shen",
        "Ruoyu Zhang",
        "S. H. Liu",
        "Shanghao Lu",
        "Shangyan Zhou",
        "Shanhuang Chen",
        "Shaofei Cai",
        "Shaoyuan Chen",
        "Shengding Hu",
        "Shengyu Liu",
        "Shiqiang Hu",
        "Shirong Ma",
        "Shiyu Wang",
        "Shuiping Yu",
        "Shunfeng Zhou",
        "Shuting Pan",
        "Songyang Zhou",
        "Tao Ni",
        "Tao Yun",
        "Tian Pei",
        "Tian Ye",
        "Tianyuan Yue",
        "Wangding Zeng",
        "Wen Liu",
        "Wenfeng Liang",
        "Wenjie Pang",
        "Wenjing Luo",
        "Wenjun Gao",
        "Wentao Zhang",
        "Xi Gao",
        "Xiangwen Wang",
        "Xiao Bi",
        "Xiaodong Liu",
        "Xiaohan Wang",
        "Xiaokang Chen",
        "Xiaokang Zhang",
        "Xiaotao Nie",
        "Xin Cheng",
        "Xin Liu",
        "Xin Xie",
        "Xingchao Liu",
        "Xingkai Yu",
        "Xingyou Li",
        "Xinyu Yang",
        "Xinyuan Li",
        "Xu Chen",
        "Xuecheng Su",
        "Xuehai Pan",
        "Xuheng Lin",
        "Xuwei Fu",
        "Y. Q. Wang",
        "Yang Zhang",
        "Yanhong Xu",
        "Yanru Ma",
        "Yao Li",
        "Yao Li",
        "Yao Zhao",
        "Yaofeng Sun",
        "Yaohui Wang",
        "Yi Qian",
        "Yi Yu",
        "Yichao Zhang",
        "Yifan Ding",
        "Yifan Shi",
        "Yiliang Xiong",
        "Ying He",
        "Ying Zhou",
        "Yinmin Zhong",
        "Yishi Piao",
        "Yisong Wang",
        "Yixiao Chen",
        "Yixuan Tan",
        "Yixuan Wei",
        "Yiyang Ma",
        "Yiyuan Liu",
        "Yonglun Yang",
        "Yongqiang Guo",
        "Yongtong Wu",
        "Yu Wu",
        "Yuan Cheng",
        "Yuan Ou",
        "Yuanfan Xu",
        "Yuduan Wang",
        "Yue Gong",
        "Yuhan Wu",
        "Yuheng Zou",
        "Yukun Li",
        "Yunfan Xiong",
        "Yuxiang Luo",
        "Yuxiang You",
        "Yuxuan Liu",
        "Yuyang Zhou",
        "Z. F. Wu",
        "Z. Z. Ren",
        "Zehua Zhao",
        "Zehui Ren",
        "Zhangli Sha",
        "Zhe Fu",
        "Zhean Xu",
        "Zhenda Xie",
        "Zhengyan Zhang",
        "Zhewen Hao",
        "Zhibin Gou",
        "Zhicheng Ma",
        "Zhigang Yan",
        "Zhihong Shao",
        "Zhixian Huang",
        "Zhiyu Wu",
        "Zhuoshu Li",
        "Zhuping Zhang",
        "Zian Xu",
        "Zihao Wang",
        "Zihui Gu",
        "Zijia Zhu",
        "Zilin Li",
        "Zipeng Zhang",
        "Ziwei Xie",
        "Ziyi Gao",
        "Zizheng Pan",
        "Zongqing Yao",
        "Bei Feng",
        "Hui Li",
        "J. L. Cai",
        "Jiaqi Ni",
        "Lei Xu",
        "Meng Li",
        "Ning Tian",
        "R. J. Chen",
        "R. L. Jin",
        "S. S. Li",
        "Shuang Zhou",
        "Tianyu Sun",
        "X. Q. Li",
        "Xiangyue Jin",
        "Xiaojin Shen",
        "Xiaosha Chen",
        "Xinnan Song",
        "Xinyi Zhou",
        "Y. X. Zhu",
        "Yanping Huang",
        "Yaohui Li",
        "Yi Zheng",
        "Yuchen Zhu",
        "Yunxian Ma",
        "Zhen Huang",
        "Zhipeng Xu",
        "Zhongyu Zhang",
        "Dongjie Ji",
        "Jian Liang",
        "Jianzhong Guo",
        "Jin Chen",
        "Leyi Xia",
        "Miaojun Wang",
        "Mingming Li",
        "Peng Zhang",
        "Ruyi Chen",
        "Shangmian Sun",
        "Shaoqing Wu",
        "Shengfeng Ye",
        "T. Wang",
        "W. L. Xiao",
        "Wei An",
        "Xianzu Wang",
        "Xiaowen Sun",
        "Xiaoxiang Wang",
        "Ying Tang",
        "Yukun Zha",
        "Zekai Zhang",
        "Zhe Ju",
        "Zhen Zhang",
        "Zihua Qu"
      ],
      "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.",
      "fetch_date": "2025-12-03",
      "num_comments": 6,
      "ai_summary": "DeepSeek-V3.2 introduces DeepSeek Sparse Attention and a scalable reinforcement learning framework, achieving superior reasoning and performance compared to GPT-5 and Gemini-3.0-Pro in complex reasoning tasks.",
      "ai_keywords": [
        "DeepSeek Sparse Attention",
        "DSA",
        "reinforcement learning framework",
        "agentic task synthesis pipeline",
        "computational efficiency",
        "long-context scenarios",
        "gold-medal performance",
        "International Mathematical Olympiad",
        "International Olympiad in Informatics",
        "reasoning proficiency"
      ]
    }
  },
  {
    "id": "adcafb2025f84cc88e13db2e967d246f",
    "source": "huggingface",
    "type": "paper",
    "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI",
    "description": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current pra...<br/>Upvotes: 195<br/>GitHub Stars: 1988<br/>Authors: Hao Liang, Xiaochen Ma, Zhou Liu<br/>ðŸ”— <a href=\"https://github.com/OpenDCAI/DataFlow\">GitHub</a><br/>ðŸ”— <a href=\"https://github.com/OpenDCAI/DataFlow\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2512.16676\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2512.16676",
    "external_url": "https://github.com/OpenDCAI/DataFlow",
    "published_date": "2025-12-18T10:46:15.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2512.16676",
      "upvotes": 195,
      "github_stars": 1988,
      "github_url": "https://github.com/OpenDCAI/DataFlow",
      "project_url": "https://github.com/OpenDCAI/DataFlow",
      "authors": [
        "Hao Liang",
        "Xiaochen Ma",
        "Zhou Liu",
        "Zhen Hao Wong",
        "Zhengyang Zhao",
        "Zimo Meng",
        "Runming He",
        "Chengyu Shen",
        "Qifeng Cai",
        "Zhaoyang Han",
        "Meiyi Qiang",
        "Yalin Feng",
        "Tianyi Bai",
        "Zewei Pan",
        "Ziyi Guo",
        "Yizhen Jiang",
        "Jingwen Deng",
        "Qijie You",
        "Peichao Lai",
        "Tianyu Guo",
        "Chi Hsu Tsai",
        "Hengyi Feng",
        "Rui Hu",
        "Wenkai Yu",
        "Junbo Niu",
        "Bohan Zeng",
        "Ruichuan An",
        "Lu Ma",
        "Jihao Huang",
        "Yaowei Zheng",
        "Conghui He",
        "Linpeng Tang",
        "Bin Cui",
        "Weinan E",
        "Wentao Zhang"
      ],
      "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.",
      "fetch_date": "2025-12-23",
      "num_comments": 4,
      "ai_summary": "DataFlow is an LLM-driven data preparation framework that enhances data quality and reproducibility for various tasks, improving LLM performance with automatically generated pipelines.",
      "ai_keywords": [
        "DataFlow",
        "Large Language Models (LLMs)",
        "data preparation pipelines",
        "system-level abstractions",
        "PyTorch-style pipeline construction API",
        "reusable operators",
        "domain-general pipelines",
        "Text-to-SQL",
        "agentic RAG",
        "large-scale knowledge extraction",
        "DataFlow-Agent",
        "operator synthesis",
        "pipeline planning",
        "iterative verification"
      ]
    }
  },
  {
    "id": "bdd6e066d5b32b9c61996386576f3682",
    "source": "huggingface",
    "type": "paper",
    "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length",
    "description": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audi...<br/>Upvotes: 167<br/>GitHub Stars: 1279<br/>Authors: Yubo Huang, Hailong Guo, Fangtai Wu<br/>ðŸ”— <a href=\"https://github.com/Alibaba-Quark/LiveAvatar\">GitHub</a><br/>ðŸ”— <a href=\"https://liveavatar.github.io/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2512.04677\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2512.04677",
    "external_url": "https://github.com/Alibaba-Quark/LiveAvatar",
    "published_date": "2025-12-04T06:11:24.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2512.04677",
      "upvotes": 167,
      "github_stars": 1279,
      "github_url": "https://github.com/Alibaba-Quark/LiveAvatar",
      "project_url": "https://liveavatar.github.io/",
      "authors": [
        "Yubo Huang",
        "Hailong Guo",
        "Fangtai Wu",
        "Shifeng Zhang",
        "Shijie Huang",
        "Qijun Gan",
        "Lin Liu",
        "Sirui Zhao",
        "Enhong Chen",
        "Jiaming Liu",
        "Steven Hoi"
      ],
      "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.",
      "fetch_date": "2025-12-05",
      "num_comments": 6,
      "ai_summary": "Live Avatar uses a 14-billion-parameter diffusion model with Timestep-forcing Pipeline Parallelism and Rolling Sink Frame Mechanism to achieve real-time, high-fidelity avatar generation.",
      "ai_keywords": [
        "diffusion-based video generation",
        "sequential computation",
        "long-horizon inconsistency",
        "real-time",
        "streaming audio-driven avatar synthesis",
        "Timestep-forcing Pipeline Parallelism",
        "distributed inference paradigm",
        "pipeline parallelism",
        "denoising steps",
        "autoregressive bottleneck",
        "low-latency real-time streaming",
        "Rolling Sink Frame Mechanism",
        "sequence fidelity",
        "Self-Forcing Distribution Matching Distillation",
        "causal",
        "streamable adaptation",
        "visual quality",
        "end-to-end generation",
        "H800 GPUs"
      ]
    }
  },
  {
    "id": "8328813bd23f18dae6c4e2c63c9de3c9",
    "source": "huggingface",
    "type": "paper",
    "title": "Kling-Omni Technical Report",
    "description": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bri...<br/>Upvotes: 163<br/>Authors: Kling Team, Jialu Chen, Yuanzheng Ci<br/>ðŸ”— <a href=\"https://huggingface.co/papers/2512.16776\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2512.16776",
    "external_url": "",
    "published_date": "2025-12-18T12:08:12.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2512.16776",
      "upvotes": 163,
      "github_stars": 0,
      "github_url": "",
      "project_url": "",
      "authors": [
        "Kling Team",
        "Jialu Chen",
        "Yuanzheng Ci",
        "Xiangyu Du",
        "Zipeng Feng",
        "Kun Gai",
        "Sainan Guo",
        "Feng Han",
        "Jingbin He",
        "Kang He",
        "Xiao Hu",
        "Xiaohua Hu",
        "Boyuan Jiang",
        "Fangyuan Kong",
        "Hang Li",
        "Jie Li",
        "Qingyu Li",
        "Shen Li",
        "Xiaohan Li",
        "Yan Li",
        "Jiajun Liang",
        "Borui Liao",
        "Yiqiao Liao",
        "Weihong Lin",
        "Quande Liu",
        "Xiaokun Liu",
        "Yilun Liu",
        "Yuliang Liu",
        "Shun Lu",
        "Hangyu Mao",
        "Yunyao Mao",
        "Haodong Ouyang",
        "Wenyu Qin",
        "Wanqi Shi",
        "Xiaoyu Shi",
        "Lianghao Su",
        "Haozhi Sun",
        "Peiqin Sun",
        "Pengfei Wan",
        "Chao Wang",
        "Chenyu Wang",
        "Meng Wang",
        "Qiulin Wang",
        "Runqi Wang",
        "Xintao Wang",
        "Xuebo Wang",
        "Zekun Wang",
        "Min Wei",
        "Tiancheng Wen",
        "Guohao Wu",
        "Xiaoshi Wu",
        "Zhenhua Wu",
        "Da Xie",
        "Yingtong Xiong",
        "Yulong Xu",
        "Sile Yang",
        "Zikang Yang",
        "Weicai Ye",
        "Ziyang Yuan",
        "Shenglong Zhang",
        "Shuaiyu Zhang",
        "Yuanxing Zhang",
        "Yufan Zhang",
        "Wenzheng Zhao",
        "Ruiliang Zhou",
        "Yan Zhou",
        "Guosheng Zhu",
        "Yongjie Zhu"
      ],
      "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.",
      "fetch_date": "2025-12-19",
      "num_comments": 6,
      "ai_summary": "Kling-Omni is a versatile generative framework that synthesizes high-quality videos from multimodal inputs, integrating generation, editing, and reasoning into a unified system.",
      "ai_keywords": [
        "generative framework",
        "multimodal visual language inputs",
        "end-to-end",
        "video generation",
        "editing",
        "intelligent reasoning",
        "unified multimodal representation",
        "cinematic-quality",
        "efficient large-scale pre-training",
        "inference optimizations",
        "in-context generation",
        "reasoning-based editing",
        "multimodal instruction following",
        "multimodal world simulators"
      ]
    }
  },
  {
    "id": "d7ac04c5b6b75ab304cbc5a5329966c8",
    "source": "huggingface",
    "type": "paper",
    "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle",
    "description": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insi...<br/>Upvotes: 149<br/>GitHub Stars: 301<br/>Authors: Fangyu Lei, Jinxiang Meng, Yiming Huang<br/>ðŸ”— <a href=\"https://github.com/ByteDance-Seed/DAComp\">GitHub</a><br/>ðŸ”— <a href=\"https://da-comp.github.io/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2512.04324\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2512.04324",
    "external_url": "https://github.com/ByteDance-Seed/DAComp",
    "published_date": "2025-12-03T18:21:28.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2512.04324",
      "upvotes": 149,
      "github_stars": 301,
      "github_url": "https://github.com/ByteDance-Seed/DAComp",
      "project_url": "https://da-comp.github.io/",
      "authors": [
        "Fangyu Lei",
        "Jinxiang Meng",
        "Yiming Huang",
        "Junjie Zhao",
        "Yitong Zhang",
        "Jianwen Luo",
        "Xin Zou",
        "Ruiyi Yang",
        "Wenbo Shi",
        "Yan Gao",
        "Shizhu He",
        "Zuo Wang",
        "Qian Liu",
        "Yang Wang",
        "Ke Wang",
        "Jun Zhao",
        "Kang Liu"
      ],
      "summary": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io",
      "fetch_date": "2025-12-05",
      "num_comments": 6,
      "ai_summary": "DAComp is a benchmark of 210 tasks that evaluates the capabilities of agents in real-world data engineering and data analysis workflows, revealing significant deficiencies in both areas.",
      "ai_keywords": [
        "data engineering",
        "data analysis",
        "DE tasks",
        "DA tasks",
        "SQL pipelines",
        "multi-metric evaluation",
        "LLM-judge",
        "hierarchical rubrics",
        "autonomous data agents"
      ]
    }
  },
  {
    "id": "5b6ff13ede65701e5f764a0267eca8df",
    "source": "huggingface",
    "type": "paper",
    "title": "Qwen3-VL Technical Report",
    "description": "We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks. It natively supports interleaved...<br/>Upvotes: 147<br/>GitHub Stars: 17512<br/>Authors: Shuai Bai, Yuxuan Cai, Ruizhe Chen<br/>ðŸ”— <a href=\"https://github.com/QwenLM/Qwen3-VL\">GitHub</a><br/>ðŸ”— <a href=\"https://chat.qwen.ai\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2511.21631\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2511.21631",
    "external_url": "https://github.com/QwenLM/Qwen3-VL",
    "published_date": "2025-11-26T12:59:08.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2511.21631",
      "upvotes": 147,
      "github_stars": 17512,
      "github_url": "https://github.com/QwenLM/Qwen3-VL",
      "project_url": "https://chat.qwen.ai",
      "authors": [
        "Shuai Bai",
        "Yuxuan Cai",
        "Ruizhe Chen",
        "Keqin Chen",
        "Xionghui Chen",
        "Zesen Cheng",
        "Lianghao Deng",
        "Wei Ding",
        "Chang Gao",
        "Chunjiang Ge",
        "Wenbin Ge",
        "Zhifang Guo",
        "Qidong Huang",
        "Jie Huang",
        "Fei Huang",
        "Binyuan Hui",
        "Shutong Jiang",
        "Zhaohai Li",
        "Mingsheng Li",
        "Mei Li",
        "Kaixin Li",
        "Zicheng Lin",
        "Junyang Lin",
        "Xuejing Liu",
        "Jiawei Liu",
        "Chenglong Liu",
        "Yang Liu",
        "Dayiheng Liu",
        "Shixuan Liu",
        "Dunjie Lu",
        "Ruilin Luo",
        "Chenxu Lv",
        "Rui Men",
        "Lingchen Meng",
        "Xuancheng Ren",
        "Xingzhang Ren",
        "Sibo Song",
        "Yuchong Sun",
        "Jun Tang",
        "Jianhong Tu",
        "Jianqiang Wan",
        "Peng Wang",
        "Pengfei Wang",
        "Qiuyue Wang",
        "Yuxuan Wang",
        "Tianbao Xie",
        "Yiheng Xu",
        "Haiyang Xu",
        "Jin Xu",
        "Zhibo Yang",
        "Mingkun Yang",
        "Jianxin Yang",
        "An Yang",
        "Bowen Yu",
        "Fei Zhang",
        "Hang Zhang",
        "Xi Zhang",
        "Bo Zheng",
        "Humen Zhong",
        "Jingren Zhou",
        "Fan Zhou",
        "Jing Zhou",
        "Yuanzhi Zhu",
        "Ke Zhu"
      ],
      "summary": "We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks. It natively supports interleaved contexts of up to 256K tokens, seamlessly integrating text, images, and video. The model family includes both dense (2B/4B/8B/32B) and mixture-of-experts (30B-A3B/235B-A22B) variants to accommodate diverse latency-quality trade-offs. Qwen3-VL delivers three core pillars: (i) markedly stronger pure-text understanding, surpassing comparable text-only backbones in several cases; (ii) robust long-context comprehension with a native 256K-token window for both text and interleaved multimodal inputs, enabling faithful retention, retrieval, and cross-referencing across long documents and videos; and (iii) advanced multimodal reasoning across single-image, multi-image, and video tasks, demonstrating leading performance on comprehensive evaluations such as MMMU and visual-math benchmarks (e.g., MathVista and MathVision). Architecturally, we introduce three key upgrades: (i) an enhanced interleaved-MRoPE for stronger spatial-temporal modeling across images and video; (ii) DeepStack integration, which effectively leverages multi-level ViT features to tighten vision-language alignment; and (iii) text-based time alignment for video, evolving from T-RoPE to explicit textual timestamp alignment for more precise temporal grounding. Under comparable token budgets and latency constraints, Qwen3-VL achieves superior performance in both dense and Mixture-of-Experts (MoE) architectures. We envision Qwen3-VL serving as a foundational engine for image-grounded reasoning, agentic decision-making, and multimodal code intelligence in real-world workflows.",
      "fetch_date": "2025-12-04",
      "num_comments": 4,
      "ai_summary": "Qwen3-VL, a vision-language model, excels in text and multimodal understanding through advanced architectures and larger contexts, achieving superior performance across benchmarks.",
      "ai_keywords": [
        "vision-language model",
        "interleaved contexts",
        "multimodal benchmarks",
        "dense variants",
        "mixture-of-experts",
        "pure-text understanding",
        "long-context comprehension",
        "multimodal reasoning",
        "MMMU",
        "visual-math benchmarks",
        "interleaved-MRoPE",
        "DeepStack",
        "text-based time alignment",
        "T-RoPE",
        "explicit textual timestamp alignment",
        "vision-language alignment",
        "image-grounded reasoning",
        "agentic decision-making",
        "multimodal code intelligence"
      ]
    }
  },
  {
    "id": "84e0c9ab3b32b3eab139bdd727fe4f8c",
    "source": "huggingface",
    "type": "paper",
    "title": "DeepCode: Open Agentic Coding",
    "description": "Recent advances in large language models (LLMs) have given rise to powerful coding agents, making it possible for code assistants to evolve into code engineers. However, existing methods still face si...<br/>Upvotes: 31<br/>GitHub Stars: 13233<br/>Authors: Zongwei Li, Zhonghang Li, Zirui Guo<br/>ðŸ”— <a href=\"https://github.com/HKUDS/DeepCode\">GitHub</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2512.07921\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2512.07921",
    "external_url": "https://github.com/HKUDS/DeepCode",
    "published_date": "2025-12-08T11:07:13.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2512.07921",
      "upvotes": 31,
      "github_stars": 13233,
      "github_url": "https://github.com/HKUDS/DeepCode",
      "project_url": "",
      "authors": [
        "Zongwei Li",
        "Zhonghang Li",
        "Zirui Guo",
        "Xubin Ren",
        "Chao Huang"
      ],
      "summary": "Recent advances in large language models (LLMs) have given rise to powerful coding agents, making it possible for code assistants to evolve into code engineers. However, existing methods still face significant challenges in achieving high-fidelity document-to-codebase synthesis--such as scientific papers to code--primarily due to a fundamental conflict between information overload and the context bottlenecks of LLMs. In this work, we introduce DeepCode, a fully autonomous framework that fundamentally addresses this challenge through principled information-flow management. By treating repository synthesis as a channel optimization problem, DeepCode seamlessly orchestrates four information operations to maximize task-relevant signals under finite context budgets: source compression via blueprint distillation, structured indexing using stateful code memory, conditional knowledge injection via retrieval-augmented generation, and closed-loop error correction. Extensive evaluations on the PaperBench benchmark demonstrate that DeepCode achieves state-of-the-art performance, decisively outperforming leading commercial agents such as Cursor and Claude Code, and crucially, surpassing PhD-level human experts from top institutes on key reproduction metrics. By systematically transforming paper specifications into production-grade implementations comparable to human expert quality, this work establishes new foundations for autonomous scientific reproduction that can accelerate research evaluation and discovery.",
      "fetch_date": "2025-12-10",
      "num_comments": 2,
      "ai_summary": "DeepCode, a fully autonomous framework, addresses the challenges of document-to-codebase synthesis by optimizing information flow through source compression, structured indexing, knowledge injection, and error correction, achieving state-of-the-art performance and surpassing human experts.",
      "ai_keywords": [
        "large language models",
        "coding agents",
        "document-to-codebase synthesis",
        "information overload",
        "context bottlenecks",
        "DeepCode",
        "channel optimization",
        "blueprint distillation",
        "stateful code memory",
        "retrieval-augmented generation",
        "closed-loop error correction",
        "PaperBench",
        "autonomous scientific reproduction"
      ]
    }
  },
  {
    "id": "b0feeb7f591514204e557e85f0c2b6ca",
    "source": "huggingface",
    "type": "paper",
    "title": "Sharp Monocular View Synthesis in Less Than a Second",
    "description": "We present SHARP, an approach to photorealistic view synthesis from a single image. Given a single photograph, SHARP regresses the parameters of a 3D Gaussian representation of the depicted scene. Thi...<br/>Upvotes: 23<br/>GitHub Stars: 6301<br/>Authors: Lars Mescheder, Wei Dong, Shiwei Li<br/>ðŸ”— <a href=\"https://github.com/apple/ml-sharp\">GitHub</a><br/>ðŸ”— <a href=\"https://apple.github.io/ml-sharp/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2512.10685\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2512.10685",
    "external_url": "https://github.com/apple/ml-sharp",
    "published_date": "2025-12-11T09:34:11.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2512.10685",
      "upvotes": 23,
      "github_stars": 6301,
      "github_url": "https://github.com/apple/ml-sharp",
      "project_url": "https://apple.github.io/ml-sharp/",
      "authors": [
        "Lars Mescheder",
        "Wei Dong",
        "Shiwei Li",
        "Xuyang Bai",
        "Marcel Santos",
        "Peiyun Hu",
        "Bruno Lecouat",
        "Mingmin Zhen",
        "AmaÃ«l Delaunoy",
        "Tian Fang",
        "Yanghai Tsin",
        "Stephan R. Richter",
        "Vladlen Koltun"
      ],
      "summary": "We present SHARP, an approach to photorealistic view synthesis from a single image. Given a single photograph, SHARP regresses the parameters of a 3D Gaussian representation of the depicted scene. This is done in less than a second on a standard GPU via a single feedforward pass through a neural network. The 3D Gaussian representation produced by SHARP can then be rendered in real time, yielding high-resolution photorealistic images for nearby views. The representation is metric, with absolute scale, supporting metric camera movements. Experimental results demonstrate that SHARP delivers robust zero-shot generalization across datasets. It sets a new state of the art on multiple datasets, reducing LPIPS by 25-34% and DISTS by 21-43% versus the best prior model, while lowering the synthesis time by three orders of magnitude. Code and weights are provided at https://github.com/apple/ml-sharp",
      "fetch_date": "2025-12-15",
      "num_comments": 2,
      "ai_summary": "SHARP synthesizes photorealistic views from a single image using a 3D Gaussian representation, achieving state-of-the-art results with rapid processing.",
      "ai_keywords": [
        "photorealistic view synthesis",
        "3D Gaussian representation",
        "neural network",
        "feedforward pass",
        "real-time rendering",
        "metric representation",
        "zero-shot generalization",
        "LPIPS",
        "DISTS"
      ]
    }
  },
  {
    "id": "8fb3e0f897edda830f8a5713a671e918",
    "source": "huggingface",
    "type": "paper",
    "title": "Skywork-R1V4: Toward Agentic Multimodal Intelligence through Interleaved Thinking with Images and DeepResearch",
    "description": "Despite recent progress in multimodal agentic systems, existing approaches often treat image manipulation and web search as disjoint capabilities, rely heavily on costly reinforcement learning, and la...<br/>Upvotes: 47<br/>GitHub Stars: 3135<br/>Authors: Yifan Zhang, Liang Hu, Haofeng Sun<br/>ðŸ”— <a href=\"https://github.com/SkyworkAI/Skywork-R1V/tree/main/r1v4\">GitHub</a><br/>ðŸ”— <a href=\"https://docs.skyworkmodel.ai/r1v4/api-reference/completions.html\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2512.02395\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2512.02395",
    "external_url": "https://github.com/SkyworkAI/Skywork-R1V/tree/main/r1v4",
    "published_date": "2025-12-01T23:12:57.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2512.02395",
      "upvotes": 47,
      "github_stars": 3135,
      "github_url": "https://github.com/SkyworkAI/Skywork-R1V/tree/main/r1v4",
      "project_url": "https://docs.skyworkmodel.ai/r1v4/api-reference/completions.html",
      "authors": [
        "Yifan Zhang",
        "Liang Hu",
        "Haofeng Sun",
        "Peiyu Wang",
        "Yichen Wei",
        "Shukang Yin",
        "Jiangbo Pei",
        "Wei Shen",
        "Peng Xia",
        "Yi Peng",
        "Tianyidan Xie",
        "Eric Li",
        "Yang Liu",
        "Xuchen Song",
        "Yahui Zhou"
      ],
      "summary": "Despite recent progress in multimodal agentic systems, existing approaches often treat image manipulation and web search as disjoint capabilities, rely heavily on costly reinforcement learning, and lack planning grounded in real tool-execution traces. To address these limitations, we present Skywork-R1V4, a 30B (A3B) parameter multimodal agentic model that unifies multimodal planning, active image manipulation (\"thinking with images\"), deep multimodal search, and, most critically, interleaved reasoning that dynamically alternates between visual operations and external knowledge retrieval. Trained solely via supervised fine-tuning on fewer than 30,000 high-quality, planning-execution-consistent trajectories and validated through stepwise consistency filtering, Skywork-R1V4 achieves state-of-the-art results across perception and multimodal search benchmarks: it scores 66.1 on MMSearch and 67.2 on FVQA, surpassing Gemini 2.5 Flash on all 11 metrics. Skywork-R1V4 exhibits emergent long-horizon reasoning at inference time, successfully orchestrating more than 10 tool calls to solve complex, multi-step tasks. Our results demonstrate that sophisticated agentic multimodal intelligence can be achieved through carefully curated supervised learning alone, without any reliance on reinforcement learning.",
      "fetch_date": "2025-12-03",
      "num_comments": 2,
      "ai_summary": "Skywork-R1V4, a 30B parameter multimodal agentic model, achieves top performance in perception and multimodal search benchmarks through supervised fine-tuning and interleaved reasoning without reinforcement learning.",
      "ai_keywords": [
        "multimodal agentic systems",
        "image manipulation",
        "web search",
        "reinforcement learning",
        "multimodal planning",
        "active image manipulation",
        "deep multimodal search",
        "interleaved reasoning",
        "supervised fine-tuning",
        "MMSearch",
        "FVQA",
        "Gemini 2.5 Flash",
        "long-horizon reasoning"
      ]
    }
  },
  {
    "id": "5bb778f47e493a1d7bcf12aaaec0e25e",
    "source": "huggingface",
    "type": "paper",
    "title": "TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times",
    "description": "We introduce TurboDiffusion, a video generation acceleration framework that can speed up end-to-end diffusion generation by 100-200x while maintaining video quality. TurboDiffusion mainly relies on se...<br/>Upvotes: 90<br/>GitHub Stars: 2948<br/>Authors: Jintao Zhang, Kaiwen Zheng, Kai Jiang<br/>ðŸ”— <a href=\"https://github.com/thu-ml/TurboDiffusion\">GitHub</a><br/>ðŸ”— <a href=\"https://github.com/thu-ml/TurboDiffusion\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2512.16093\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2512.16093",
    "external_url": "https://github.com/thu-ml/TurboDiffusion",
    "published_date": "2025-12-17T21:21:30.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2512.16093",
      "upvotes": 90,
      "github_stars": 2948,
      "github_url": "https://github.com/thu-ml/TurboDiffusion",
      "project_url": "https://github.com/thu-ml/TurboDiffusion",
      "authors": [
        "Jintao Zhang",
        "Kaiwen Zheng",
        "Kai Jiang",
        "Haoxu Wang",
        "Ion Stoica",
        "Joseph E. Gonzalez",
        "Jianfei Chen",
        "Jun Zhu"
      ],
      "summary": "We introduce TurboDiffusion, a video generation acceleration framework that can speed up end-to-end diffusion generation by 100-200x while maintaining video quality. TurboDiffusion mainly relies on several components for acceleration: (1) Attention acceleration: TurboDiffusion uses low-bit SageAttention and trainable Sparse-Linear Attention (SLA) to speed up attention computation. (2) Step distillation: TurboDiffusion adopts rCM for efficient step distillation. (3) W8A8 quantization: TurboDiffusion quantizes model parameters and activations to 8 bits to accelerate linear layers and compress the model. In addition, TurboDiffusion incorporates several other engineering optimizations.\n  We conduct experiments on the Wan2.2-I2V-14B-720P, Wan2.1-T2V-1.3B-480P, Wan2.1-T2V-14B-720P, and Wan2.1-T2V-14B-480P models. Experimental results show that TurboDiffusion achieves 100-200x speedup for video generation even on a single RTX 5090 GPU, while maintaining comparable video quality. The GitHub repository, which includes model checkpoints and easy-to-use code, is available at https://github.com/thu-ml/TurboDiffusion.",
      "fetch_date": "2025-12-25",
      "num_comments": 7,
      "ai_summary": "TurboDiffusion accelerates video generation by 100-200x using attention acceleration, step distillation, and quantization, while maintaining video quality.",
      "ai_keywords": [
        "SageAttention",
        "Sparse-Linear Attention",
        "rCM",
        "W8A8 quantization",
        "diffusion generation",
        "video generation",
        "RTX 5090 GPU"
      ]
    }
  },
  {
    "id": "1c5e2b4f68f2d0420edc475a4ac95129",
    "source": "huggingface",
    "type": "paper",
    "title": "SAM Audio: Segment Anything in Audio",
    "description": "General audio source separation is a key capability for multimodal AI systems that can perceive and reason about sound. Despite substantial progress in recent years, existing separation models are eit...<br/>Upvotes: 21<br/>GitHub Stars: 2694<br/>Authors: Bowen Shi, Andros Tjandra, John Hoffman<br/>ðŸ”— <a href=\"https://github.com/facebookresearch/sam-audio\">GitHub</a><br/>ðŸ”— <a href=\"https://ai.meta.com/samaudio/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2512.18099\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2512.18099",
    "external_url": "https://github.com/facebookresearch/sam-audio",
    "published_date": "2025-12-19T17:14:23.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2512.18099",
      "upvotes": 21,
      "github_stars": 2694,
      "github_url": "https://github.com/facebookresearch/sam-audio",
      "project_url": "https://ai.meta.com/samaudio/",
      "authors": [
        "Bowen Shi",
        "Andros Tjandra",
        "John Hoffman",
        "Helin Wang",
        "Yi-Chiao Wu",
        "Luya Gao",
        "Julius Richter",
        "Matt Le",
        "Apoorv Vyas",
        "Sanyuan Chen",
        "Christoph Feichtenhofer",
        "Piotr DollÃ¡r",
        "Wei-Ning Hsu",
        "Ann Lee"
      ],
      "summary": "General audio source separation is a key capability for multimodal AI systems that can perceive and reason about sound. Despite substantial progress in recent years, existing separation models are either domain-specific, designed for fixed categories such as speech or music, or limited in controllability, supporting only a single prompting modality such as text. In this work, we present SAM Audio, a foundation model for general audio separation that unifies text, visual, and temporal span prompting within a single framework. Built on a diffusion transformer architecture, SAM Audio is trained with flow matching on large-scale audio data spanning speech, music, and general sounds, and can flexibly separate target sources described by language, visual masks, or temporal spans. The model achieves state-of-the-art performance across a diverse suite of benchmarks, including general sound, speech, music, and musical instrument separation in both in-the-wild and professionally produced audios, substantially outperforming prior general-purpose and specialized systems. Furthermore, we introduce a new real-world separation benchmark with human-labeled multimodal prompts and a reference-free evaluation model that correlates strongly with human judgment.",
      "fetch_date": "2025-12-24",
      "num_comments": 1,
      "ai_summary": "SAM Audio, a diffusion transformer-based foundation model, achieves superior performance in general audio separation using unified text, visual, and temporal span prompts across various audio types.",
      "ai_keywords": [
        "diffusion transformer architecture",
        "flow matching",
        "audio separation",
        "general sound",
        "speech",
        "music",
        "musical instrument separation",
        "in-the-wild audio",
        "professionally produced audio",
        "real-world separation benchmark",
        "human-labeled multimodal prompts",
        "reference-free evaluation model"
      ]
    }
  }
]