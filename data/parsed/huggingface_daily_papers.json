[
  {
    "id": "8c7bb673a7c23605ce494554bc937b12",
    "source": "huggingface",
    "type": "paper",
    "title": "Scaling Latent Reasoning via Looped Language Models",
    "description": "Modern LLMs are trained to \"think\" primarily via explicit text generation,\nsuch as chain-of-thought (CoT), which defers reasoning to post-training and\nunder-leverages pre-training data. We present and...<br/>Upvotes: 216<br/>Authors: Rui-Jie Zhu, Zixuan Wang, Kai Hua<br/>ðŸ”— <a href=\"https://ouro-llm.github.io/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2510.25741\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2510.25741",
    "external_url": "",
    "published_date": "2025-10-29T13:45:42.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2510.25741",
      "upvotes": 216,
      "github_stars": 0,
      "github_url": "",
      "project_url": "https://ouro-llm.github.io/",
      "authors": [
        "Rui-Jie Zhu",
        "Zixuan Wang",
        "Kai Hua",
        "Tianyu Zhang",
        "Ziniu Li",
        "Haoran Que",
        "Boyi Wei",
        "Zixin Wen",
        "Fan Yin",
        "He Xing",
        "Lu Li",
        "Jiajun Shi",
        "Kaijing Ma",
        "Shanda Li",
        "Taylor Kergan",
        "Andrew Smith",
        "Xingwei Qu",
        "Mude Hui",
        "Bohong Wu",
        "Qiyang Min",
        "Hongzhi Huang",
        "Xun Zhou",
        "Wei Ye",
        "Jiaheng Liu",
        "Jian Yang",
        "Yunfeng Shi",
        "Chenghua Lin",
        "Enduo Zhao",
        "Tianle Cai",
        "Ge Zhang",
        "Wenhao Huang",
        "Yoshua Bengio",
        "Jason Eshraghian"
      ],
      "summary": "Modern LLMs are trained to \"think\" primarily via explicit text generation,\nsuch as chain-of-thought (CoT), which defers reasoning to post-training and\nunder-leverages pre-training data. We present and open-source Ouro, named after\nthe recursive Ouroboros, a family of pre-trained Looped Language Models\n(LoopLM) that instead build reasoning into the pre-training phase through (i)\niterative computation in latent space, (ii) an entropy-regularized objective\nfor learned depth allocation, and (iii) scaling to 7.7T tokens. Ouro 1.4B and\n2.6B models enjoy superior performance that match the results of up to 12B SOTA\nLLMs across a wide range of benchmarks. Through controlled experiments, we show\nthis advantage stems not from increased knowledge capacity, but from superior\nknowledge manipulation capabilities. We also show that LoopLM yields reasoning\ntraces more aligned with final outputs than explicit CoT. We hope our results\nshow the potential of LoopLM as a novel scaling direction in the reasoning era.\nOur model could be found in: http://ouro-llm.github.io.",
      "fetch_date": "2025-10-30",
      "num_comments": 6,
      "ai_summary": "LoopLM, a family of pre-trained Looped Language Models, enhances reasoning by integrating iterative computation and entropy regularization during pre-training, achieving superior performance with better knowledge manipulation.",
      "ai_keywords": [
        "Looped Language Models",
        "LoopLM",
        "iterative computation",
        "latent space",
        "entropy-regularized objective",
        "learned depth allocation",
        "chain-of-thought",
        "knowledge manipulation",
        "reasoning traces"
      ]
    }
  },
  {
    "id": "b81e62a9e2bd28a9fe78c760c27308b6",
    "source": "huggingface",
    "type": "paper",
    "title": "Thinking with Video: Video Generation as a Promising Multimodal\n  Reasoning Paradigm",
    "description": "\"Thinking with Text\" and \"Thinking with Images\" paradigm significantly\nimprove the reasoning ability of large language models (LLMs) and Vision\nLanguage Models (VLMs). However, these paradigms have in...<br/>Upvotes: 197<br/>GitHub Stars: 201<br/>Authors: Jingqi Tong, Yurong Mou, Hangcheng Li<br/>ðŸ”— <a href=\"https://github.com/tongjingqi/Thinking-with-Video\">GitHub</a><br/>ðŸ”— <a href=\"https://thinking-with-video.github.io/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2511.04570\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2511.04570",
    "external_url": "https://github.com/tongjingqi/Thinking-with-Video",
    "published_date": "2025-11-06T12:25:23.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2511.04570",
      "upvotes": 197,
      "github_stars": 201,
      "github_url": "https://github.com/tongjingqi/Thinking-with-Video",
      "project_url": "https://thinking-with-video.github.io/",
      "authors": [
        "Jingqi Tong",
        "Yurong Mou",
        "Hangcheng Li",
        "Mingzhe Li",
        "Yongzhuo Yang",
        "Ming Zhang",
        "Qiguang Chen",
        "Tianyi Liang",
        "Xiaomeng Hu",
        "Yining Zheng",
        "Xinchi Chen",
        "Jun Zhao",
        "Xuanjing Huang",
        "Xipeng Qiu"
      ],
      "summary": "\"Thinking with Text\" and \"Thinking with Images\" paradigm significantly\nimprove the reasoning ability of large language models (LLMs) and Vision\nLanguage Models (VLMs). However, these paradigms have inherent limitations. (1)\nImages capture only single moments and fail to represent dynamic processes or\ncontinuous changes, and (2) The separation of text and vision as distinct\nmodalities, hindering unified multimodal understanding and generation. To\novercome these limitations, we introduce \"Thinking with Video\", a new paradigm\nthat leverages video generation models, such as Sora-2, to bridge visual and\ntextual reasoning in a unified temporal framework. To support this exploration,\nwe developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBench\nencompasses two task categories: (1) vision-centric tasks (e.g., Eyeballing\nPuzzles), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Our\nevaluation establishes Sora-2 as a capable reasoner. On vision-centric tasks,\nSora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even\nsurpasses VLMs on several tasks, such as Eyeballing Games. On text-centric\ntasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU.\nFurthermore, we systematically analyse the source of these abilities. We also\nfind that self-consistency and in-context learning can improve Sora-2's\nperformance. In summary, our findings demonstrate that the video generation\nmodel is the potential unified multimodal understanding and generation model,\npositions \"thinking with video\" as a unified multimodal reasoning paradigm.",
      "fetch_date": "2025-11-07",
      "num_comments": 4,
      "ai_summary": "The \"Thinking with Video\" paradigm enhances multimodal reasoning by integrating video generation models, demonstrated through the Video Thinking Benchmark and improved performance on both vision and text tasks.",
      "ai_keywords": [
        "Thinking with Text",
        "Thinking with Images",
        "large language models",
        "Vision Language Models",
        "Thinking with Video",
        "video generation models",
        "Video Thinking Benchmark",
        "vision-centric tasks",
        "text-centric tasks",
        "Eyeballing Puzzles",
        "GSM8K",
        "MMMU",
        "self-consistency",
        "in-context learning"
      ]
    }
  },
  {
    "id": "0139ab6619114102f84bcc9c449fd176",
    "source": "huggingface",
    "type": "paper",
    "title": "Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation",
    "description": "This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kand...<br/>Upvotes: 184<br/>GitHub Stars: 395<br/>Authors: Vladimir Arkhipkin, Vladimir Korviakov, Nikolai Gerasimenko<br/>ðŸ”— <a href=\"https://github.com/kandinskylab/kandinsky-5\">GitHub</a><br/>ðŸ”— <a href=\"https://kandinskylab.ai/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2511.14993\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2511.14993",
    "external_url": "https://github.com/kandinskylab/kandinsky-5",
    "published_date": "2025-11-18T19:23:22.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2511.14993",
      "upvotes": 184,
      "github_stars": 395,
      "github_url": "https://github.com/kandinskylab/kandinsky-5",
      "project_url": "https://kandinskylab.ai/",
      "authors": [
        "Vladimir Arkhipkin",
        "Vladimir Korviakov",
        "Nikolai Gerasimenko",
        "Denis Parkhomenko",
        "Viacheslav Vasilev",
        "Alexey Letunovskiy",
        "Maria Kovaleva",
        "Nikolai Vaulin",
        "Ivan Kirillov",
        "Lev Novitskiy",
        "Denis Koposov",
        "Nikita Kiselev",
        "Alexander Varlamov",
        "Dmitrii Mikhailov",
        "Vladimir Polovnikov",
        "Andrey Shutkin",
        "Ilya Vasiliev",
        "Julia Agafonova",
        "Anastasiia Kargapoltseva",
        "Anna Dmitrienko",
        "Anastasia Maltseva",
        "Anna Averchenkova",
        "Olga Kim",
        "Tatiana Nikulina",
        "Denis Dimitrov"
      ],
      "summary": "This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kandinsky 5.0 Image Lite - a line-up of 6B parameter image generation models, Kandinsky 5.0 Video Lite - a fast and lightweight 2B parameter text-to-video and image-to-video models, and Kandinsky 5.0 Video Pro - 19B parameter models that achieves superior video generation quality. We provide a comprehensive review of the data curation lifecycle - including collection, processing, filtering and clustering - for the multi-stage training pipeline that involves extensive pre-training and incorporates quality-enhancement techniques such as self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. We also present novel architectural, training, and inference optimizations that enable Kandinsky 5.0 to achieve high generation speeds and state-of-the-art performance across various tasks, as demonstrated by human evaluation. As a large-scale, publicly available generative framework, Kandinsky 5.0 leverages the full potential of its pre-training and subsequent stages to be adapted for a wide range of generative applications. We hope that this report, together with the release of our open-source code and training checkpoints, will substantially advance the development and accessibility of high-quality generative models for the research community.",
      "fetch_date": "2025-11-20",
      "num_comments": 5,
      "ai_summary": "Kandinsky 5.0 is a family of state-of-the-art generative models for high-resolution images and short videos, featuring model lineups with varying parameters and enhanced training techniques to achieve superior quality and performance.",
      "ai_keywords": [
        "foundation models",
        "high-resolution image synthesis",
        "10-second video synthesis",
        "image generation models",
        "text-to-video models",
        "image-to-video models",
        "multi-stage training pipeline",
        "self-supervised fine-tuning",
        "reinforcement learning",
        "pre-training",
        "quality-enhancement techniques",
        "architectural optimizations",
        "training optimizations",
        "inference optimizations",
        "human evaluation",
        "generative framework",
        "open-source code",
        "training checkpoints"
      ]
    }
  },
  {
    "id": "4622a47084403f1652a1baac4335b4fa",
    "source": "huggingface",
    "type": "paper",
    "title": "Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds",
    "description": "We introduce Lumine, the first open recipe for developing generalist agents capable of completing hours-long complex missions in real time within challenging 3D open-world environments. Lumine adopts ...<br/>Upvotes: 176<br/>Authors: Weihao Tan, Xiangyang Li, Yunhao Fang<br/>ðŸ”— <a href=\"https://www.lumine-ai.org/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2511.08892\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2511.08892",
    "external_url": "",
    "published_date": "2025-11-11T21:01:26.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2511.08892",
      "upvotes": 176,
      "github_stars": 0,
      "github_url": "",
      "project_url": "https://www.lumine-ai.org/",
      "authors": [
        "Weihao Tan",
        "Xiangyang Li",
        "Yunhao Fang",
        "Heyuan Yao",
        "Shi Yan",
        "Hao Luo",
        "Tenglong Ao",
        "Huihui Li",
        "Hongbin Ren",
        "Bairen Yi",
        "Yujia Qin",
        "Bo An",
        "Libin Liu",
        "Guang Shi"
      ],
      "summary": "We introduce Lumine, the first open recipe for developing generalist agents capable of completing hours-long complex missions in real time within challenging 3D open-world environments. Lumine adopts a human-like interaction paradigm that unifies perception, reasoning, and action in an end-to-end manner, powered by a vision-language model. It processes raw pixels at 5 Hz to produce precise 30 Hz keyboard-mouse actions and adaptively invokes reasoning only when necessary. Trained in Genshin Impact, Lumine successfully completes the entire five-hour Mondstadt main storyline on par with human-level efficiency and follows natural language instructions to perform a broad spectrum of tasks in both 3D open-world exploration and 2D GUI manipulation across collection, combat, puzzle-solving, and NPC interaction. In addition to its in-domain performance, Lumine demonstrates strong zero-shot cross-game generalization. Without any fine-tuning, it accomplishes 100-minute missions in Wuthering Waves and the full five-hour first chapter of Honkai: Star Rail. These promising results highlight Lumine's effectiveness across distinct worlds and interaction dynamics, marking a concrete step toward generalist agents in open-ended environments.",
      "fetch_date": "2025-11-13",
      "num_comments": 12,
      "ai_summary": "Lumine, a vision-language model-based agent, completes complex missions in real-time across different 3D open-world environments with human-like efficiency and zero-shot cross-game generalization.",
      "ai_keywords": [
        "vision-language model",
        "end-to-end",
        "3D open-world environments",
        "human-like interaction",
        "real-time",
        "zero-shot cross-game generalization"
      ]
    }
  },
  {
    "id": "9f97533076b36bd588d78f78c4352369",
    "source": "huggingface",
    "type": "paper",
    "title": "Concerto: Joint 2D-3D Self-Supervised Learning Emerges Spatial\n  Representations",
    "description": "Humans learn abstract concepts through multisensory synergy, and once formed,\nsuch representations can often be recalled from a single modality. Inspired by\nthis principle, we introduce Concerto, a mi...<br/>Upvotes: 172<br/>GitHub Stars: 424<br/>Authors: Yujia Zhang, Xiaoyang Wu, Yixing Lao<br/>ðŸ”— <a href=\"https://github.com/Pointcept/Concerto\">GitHub</a><br/>ðŸ”— <a href=\"https://pointcept.github.io/Concerto/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2510.23607\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2510.23607",
    "external_url": "https://github.com/Pointcept/Concerto",
    "published_date": "2025-10-27T13:59:59.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2510.23607",
      "upvotes": 172,
      "github_stars": 424,
      "github_url": "https://github.com/Pointcept/Concerto",
      "project_url": "https://pointcept.github.io/Concerto/",
      "authors": [
        "Yujia Zhang",
        "Xiaoyang Wu",
        "Yixing Lao",
        "Chengyao Wang",
        "Zhuotao Tian",
        "Naiyan Wang",
        "Hengshuang Zhao"
      ],
      "summary": "Humans learn abstract concepts through multisensory synergy, and once formed,\nsuch representations can often be recalled from a single modality. Inspired by\nthis principle, we introduce Concerto, a minimalist simulation of human concept\nlearning for spatial cognition, combining 3D intra-modal self-distillation with\n2D-3D cross-modal joint embedding. Despite its simplicity, Concerto learns more\ncoherent and informative spatial features, as demonstrated by zero-shot\nvisualizations. It outperforms both standalone SOTA 2D and 3D self-supervised\nmodels by 14.2% and 4.8%, respectively, as well as their feature concatenation,\nin linear probing for 3D scene perception. With full fine-tuning, Concerto sets\nnew SOTA results across multiple scene understanding benchmarks (e.g., 80.7%\nmIoU on ScanNet). We further present a variant of Concerto tailored for\nvideo-lifted point cloud spatial understanding, and a translator that linearly\nprojects Concerto representations into CLIP's language space, enabling\nopen-world perception. These results highlight that Concerto emerges spatial\nrepresentations with superior fine-grained geometric and semantic consistency.",
      "fetch_date": "2025-10-28",
      "num_comments": 4,
      "ai_summary": "Concerto, a minimalist model combining 3D self-distillation and 2D-3D joint embedding, achieves superior spatial feature learning and outperforms existing models in scene understanding and open-world perception.",
      "ai_keywords": [
        "3D intra-modal self-distillation",
        "2D-3D cross-modal joint embedding",
        "zero-shot visualizations",
        "linear probing",
        "3D scene perception",
        "ScanNet",
        "mIoU",
        "video-lifted point cloud",
        "CLIP's language space",
        "open-world perception",
        "fine-grained geometric and semantic consistency"
      ]
    }
  },
  {
    "id": "0a60cb0f535685cc7eb12b1b01de62bc",
    "source": "huggingface",
    "type": "paper",
    "title": "MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling",
    "description": "We present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or co...<br/>Upvotes: 150<br/>GitHub Stars: 1050<br/>Authors: MiroMind Team, Song Bai, Lidong Bing<br/>ðŸ”— <a href=\"https://github.com/MiroMindAI/MiroThinker\">GitHub</a><br/>ðŸ”— <a href=\"https://dr.miromind.ai/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2511.11793\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2511.11793",
    "external_url": "https://github.com/MiroMindAI/MiroThinker",
    "published_date": "2025-11-14T13:52:07.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2511.11793",
      "upvotes": 150,
      "github_stars": 1050,
      "github_url": "https://github.com/MiroMindAI/MiroThinker",
      "project_url": "https://dr.miromind.ai/",
      "authors": [
        "MiroMind Team",
        "Song Bai",
        "Lidong Bing",
        "Carson Chen",
        "Guanzheng Chen",
        "Yuntao Chen",
        "Zhe Chen",
        "Ziyi Chen",
        "Jifeng Dai",
        "Xuan Dong",
        "Yue Deng",
        "Yunjie Fu",
        "Junqi Ge",
        "Chenxia Han",
        "Tammy Huang",
        "Zhenhang Huang",
        "Jerry Jiao",
        "Shilei Jiang",
        "Tianyu Jiao",
        "Xiaoqi Jian",
        "Lei Lei",
        "Ruilin Li",
        "Ryan Luo",
        "Tiantong Li",
        "Xiang Lin",
        "Ziyuan Liu",
        "Zhiqi Li",
        "Jie Ni",
        "Qiang Ren",
        "Pax Sun",
        "Shiqian Su",
        "Chenxin Tao",
        "Bin Wang",
        "Hellen Wang",
        "Haonan Wang",
        "James Wang",
        "Jin Wang",
        "Jojo Wang",
        "Letian Wang",
        "Shizun Wang",
        "Weizhi Wang",
        "Zixuan Wang",
        "Jinfan Xu",
        "Sen Xing",
        "Chenyu Yang",
        "Hai Ye",
        "Jiaheng Yu",
        "Yue Yu",
        "Muyan Zhong",
        "Tianchen Zhao",
        "Xizhou Zhu",
        "Yanpeng Zhou",
        "Yifan Zhang",
        "Zhi Zhu"
      ],
      "summary": "We present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or context length, MiroThinker explores interaction scaling at the model level, systematically training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement. Unlike LLM test-time scaling, which operates in isolation and risks degradation with longer reasoning chains, interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories. Through reinforcement learning, the model achieves efficient interaction scaling: with a 256K context window, it can perform up to 600 tool calls per task, enabling sustained multi-turn reasoning and complex real-world research workflows. Across four representative benchmarks-GAIA, HLE, BrowseComp, and BrowseComp-ZH-the 72B variant achieves up to 81.9%, 37.7%, 47.1%, and 55.6% accuracy respectively, surpassing previous open-source agents and approaching commercial counterparts such as GPT-5-high. Our analysis reveals that MiroThinker benefits from interactive scaling consistently: research performance improves predictably as the model engages in deeper and more frequent agent-environment interactions, demonstrating that interaction depth exhibits scaling behaviors analogous to model size and context length. These findings establish interaction scaling as a third critical dimension for building next-generation open research agents, complementing model capacity and context windows.",
      "fetch_date": "2025-11-18",
      "num_comments": 4,
      "ai_summary": "",
      "ai_keywords": []
    }
  },
  {
    "id": "bf88f955951ceb1c5ba0121481fe4bf5",
    "source": "huggingface",
    "type": "paper",
    "title": "FAPO: Flawed-Aware Policy Optimization for Efficient and Reliable\n  Reasoning",
    "description": "Reinforcement learning with verifiable rewards (RLVR) has emerged as a\npromising paradigm for enhancing the reasoning capabilities of large language\nmodels (LLMs). In this context, models explore reas...<br/>Upvotes: 9<br/>GitHub Stars: 16410<br/>Authors: Yuyang Ding, Chi Zhang, Juntao Li<br/>ðŸ”— <a href=\"https://github.com/volcengine/verl/tree/main/recipe/fapo\">GitHub</a><br/>ðŸ”— <a href=\"https://fapo-rl.github.io/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2510.22543\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2510.22543",
    "external_url": "https://github.com/volcengine/verl/tree/main/recipe/fapo",
    "published_date": "2025-10-26T01:49:38.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2510.22543",
      "upvotes": 9,
      "github_stars": 16410,
      "github_url": "https://github.com/volcengine/verl/tree/main/recipe/fapo",
      "project_url": "https://fapo-rl.github.io/",
      "authors": [
        "Yuyang Ding",
        "Chi Zhang",
        "Juntao Li",
        "Haibin Lin",
        "Xin Liu",
        "Min Zhang"
      ],
      "summary": "Reinforcement learning with verifiable rewards (RLVR) has emerged as a\npromising paradigm for enhancing the reasoning capabilities of large language\nmodels (LLMs). In this context, models explore reasoning trajectories and\nexploit rollouts with correct answers as positive signals for policy\noptimization. However, these rollouts might involve flawed patterns such as\nanswer-guessing and jump-in-reasoning. Such flawed-positive rollouts are\nrewarded identically to fully correct ones, causing policy models to\ninternalize these unreliable reasoning patterns. In this work, we first conduct\na systematic study of flawed-positive rollouts in RL and find that they enable\nrapid capability gains during the early optimization stage, while constraining\nreasoning capability later by reinforcing unreliable patterns. Building on\nthese insights, we propose Flawed-Aware Policy Optimization (FAPO), which\npresents a parameter-free reward penalty for flawed-positive rollouts, enabling\nthe policy to leverage them as useful shortcuts in the warm-up stage, securing\nstable early gains, while gradually shifting optimization toward reliable\nreasoning in the later refinement stage. To accurately and comprehensively\ndetect flawed-positive rollouts, we introduce a generative reward model (GenRM)\nwith a process-level reward that precisely localizes reasoning errors.\nExperiments show that FAPO is effective in broad domains, improving outcome\ncorrectness, process reliability, and training stability without increasing the\ntoken budget.",
      "fetch_date": "2025-10-30",
      "num_comments": 1,
      "ai_summary": "Flawed-Aware Policy Optimization (FAPO) enhances reinforcement learning with verifiable rewards by penalizing flawed-positive rollouts, improving reasoning capability and training stability in large language models.",
      "ai_keywords": [
        "reinforcement learning",
        "verifiable rewards",
        "large language models",
        "reasoning trajectories",
        "policy optimization",
        "flawed-positive rollouts",
        "parameter-free reward penalty",
        "generative reward model",
        "process-level reward",
        "reasoning errors"
      ]
    }
  },
  {
    "id": "f37a636c7cc5f515c6866d60bda27643",
    "source": "huggingface",
    "type": "paper",
    "title": "UFO^3: Weaving the Digital Agent Galaxy",
    "description": "Large language model (LLM)-powered agents are transforming digital devices from passive tools into proactive intelligent collaborators. However, most existing frameworks remain confined to a single OS...<br/>Upvotes: 17<br/>GitHub Stars: 7733<br/>Authors: Chaoyun Zhang, Liqun Li, He Huang<br/>ðŸ”— <a href=\"https://github.com/microsoft/UFO/\">GitHub</a><br/>ðŸ”— <a href=\"https://microsoft.github.io/UFO/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2511.11332\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2511.11332",
    "external_url": "https://github.com/microsoft/UFO/",
    "published_date": "2025-11-14T09:05:31.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2511.11332",
      "upvotes": 17,
      "github_stars": 7733,
      "github_url": "https://github.com/microsoft/UFO/",
      "project_url": "https://microsoft.github.io/UFO/",
      "authors": [
        "Chaoyun Zhang",
        "Liqun Li",
        "He Huang",
        "Chiming Ni",
        "Bo Qiao",
        "Si Qin",
        "Yu Kang",
        "Minghua Ma",
        "Qingwei Lin",
        "Saravan Rajmohan",
        "Dongmei Zhang"
      ],
      "summary": "Large language model (LLM)-powered agents are transforming digital devices from passive tools into proactive intelligent collaborators. However, most existing frameworks remain confined to a single OS or device, making cross-device workflows brittle and largely manual. We present UFO^3, a system that unifies heterogeneous endpoints, desktops, servers, mobile devices, and edge, into a single orchestration fabric. UFO^3 models each user request as a mutable TaskConstellation: a distributed DAG of atomic subtasks (TaskStars) with explicit control and data dependencies (TaskStarLines). The TaskConstellation continuously evolves as results stream in from distributed devices, enabling asynchronous execution, adaptive recovery, and dynamic optimization. A Constellation Orchestrator} executes tasks safely and asynchronously while applying dynamic DAG updates, and the Agent Interaction Protocol (AIP) provides persistent, low-latency channels for reliable task dispatch and result streaming. These designs dissolve the traditional boundaries between devices and platforms, allowing agents to collaborate seamlessly and amplify their collective intelligence.\n  We evaluate UFO^3 on NebulaBench, a benchmark of 55 cross-device tasks across 5 machines and 10 categories. UFO^3 achieves 83.3% subtask completion, 70.9% task success, exposes parallelism with an average width of 1.72, and reduces end-to-end latency by 31% relative to a sequential baseline. Fault-injection experiments demonstrate graceful degradation and recovery under transient and permanent agent failures. These results show that UFO^3 achieves accurate, efficient, and resilient task orchestration across heterogeneous devices, uniting isolated agents into a coherent, adaptive computing fabric that extends across the landscape of ubiquitous computing.",
      "fetch_date": "2025-11-18",
      "num_comments": 3,
      "ai_summary": "UFO$^3$ unifies heterogeneous devices into a single orchestration fabric, enabling seamless task collaboration and dynamic optimization across distributed environments.",
      "ai_keywords": [
        "TaskConstellation",
        "TaskStars",
        "TaskStarLines",
        "DAG",
        "Constellation Orchestrator",
        "Agent Interaction Protocol",
        "NebulaBench",
        "parallelism",
        "end-to-end latency",
        "fault-injection",
        "adaptive computing fabric",
        "ubiquitous computing"
      ]
    }
  },
  {
    "id": "56964b5c4b70a7c48e204a4541b8da35",
    "source": "huggingface",
    "type": "paper",
    "title": "Black-Box On-Policy Distillation of Large Language Models",
    "description": "Black-box distillation creates student large language models (LLMs) by learning from a proprietary teacher model's text outputs alone, without access to its internal logits or parameters. In this work...<br/>Upvotes: 41<br/>GitHub Stars: 4193<br/>Authors: Tianzhu Ye, Li Dong, Zewen Chi<br/>ðŸ”— <a href=\"https://github.com/microsoft/LMOps/tree/main/gad\">GitHub</a><br/>ðŸ”— <a href=\"https://aka.ms/GAD-project\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2511.10643\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2511.10643",
    "external_url": "https://github.com/microsoft/LMOps/tree/main/gad",
    "published_date": "2025-11-13T13:58:37.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2511.10643",
      "upvotes": 41,
      "github_stars": 4193,
      "github_url": "https://github.com/microsoft/LMOps/tree/main/gad",
      "project_url": "https://aka.ms/GAD-project",
      "authors": [
        "Tianzhu Ye",
        "Li Dong",
        "Zewen Chi",
        "Xun Wu",
        "Shaohan Huang",
        "Furu Wei"
      ],
      "summary": "Black-box distillation creates student large language models (LLMs) by learning from a proprietary teacher model's text outputs alone, without access to its internal logits or parameters. In this work, we introduce Generative Adversarial Distillation (GAD), which enables on-policy and black-box distillation. GAD frames the student LLM as a generator and trains a discriminator to distinguish its responses from the teacher LLM's, creating a minimax game. The discriminator acts as an on-policy reward model that co-evolves with the student, providing stable, adaptive feedback. Experimental results show that GAD consistently surpasses the commonly used sequence-level knowledge distillation. In particular, Qwen2.5-14B-Instruct (student) trained with GAD becomes comparable to its teacher, GPT-5-Chat, on the LMSYS-Chat automatic evaluation. The results establish GAD as a promising and effective paradigm for black-box LLM distillation.",
      "fetch_date": "2025-11-14",
      "num_comments": 3,
      "ai_summary": "Generative Adversarial Distillation (GAD) enhances black-box distillation by framing the student model as a generator and using a discriminator to provide adaptive feedback, surpassing traditional sequence-level knowledge distillation.",
      "ai_keywords": [
        "black-box distillation",
        "large language models (LLMs)",
        "Generative Adversarial Distillation (GAD)",
        "generator",
        "discriminator",
        "minimax game",
        "on-policy reward model",
        "sequence-level knowledge distillation",
        "LMSYS-Chat automatic evaluation"
      ]
    }
  },
  {
    "id": "979bc288b697d0a635887e56b67befef",
    "source": "huggingface",
    "type": "paper",
    "title": "SAM 3D: 3Dfy Anything in Images",
    "description": "We present SAM 3D, a generative model for visually grounded 3D object reconstruction, predicting geometry, texture, and layout from a single image. SAM 3D excels in natural images, where occlusion and...<br/>Upvotes: 64<br/>GitHub Stars: 3219<br/>Authors: SAM 3D Team, Xingyu Chen, Fu-Jen Chu<br/>ðŸ”— <a href=\"https://github.com/facebookresearch/sam-3d-objects\">GitHub</a><br/>ðŸ”— <a href=\"https://ai.meta.com/sam3d/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2511.16624\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2511.16624",
    "external_url": "https://github.com/facebookresearch/sam-3d-objects",
    "published_date": "2025-11-20T13:31:46.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2511.16624",
      "upvotes": 64,
      "github_stars": 3219,
      "github_url": "https://github.com/facebookresearch/sam-3d-objects",
      "project_url": "https://ai.meta.com/sam3d/",
      "authors": [
        "SAM 3D Team",
        "Xingyu Chen",
        "Fu-Jen Chu",
        "Pierre Gleize",
        "Kevin J Liang",
        "Alexander Sax",
        "Hao Tang",
        "Weiyao Wang",
        "Michelle Guo",
        "Thibaut Hardin",
        "Xiang Li",
        "Aohan Lin",
        "Jiawei Liu",
        "Ziqi Ma",
        "Anushka Sagar",
        "Bowen Song",
        "Xiaodong Wang",
        "Jianing Yang",
        "Bowen Zhang",
        "Piotr DollÃ¡r",
        "Georgia Gkioxari",
        "Matt Feiszli",
        "Jitendra Malik"
      ],
      "summary": "We present SAM 3D, a generative model for visually grounded 3D object reconstruction, predicting geometry, texture, and layout from a single image. SAM 3D excels in natural images, where occlusion and scene clutter are common and visual recognition cues from context play a larger role. We achieve this with a human- and model-in-the-loop pipeline for annotating object shape, texture, and pose, providing visually grounded 3D reconstruction data at unprecedented scale. We learn from this data in a modern, multi-stage training framework that combines synthetic pretraining with real-world alignment, breaking the 3D \"data barrier\". We obtain significant gains over recent work, with at least a 5:1 win rate in human preference tests on real-world objects and scenes. We will release our code and model weights, an online demo, and a new challenging benchmark for in-the-wild 3D object reconstruction.",
      "fetch_date": "2025-11-21",
      "num_comments": 3,
      "ai_summary": "SAM 3D is a generative model that reconstructs 3D objects from single images using a multi-stage training framework that includes synthetic pretraining and real-world alignment, achieving high performance in human preference tests.",
      "ai_keywords": [
        "generative model",
        "3D object reconstruction",
        "geometry",
        "texture",
        "layout",
        "human-in-the-loop",
        "object shape",
        "object pose",
        "multi-stage training",
        "synthetic pretraining",
        "real-world alignment",
        "3D data barrier",
        "human preference tests",
        "benchmark"
      ]
    }
  },
  {
    "id": "516eebe0ff75c447a2516832b9987a1b",
    "source": "huggingface",
    "type": "paper",
    "title": "Depth Anything 3: Recovering the Visual Space from Any Views",
    "description": "We present Depth Anything 3 (DA3), a model that predicts spatially consistent geometry from an arbitrary number of visual inputs, with or without known camera poses. In pursuit of minimal modeling, DA...<br/>Upvotes: 80<br/>GitHub Stars: 2636<br/>Authors: Haotong Lin, Sili Chen, Junhao Liew<br/>ðŸ”— <a href=\"https://github.com/ByteDance-Seed/depth-anything-3\">GitHub</a><br/>ðŸ”— <a href=\"https://depth-anything-3.github.io/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2511.10647\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2511.10647",
    "external_url": "https://github.com/ByteDance-Seed/depth-anything-3",
    "published_date": "2025-11-13T13:59:53.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2511.10647",
      "upvotes": 80,
      "github_stars": 2636,
      "github_url": "https://github.com/ByteDance-Seed/depth-anything-3",
      "project_url": "https://depth-anything-3.github.io/",
      "authors": [
        "Haotong Lin",
        "Sili Chen",
        "Junhao Liew",
        "Donny Y. Chen",
        "Zhenyu Li",
        "Guang Shi",
        "Jiashi Feng",
        "Bingyi Kang"
      ],
      "summary": "We present Depth Anything 3 (DA3), a model that predicts spatially consistent geometry from an arbitrary number of visual inputs, with or without known camera poses. In pursuit of minimal modeling, DA3 yields two key insights: a single plain transformer (e.g., vanilla DINO encoder) is sufficient as a backbone without architectural specialization, and a singular depth-ray prediction target obviates the need for complex multi-task learning. Through our teacher-student training paradigm, the model achieves a level of detail and generalization on par with Depth Anything 2 (DA2). We establish a new visual geometry benchmark covering camera pose estimation, any-view geometry and visual rendering. On this benchmark, DA3 sets a new state-of-the-art across all tasks, surpassing prior SOTA VGGT by an average of 44.3% in camera pose accuracy and 25.1% in geometric accuracy. Moreover, it outperforms DA2 in monocular depth estimation. All models are trained exclusively on public academic datasets.",
      "fetch_date": "2025-11-14",
      "num_comments": 4,
      "ai_summary": "Depth Anything 3 (DA3) uses a plain transformer for geometry prediction from visual inputs, achieving state-of-the-art results in camera pose estimation, any-view geometry, visual rendering, and monocular depth estimation.",
      "ai_keywords": [
        "plain transformer",
        "vanilla DINO encoder",
        "depth-ray prediction",
        "teacher-student training paradigm",
        "visual geometry benchmark",
        "camera pose estimation",
        "any-view geometry",
        "visual rendering",
        "monocular depth estimation",
        "state-of-the-art (SOTA)",
        "VGGT"
      ]
    }
  },
  {
    "id": "9f02fd60060fcf9a1cc7669055b02143",
    "source": "huggingface",
    "type": "paper",
    "title": "Emu3.5: Native Multimodal Models are World Learners",
    "description": "We introduce Emu3.5, a large-scale multimodal world model that natively\npredicts the next state across vision and language. Emu3.5 is pre-trained\nend-to-end with a unified next-token prediction object...<br/>Upvotes: 103<br/>GitHub Stars: 1272<br/>Authors: Yufeng Cui, Honghao Chen, Haoge Deng<br/>ðŸ”— <a href=\"https://github.com/baaivision/Emu3.5\">GitHub</a><br/>ðŸ”— <a href=\"https://emu.world/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2510.26583\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2510.26583",
    "external_url": "https://github.com/baaivision/Emu3.5",
    "published_date": "2025-10-30T11:11:16.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2510.26583",
      "upvotes": 103,
      "github_stars": 1272,
      "github_url": "https://github.com/baaivision/Emu3.5",
      "project_url": "https://emu.world/",
      "authors": [
        "Yufeng Cui",
        "Honghao Chen",
        "Haoge Deng",
        "Xu Huang",
        "Xinghang Li",
        "Jirong Liu",
        "Yang Liu",
        "Zhuoyan Luo",
        "Jinsheng Wang",
        "Wenxuan Wang",
        "Yueze Wang",
        "Chengyuan Wang",
        "Fan Zhang",
        "Yingli Zhao",
        "Ting Pan",
        "Xianduo Li",
        "Zecheng Hao",
        "Wenxuan Ma",
        "Zhuo Chen",
        "Yulong Ao",
        "Tiejun Huang",
        "Zhongyuan Wang",
        "Xinlong Wang"
      ],
      "summary": "We introduce Emu3.5, a large-scale multimodal world model that natively\npredicts the next state across vision and language. Emu3.5 is pre-trained\nend-to-end with a unified next-token prediction objective on a corpus of\nvision-language interleaved data containing over 10 trillion tokens, primarily\nderived from sequential frames and transcripts of internet videos. The model\nnaturally accepts interleaved vision-language inputs and generates interleaved\nvision-language outputs. Emu3.5 is further post-trained with large-scale\nreinforcement learning to enhance multimodal reasoning and generation. To\nimprove inference efficiency, we propose Discrete Diffusion Adaptation (DiDA),\nwhich converts token-by-token decoding into bidirectional parallel prediction,\naccelerating per-image inference by about 20x without sacrificing performance.\nEmu3.5 exhibits strong native multimodal capabilities, including long-horizon\nvision-language generation, any-to-image (X2I) generation, and complex\ntext-rich image generation. It also exhibits generalizable world-modeling\nabilities, enabling spatiotemporally consistent world exploration and\nopen-world embodied manipulation across diverse scenarios and tasks. For\ncomparison, Emu3.5 achieves performance comparable to Gemini 2.5 Flash Image\n(Nano Banana) on image generation and editing tasks and demonstrates superior\nresults on a suite of interleaved generation tasks. We open-source Emu3.5 at\nhttps://github.com/baaivision/Emu3.5 to support community research.",
      "fetch_date": "2025-10-31",
      "num_comments": 4,
      "ai_summary": "Emu3.5, a large-scale multimodal world model, predicts next states in vision and language, enhanced with reinforcement learning and Discrete Diffusion Adaptation for efficient inference, achieving strong performance in various multimodal tasks.",
      "ai_keywords": [
        "multimodal world model",
        "next-token prediction",
        "vision-language interleaved data",
        "Discrete Diffusion Adaptation",
        "bidirectional parallel prediction",
        "long-horizon vision-language generation",
        "any-to-image generation",
        "complex text-rich image generation",
        "spatiotemporally consistent world exploration",
        "open-world embodied manipulation"
      ]
    }
  }
]