[
  {
    "id": "8ccd2aceeb4aa375ae3aaa881748eeca",
    "source": "huggingface",
    "type": "paper",
    "title": "Sharing is Caring: Efficient LM Post-Training with Collective RL\n  Experience Sharing",
    "description": "Post-training language models (LMs) with reinforcement learning (RL) can\nenhance their complex reasoning capabilities without supervised fine-tuning, as\ndemonstrated by DeepSeek-R1-Zero. However, effe...<br/>Upvotes: 618<br/>GitHub Stars: 1442<br/>Authors: Jeffrey Amico, Gabriel Passamani Andrade, John Donaghy<br/>ðŸ”— <a href=\"https://github.com/gensyn-ai/rl-swarm\">GitHub</a><br/>ðŸ”— <a href=\"https://blog.gensyn.ai/sapo-efficient-lm-post-training-with-collective-rl/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2509.08721\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2509.08721",
    "external_url": "https://github.com/gensyn-ai/rl-swarm",
    "published_date": "2025-09-10T12:14:20.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2509.08721",
      "upvotes": 618,
      "github_stars": 1442,
      "github_url": "https://github.com/gensyn-ai/rl-swarm",
      "project_url": "https://blog.gensyn.ai/sapo-efficient-lm-post-training-with-collective-rl/",
      "authors": [
        "Jeffrey Amico",
        "Gabriel Passamani Andrade",
        "John Donaghy",
        "Ben Fielding",
        "Tristin Forbus",
        "Harry Grieve",
        "Semih Kara",
        "Jari Kolehmainen",
        "Yihua Lou",
        "Christopher Nies",
        "Edward Phillip Flores NuÃ±o",
        "Diogo Ortega",
        "Shikhar Rastogi",
        "Austin Virts",
        "Matthew J. Wright"
      ],
      "summary": "Post-training language models (LMs) with reinforcement learning (RL) can\nenhance their complex reasoning capabilities without supervised fine-tuning, as\ndemonstrated by DeepSeek-R1-Zero. However, effectively utilizing RL for LMs\nrequires significant parallelization to scale-up inference, which introduces\nnon-trivial technical challenges (e.g. latency, memory, and reliability)\nalongside ever-growing financial costs. We present Swarm sAmpling Policy\nOptimization (SAPO), a fully decentralized and asynchronous RL post-training\nalgorithm. SAPO is designed for decentralized networks of heterogenous compute\nnodes, where each node manages its own policy model(s) while \"sharing\" rollouts\nwith others in the network; no explicit assumptions about latency, model\nhomogeneity, or hardware are required and nodes can operate in silo if desired.\nAs a result, the algorithm avoids common bottlenecks in scaling RL\npost-training while also allowing (and even encouraging) new possibilities. By\nsampling rollouts \"shared\" across the network, it enables \"Aha moments\" to\npropagate, thereby bootstrapping the learning process. In this paper we show\nSAPO achieved cumulative reward gains of up to 94% in controlled experiments.\nWe also share insights from tests on a network with thousands of nodes\ncontributed by Gensyn community members running the algorithm on diverse\nhardware and models during an open-source demo.",
      "fetch_date": "2025-09-10",
      "num_comments": 53,
      "ai_summary": "Swarm sAmpling Policy Optimization (SAPO) is a decentralized and asynchronous RL algorithm that enhances post-training language models without supervised fine-tuning, achieving significant reward gains and scalability across diverse hardware.",
      "ai_keywords": [
        "reinforcement learning",
        "post-training",
        "language models",
        "DeepSeek-R1-Zero",
        "Swarm sAmpling Policy Optimization",
        "SAPO",
        "decentralized networks",
        "heterogenous compute nodes",
        "policy models",
        "rollouts",
        "cumulative reward gains"
      ]
    }
  },
  {
    "id": "f4b634605f28912c3f352503c72078a6",
    "source": "huggingface",
    "type": "paper",
    "title": "A.S.E: A Repository-Level Benchmark for Evaluating Security in\n  AI-Generated Code",
    "description": "The increasing adoption of large language models (LLMs) in software\nengineering necessitates rigorous security evaluation of their generated code.\nHowever, existing benchmarks often lack relevance to ...<br/>Upvotes: 340<br/>GitHub Stars: 348<br/>Authors: Keke Lian, Bin Wang, Lei Zhang<br/>ðŸ”— <a href=\"https://github.com/Tencent/AICGSecEval\">GitHub</a><br/>ðŸ”— <a href=\"https://aicgseceval.tencent.com/home\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2508.18106\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2508.18106",
    "external_url": "https://github.com/Tencent/AICGSecEval",
    "published_date": "2025-08-25T11:11:11.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2508.18106",
      "upvotes": 340,
      "github_stars": 348,
      "github_url": "https://github.com/Tencent/AICGSecEval",
      "project_url": "https://aicgseceval.tencent.com/home",
      "authors": [
        "Keke Lian",
        "Bin Wang",
        "Lei Zhang",
        "Libo Chen",
        "Junjie Wang",
        "Ziming Zhao",
        "Yujiu Yang",
        "Haotong Duan",
        "Haoran Zhao",
        "Shuang Liao",
        "Mingda Guo",
        "Jiazheng Quan",
        "Yilu Zhong",
        "Chenhao He",
        "Zichuan Chen",
        "Jie Wu",
        "Haoling Li",
        "Zhaoxuan Li",
        "Jiongchi Yu",
        "Hui Li",
        "Dong Zhang"
      ],
      "summary": "The increasing adoption of large language models (LLMs) in software\nengineering necessitates rigorous security evaluation of their generated code.\nHowever, existing benchmarks often lack relevance to real-world AI programming\nscenarios, making them inadequate for assessing the practical security risks\nassociated with AI-generated code in production environments. To address this\ngap, we introduce A.S.E (AI Code Generation Security Evaluation), a\nrepository-level evaluation benchmark designed to closely mirror real-world AI\nprogramming tasks, offering a comprehensive and reliable framework for\nassessing the security of AI-generated code. Our evaluation of leading LLMs on\nA.S.E reveals several key findings. In particular, current LLMs still struggle\nwith secure coding. The complexity in repository-level scenarios presents\nchallenges for LLMs that typically perform well on snippet-level tasks.\nMorever, a larger reasoning budget does not necessarily lead to better code\ngeneration. These observations offer valuable insights into the current state\nof AI code generation, assisting developers in selecting the most appropriate\nmodels for practical tasks, while laying the foundation for refining LLMs to\ngenerate secure and efficient code in real-world applications.",
      "fetch_date": "2025-09-01",
      "num_comments": 4,
      "ai_summary": "A.S.E is a repository-level benchmark for evaluating the security of AI-generated code, highlighting challenges in secure coding and the limitations of LLMs in real-world scenarios.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "AI code generation",
        "security evaluation",
        "repository-level evaluation",
        "secure coding",
        "reasoning budget"
      ]
    }
  },
  {
    "id": "a785dc23394ddf009c4095e44be6b818",
    "source": "huggingface",
    "type": "paper",
    "title": "VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action\n  Model",
    "description": "Vision-Language-Action (VLA) models typically bridge the gap between\nperceptual and action spaces by pre-training a large-scale Vision-Language\nModel (VLM) on robotic data. While this approach greatly...<br/>Upvotes: 215<br/>GitHub Stars: 443<br/>Authors: Yihao Wang, Pengxiang Ding, Lingxiao Li<br/>ðŸ”— <a href=\"https://github.com/OpenHelix-Team/VLA-Adapter\">GitHub</a><br/>ðŸ”— <a href=\"https://vla-adapter.github.io/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2509.09372\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2509.09372",
    "external_url": "https://github.com/OpenHelix-Team/VLA-Adapter",
    "published_date": "2025-09-11T07:42:21.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2509.09372",
      "upvotes": 215,
      "github_stars": 443,
      "github_url": "https://github.com/OpenHelix-Team/VLA-Adapter",
      "project_url": "https://vla-adapter.github.io/",
      "authors": [
        "Yihao Wang",
        "Pengxiang Ding",
        "Lingxiao Li",
        "Can Cui",
        "Zirui Ge",
        "Xinyang Tong",
        "Wenxuan Song",
        "Han Zhao",
        "Wei Zhao",
        "Pengxu Hou",
        "Siteng Huang",
        "Yifan Tang",
        "Wenhui Wang",
        "Ru Zhang",
        "Jianyi Liu",
        "Donglin Wang"
      ],
      "summary": "Vision-Language-Action (VLA) models typically bridge the gap between\nperceptual and action spaces by pre-training a large-scale Vision-Language\nModel (VLM) on robotic data. While this approach greatly enhances performance,\nit also incurs significant training costs. In this paper, we investigate how to\neffectively bridge vision-language (VL) representations to action (A). We\nintroduce VLA-Adapter, a novel paradigm designed to reduce the reliance of VLA\nmodels on large-scale VLMs and extensive pre-training. To this end, we first\nsystematically analyze the effectiveness of various VL conditions and present\nkey findings on which conditions are essential for bridging perception and\naction spaces. Based on these insights, we propose a lightweight Policy module\nwith Bridge Attention, which autonomously injects the optimal condition into\nthe action space. In this way, our method achieves high performance using only\na 0.5B-parameter backbone, without any robotic data pre-training. Extensive\nexperiments on both simulated and real-world robotic benchmarks demonstrate\nthat VLA-Adapter not only achieves state-of-the-art level performance, but also\noffers the fast inference speed reported to date. Furthermore, thanks to the\nproposed advanced bridging paradigm, VLA-Adapter enables the training of a\npowerful VLA model in just 8 hours on a single consumer-grade GPU, greatly\nlowering the barrier to deploying the VLA model. Project page:\nhttps://vla-adapter.github.io/.",
      "fetch_date": "2025-09-12",
      "num_comments": 5,
      "ai_summary": "VLA-Adapter reduces reliance on large-scale VLMs and extensive pre-training by using a lightweight Policy module with Bridge Attention, achieving state-of-the-art performance and fast inference speed with minimal computational resources.",
      "ai_keywords": [
        "VLA models",
        "Vision-Language Model (VLM)",
        "robotic data",
        "VL conditions",
        "Policy module",
        "Bridge Attention",
        "parameter backbone",
        "simulated benchmarks",
        "real-world benchmarks",
        "inference speed",
        "consumer-grade GPU"
      ]
    }
  },
  {
    "id": "b3695a804f022da86db7e28029ec25d4",
    "source": "huggingface",
    "type": "paper",
    "title": "The Landscape of Agentic Reinforcement Learning for LLMs: A Survey",
    "description": "The emergence of agentic reinforcement learning (Agentic RL) marks a paradigm\nshift from conventional reinforcement learning applied to large language models\n(LLM RL), reframing LLMs from passive sequ...<br/>Upvotes: 206<br/>GitHub Stars: 813<br/>Authors: Guibin Zhang, Hejia Geng, Xiaohang Yu<br/>ðŸ”— <a href=\"https://github.com/xhyumiracle/Awesome-AgenticLLM-RL-Papers\">GitHub</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2509.02547\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2509.02547",
    "external_url": "https://github.com/xhyumiracle/Awesome-AgenticLLM-RL-Papers",
    "published_date": "2025-09-02T13:46:26.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2509.02547",
      "upvotes": 206,
      "github_stars": 813,
      "github_url": "https://github.com/xhyumiracle/Awesome-AgenticLLM-RL-Papers",
      "project_url": "",
      "authors": [
        "Guibin Zhang",
        "Hejia Geng",
        "Xiaohang Yu",
        "Zhenfei Yin",
        "Zaibin Zhang",
        "Zelin Tan",
        "Heng Zhou",
        "Zhongzhi Li",
        "Xiangyuan Xue",
        "Yijiang Li",
        "Yifan Zhou",
        "Yang Chen",
        "Chen Zhang",
        "Yutao Fan",
        "Zihu Wang",
        "Songtao Huang",
        "Yue Liao",
        "Hongru Wang",
        "Mengyue Yang",
        "Heng Ji",
        "Michael Littman",
        "Jun Wang",
        "Shuicheng Yan",
        "Philip Torr",
        "Lei Bai"
      ],
      "summary": "The emergence of agentic reinforcement learning (Agentic RL) marks a paradigm\nshift from conventional reinforcement learning applied to large language models\n(LLM RL), reframing LLMs from passive sequence generators into autonomous,\ndecision-making agents embedded in complex, dynamic worlds. This survey\nformalizes this conceptual shift by contrasting the degenerate single-step\nMarkov Decision Processes (MDPs) of LLM-RL with the temporally extended,\npartially observable Markov decision processes (POMDPs) that define Agentic RL.\nBuilding on this foundation, we propose a comprehensive twofold taxonomy: one\norganized around core agentic capabilities, including planning, tool use,\nmemory, reasoning, self-improvement, and perception, and the other around their\napplications across diverse task domains. Central to our thesis is that\nreinforcement learning serves as the critical mechanism for transforming these\ncapabilities from static, heuristic modules into adaptive, robust agentic\nbehavior. To support and accelerate future research, we consolidate the\nlandscape of open-source environments, benchmarks, and frameworks into a\npractical compendium. By synthesizing over five hundred recent works, this\nsurvey charts the contours of this rapidly evolving field and highlights the\nopportunities and challenges that will shape the development of scalable,\ngeneral-purpose AI agents.",
      "fetch_date": "2025-09-03",
      "num_comments": 3,
      "ai_summary": "Agentic reinforcement learning transforms large language models into autonomous decision-making agents by leveraging temporally extended POMDPs, enhancing capabilities like planning and reasoning through reinforcement learning.",
      "ai_keywords": [
        "agentic reinforcement learning",
        "LLM RL",
        "Markov Decision Processes",
        "POMDPs",
        "planning",
        "tool use",
        "memory",
        "reasoning",
        "self-improvement",
        "perception",
        "reinforcement learning",
        "open-source environments",
        "benchmarks",
        "frameworks"
      ]
    }
  },
  {
    "id": "2a53ea33f47888c723fab2a38e7d2442",
    "source": "huggingface",
    "type": "paper",
    "title": "Drivel-ology: Challenging LLMs with Interpreting Nonsense with Depth",
    "description": "We introduce Drivelology, a unique linguistic phenomenon characterised as\n\"nonsense with depth\", utterances that are syntactically coherent yet\npragmatically paradoxical, emotionally loaded, or rhetor...<br/>Upvotes: 203<br/>GitHub Stars: 6<br/>Authors: Yang Wang, Chenghao Xiao, Chia-Yi Hsiao<br/>ðŸ”— <a href=\"https://github.com/ExtraOrdinaryLab/drivelology\">GitHub</a><br/>ðŸ”— <a href=\"https://huggingface.co/datasets/extraordinarylab/drivel-hub\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2509.03867\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2509.03867",
    "external_url": "https://github.com/ExtraOrdinaryLab/drivelology",
    "published_date": "2025-09-03T23:58:55.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2509.03867",
      "upvotes": 203,
      "github_stars": 6,
      "github_url": "https://github.com/ExtraOrdinaryLab/drivelology",
      "project_url": "https://huggingface.co/datasets/extraordinarylab/drivel-hub",
      "authors": [
        "Yang Wang",
        "Chenghao Xiao",
        "Chia-Yi Hsiao",
        "Zi Yan Chang",
        "Chi-Li Chen",
        "Tyler Loakman",
        "Chenghua Lin"
      ],
      "summary": "We introduce Drivelology, a unique linguistic phenomenon characterised as\n\"nonsense with depth\", utterances that are syntactically coherent yet\npragmatically paradoxical, emotionally loaded, or rhetorically subversive.\nWhile such expressions may resemble surface-level nonsense, they encode\nimplicit meaning requiring contextual inference, moral reasoning, or emotional\ninterpretation. We find that current large language models (LLMs), despite\nexcelling at many natural language processing (NLP) tasks, consistently fail to\ngrasp the layered semantics of Drivelological text. To investigate this, we\nconstruct a small but diverse benchmark dataset of over 1,200 meticulously\ncurated examples, with select instances in English, Mandarin, Spanish, French,\nJapanese, and Korean. Annotation was especially challenging: each of the\nexamples required careful expert review to verify that it truly reflected\nDrivelological characteristics. The process involved multiple rounds of\ndiscussion and adjudication to address disagreements, highlighting the subtle\nand subjective nature of the Drivelology. We evaluate a range of LLMs on\nclassification, generation, and reasoning tasks. Our results reveal clear\nlimitations of LLMs: models often confuse Drivelology with shallow nonsense,\nproduce incoherent justifications, or miss the implied rhetorical function\naltogether. These findings highlight a deeper representational gap in LLMs'\npragmatic understanding and challenge the assumption that statistical fluency\nimplies cognitive comprehension. We release our dataset and code to facilitate\nfurther research in modelling linguistic depth beyond surface-level coherence.",
      "fetch_date": "2025-09-05",
      "num_comments": 10,
      "ai_summary": "LLMs struggle with understanding the nuanced, context-dependent meanings of Drivelological text, which appears nonsensical but contains deeper semantic layers.",
      "ai_keywords": [
        "Drivelology",
        "large language models",
        "LLMs",
        "natural language processing",
        "NLP",
        "benchmark dataset",
        "Drivelological text",
        "classification",
        "generation",
        "reasoning tasks",
        "pragmatic understanding",
        "cognitive comprehension"
      ]
    }
  },
  {
    "id": "f7bf7ea1a54c9f21c1a3c2958aa1d217",
    "source": "huggingface",
    "type": "paper",
    "title": "Why Language Models Hallucinate",
    "description": "Like students facing hard exam questions, large language models sometimes\nguess when uncertain, producing plausible yet incorrect statements instead of\nadmitting uncertainty. Such \"hallucinations\" per...<br/>Upvotes: 178<br/>Authors: Adam Tauman Kalai, Ofir Nachum, Santosh S. Vempala<br/>ðŸ”— <a href=\"https://huggingface.co/papers/2509.04664\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2509.04664",
    "external_url": "",
    "published_date": "2025-09-04T17:26:31.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2509.04664",
      "upvotes": 178,
      "github_stars": 0,
      "github_url": "",
      "project_url": "",
      "authors": [
        "Adam Tauman Kalai",
        "Ofir Nachum",
        "Santosh S. Vempala",
        "Edwin Zhang"
      ],
      "summary": "Like students facing hard exam questions, large language models sometimes\nguess when uncertain, producing plausible yet incorrect statements instead of\nadmitting uncertainty. Such \"hallucinations\" persist even in state-of-the-art\nsystems and undermine trust. We argue that language models hallucinate because\nthe training and evaluation procedures reward guessing over acknowledging\nuncertainty, and we analyze the statistical causes of hallucinations in the\nmodern training pipeline. Hallucinations need not be mysterious -- they\noriginate simply as errors in binary classification. If incorrect statements\ncannot be distinguished from facts, then hallucinations in pretrained language\nmodels will arise through natural statistical pressures. We then argue that\nhallucinations persist due to the way most evaluations are graded -- language\nmodels are optimized to be good test-takers, and guessing when uncertain\nimproves test performance. This \"epidemic\" of penalizing uncertain responses\ncan only be addressed through a socio-technical mitigation: modifying the\nscoring of existing benchmarks that are misaligned but dominate leaderboards,\nrather than introducing additional hallucination evaluations. This change may\nsteer the field toward more trustworthy AI systems.",
      "fetch_date": "2025-09-08",
      "num_comments": 8,
      "ai_summary": "Language models produce incorrect statements due to training and evaluation procedures that reward guessing over acknowledging uncertainty, leading to a need for socio-technical changes in benchmark scoring.",
      "ai_keywords": [
        "hallucinations",
        "binary classification",
        "uncertain responses",
        "trustworthy AI systems"
      ]
    }
  },
  {
    "id": "59f26db9f2ae0d3d43973c1ffbd40568",
    "source": "huggingface",
    "type": "paper",
    "title": "MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and\n  Training Recipe",
    "description": "Multimodal Large Language Models (MLLMs) are undergoing rapid progress and\nrepresent the frontier of AI development. However, their training and inference\nefficiency have emerged as a core bottleneck ...<br/>Upvotes: 44<br/>GitHub Stars: 21993<br/>Authors: Tianyu Yu, Zefan Wang, Chongyi Wang<br/>ðŸ”— <a href=\"https://github.com/OpenBMB/MiniCPM-V\">GitHub</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2509.18154\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2509.18154",
    "external_url": "https://github.com/OpenBMB/MiniCPM-V",
    "published_date": "2025-09-16T15:41:48.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2509.18154",
      "upvotes": 44,
      "github_stars": 21993,
      "github_url": "https://github.com/OpenBMB/MiniCPM-V",
      "project_url": "",
      "authors": [
        "Tianyu Yu",
        "Zefan Wang",
        "Chongyi Wang",
        "Fuwei Huang",
        "Wenshuo Ma",
        "Zhihui He",
        "Tianchi Cai",
        "Weize Chen",
        "Yuxiang Huang",
        "Yuanqian Zhao",
        "Bokai Xu",
        "Junbo Cui",
        "Yingjing Xu",
        "Liqing Ruan",
        "Luoyuan Zhang",
        "Hanyu Liu",
        "Jingkun Tang",
        "Hongyuan Liu",
        "Qining Guo",
        "Wenhao Hu",
        "Bingxiang He",
        "Jie Zhou",
        "Jie Cai",
        "Ji Qi",
        "Zonghao Guo",
        "Chi Chen",
        "Guoyang Zeng",
        "Yuxuan Li",
        "Ganqu Cui",
        "Ning Ding",
        "Xu Han",
        "Yuan Yao",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "summary": "Multimodal Large Language Models (MLLMs) are undergoing rapid progress and\nrepresent the frontier of AI development. However, their training and inference\nefficiency have emerged as a core bottleneck in making MLLMs more accessible\nand scalable. To address the challenges, we present MiniCPM-V 4.5, an 8B\nparameter model designed for high efficiency and strong performance. We\nintroduce three core improvements in model architecture, data strategy and\ntraining method: a unified 3D-Resampler model architecture for highly compact\nencoding over images and videos, a unified learning paradigm for document\nknowledge and text recognition without heavy data engineering, and a hybrid\nreinforcement learning strategy for proficiency in both short and long\nreasoning modes. Comprehensive experimental results in OpenCompass evaluation\nshow that MiniCPM-V 4.5 surpasses widely used proprietary models such as\nGPT-4o-latest, and significantly larger open-source models such as Qwen2.5-VL\n72B. Notably, the strong performance is achieved with remarkable efficiency.\nFor example, on the widely adopted VideoMME benchmark, MiniCPM-V 4.5 achieves\nstate-of-the-art performance among models under 30B size, using just 46.7\\% GPU\nmemory cost and 8.7\\% inference time of Qwen2.5-VL 7B.",
      "fetch_date": "2025-09-24",
      "num_comments": 4,
      "ai_summary": "MiniCPM-V 4.5, a 8B parameter multimodal large language model, achieves high performance and efficiency through a unified 3D-Resampler architecture, a unified learning paradigm, and a hybrid reinforcement learning strategy.",
      "ai_keywords": [
        "3D-Resampler",
        "unified learning paradigm",
        "hybrid reinforcement learning strategy",
        "multimodal large language models",
        "OpenCompass evaluation",
        "VideoMME benchmark"
      ]
    }
  },
  {
    "id": "17d435d0d4a4864c8247d4e12399b781",
    "source": "huggingface",
    "type": "paper",
    "title": "Scaling Agents via Continual Pre-training",
    "description": "Large language models (LLMs) have evolved into agentic systems capable of\nautonomous tool use and multi-step reasoning for complex problem-solving.\nHowever, post-training approaches building upon gene...<br/>Upvotes: 103<br/>GitHub Stars: 14776<br/>Authors: Liangcai Su, Zhen Zhang, Guangyu Li<br/>ðŸ”— <a href=\"https://github.com/Alibaba-NLP/DeepResearch///\">GitHub</a><br/>ðŸ”— <a href=\"https://tongyi-agent.github.io/blog/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2509.13310\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2509.13310",
    "external_url": "https://github.com/Alibaba-NLP/DeepResearch///",
    "published_date": "2025-09-16T13:57:19.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2509.13310",
      "upvotes": 103,
      "github_stars": 14776,
      "github_url": "https://github.com/Alibaba-NLP/DeepResearch///",
      "project_url": "https://tongyi-agent.github.io/blog/",
      "authors": [
        "Liangcai Su",
        "Zhen Zhang",
        "Guangyu Li",
        "Zhuo Chen",
        "Chenxi Wang",
        "Maojia Song",
        "Xinyu Wang",
        "Kuan Li",
        "Jialong Wu",
        "Xuanzhong Chen",
        "Zile Qiao",
        "Zhongwang Zhang",
        "Huifeng Yin",
        "Shihao Cai",
        "Runnan Fang",
        "Zhengwei Tao",
        "Wenbiao Yin",
        "Chenxiong Qian",
        "Yong Jiang",
        "Pengjun Xie",
        "Fei Huang",
        "Jingren Zhou"
      ],
      "summary": "Large language models (LLMs) have evolved into agentic systems capable of\nautonomous tool use and multi-step reasoning for complex problem-solving.\nHowever, post-training approaches building upon general-purpose foundation\nmodels consistently underperform in agentic tasks, particularly in open-source\nimplementations. We identify the root cause: the absence of robust agentic\nfoundation models forces models during post-training to simultaneously learn\ndiverse agentic behaviors while aligning them to expert demonstrations, thereby\ncreating fundamental optimization tensions. To this end, we are the first to\npropose incorporating Agentic Continual Pre-training (Agentic CPT) into the\ndeep research agents training pipeline to build powerful agentic foundational\nmodels. Based on this approach, we develop a deep research agent model named\nAgentFounder. We evaluate our AgentFounder-30B on 10 benchmarks and achieve\nstate-of-the-art performance while retains strong tool-use ability, notably\n39.9% on BrowseComp-en, 43.3% on BrowseComp-zh, and 31.5% Pass@1 on HLE.",
      "fetch_date": "2025-09-17",
      "num_comments": 9,
      "ai_summary": "AgentFounder, a deep research agent model incorporating Agentic Continual Pre-training, achieves state-of-the-art performance in agentic tasks while maintaining strong tool-use ability.",
      "ai_keywords": [
        "Large language models",
        "agentic systems",
        "autonomous tool use",
        "multi-step reasoning",
        "post-training approaches",
        "general-purpose foundation models",
        "agentic foundation models",
        "Agentic Continual Pre-training",
        "deep research agents",
        "AgentFounder",
        "BrowseComp-en",
        "BrowseComp-zh",
        "HLE"
      ]
    }
  },
  {
    "id": "27945bd6f1634bfd914ab287d0fdaf8e",
    "source": "huggingface",
    "type": "paper",
    "title": "WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for\n  Open-Ended Deep Research",
    "description": "This paper tackles open-ended deep research (OEDR), a complex challenge where\nAI agents must synthesize vast web-scale information into insightful reports.\nCurrent approaches are plagued by dual-fold ...<br/>Upvotes: 100<br/>GitHub Stars: 14776<br/>Authors: Zijian Li, Xin Guan, Bo Zhang<br/>ðŸ”— <a href=\"https://github.com/Alibaba-NLP/DeepResearch\">GitHub</a><br/>ðŸ”— <a href=\"https://tongyi-agent.github.io/blog/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2509.13312\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2509.13312",
    "external_url": "https://github.com/Alibaba-NLP/DeepResearch",
    "published_date": "2025-09-16T13:57:21.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2509.13312",
      "upvotes": 100,
      "github_stars": 14776,
      "github_url": "https://github.com/Alibaba-NLP/DeepResearch",
      "project_url": "https://tongyi-agent.github.io/blog/",
      "authors": [
        "Zijian Li",
        "Xin Guan",
        "Bo Zhang",
        "Shen Huang",
        "Houquan Zhou",
        "Shaopeng Lai",
        "Ming Yan",
        "Yong Jiang",
        "Pengjun Xie",
        "Fei Huang",
        "Jun Zhang",
        "Jingren Zhou"
      ],
      "summary": "This paper tackles open-ended deep research (OEDR), a complex challenge where\nAI agents must synthesize vast web-scale information into insightful reports.\nCurrent approaches are plagued by dual-fold limitations: static research\npipelines that decouple planning from evidence acquisition and one-shot\ngeneration paradigms that easily suffer from long-context failure issues like\n\"loss in the middle\" and hallucinations. To address these challenges, we\nintroduce WebWeaver, a novel dual-agent framework that emulates the human\nresearch process. The planner operates in a dynamic cycle, iteratively\ninterleaving evidence acquisition with outline optimization to produce a\ncomprehensive, source-grounded outline linking to a memory bank of evidence.\nThe writer then executes a hierarchical retrieval and writing process,\ncomposing the report section by section. By performing targeted retrieval of\nonly the necessary evidence from the memory bank for each part, it effectively\nmitigates long-context issues. Our framework establishes a new state-of-the-art\nacross major OEDR benchmarks, including DeepResearch Bench, DeepConsult, and\nDeepResearchGym. These results validate our human-centric, iterative\nmethodology, demonstrating that adaptive planning and focused synthesis are\ncrucial for producing high-quality, reliable, and well-structured reports.",
      "fetch_date": "2025-09-17",
      "num_comments": 5,
      "ai_summary": "WebWeaver, a dual-agent framework, addresses open-ended deep research challenges by integrating adaptive planning and focused synthesis to produce high-quality, reliable reports.",
      "ai_keywords": [
        "open-ended deep research",
        "AI agents",
        "static research pipelines",
        "one-shot generation",
        "long-context failure",
        "loss in the middle",
        "hallucinations",
        "dual-agent framework",
        "human research process",
        "planner",
        "evidence acquisition",
        "outline optimization",
        "memory bank",
        "writer",
        "hierarchical retrieval",
        "writing process",
        "DeepResearch Bench",
        "DeepConsult",
        "DeepResearchGym"
      ]
    }
  },
  {
    "id": "d4b4f91354d2792a0826158fb9d00e0b",
    "source": "huggingface",
    "type": "paper",
    "title": "WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic\n  Data and Scalable Reinforcement Learning",
    "description": "Transcending human cognitive limitations represents a critical frontier in\nLLM training. Proprietary agentic systems like DeepResearch have demonstrated\nsuperhuman capabilities on extremely complex in...<br/>Upvotes: 83<br/>GitHub Stars: 14776<br/>Authors: Kuan Li, Zhongwang Zhang, Huifeng Yin<br/>ðŸ”— <a href=\"https://github.com/Alibaba-NLP/DeepResearch/\">GitHub</a><br/>ðŸ”— <a href=\"https://tongyi-agent.github.io/blog/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2509.13305\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2509.13305",
    "external_url": "https://github.com/Alibaba-NLP/DeepResearch/",
    "published_date": "2025-09-16T13:57:03.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2509.13305",
      "upvotes": 83,
      "github_stars": 14776,
      "github_url": "https://github.com/Alibaba-NLP/DeepResearch/",
      "project_url": "https://tongyi-agent.github.io/blog/",
      "authors": [
        "Kuan Li",
        "Zhongwang Zhang",
        "Huifeng Yin",
        "Rui Ye",
        "Yida Zhao",
        "Liwen Zhang",
        "Litu Ou",
        "Dingchu Zhang",
        "Xixi Wu",
        "Jialong Wu",
        "Xinyu Wang",
        "Zile Qiao",
        "Zhen Zhang",
        "Yong Jiang",
        "Pengjun Xie",
        "Fei Huang",
        "Jingren Zhou"
      ],
      "summary": "Transcending human cognitive limitations represents a critical frontier in\nLLM training. Proprietary agentic systems like DeepResearch have demonstrated\nsuperhuman capabilities on extremely complex information-seeking benchmarks\nsuch as BrowseComp, a feat previously unattainable. We posit that their success\nhinges on a sophisticated reasoning pattern absent in open-source models: the\nability to systematically reduce extreme uncertainty when navigating vast\ninformation landscapes. Based on this insight, we introduce WebSailor, a\ncomplete post-training methodology designed to instill this crucial capability.\nOur approach involves generating novel, high-uncertainty tasks through\nstructured sampling and information obfuscation, RFT cold start, and an\nefficient agentic RL training algorithm, Duplicating Sampling Policy\nOptimization (DUPO). With this integrated pipeline, WebSailor significantly\noutperforms all open-source agents in complex information-seeking tasks,\nmatching proprietary agents' performance and closing the capability gap.",
      "fetch_date": "2025-09-17",
      "num_comments": 4,
      "ai_summary": "WebSailor, a post-training methodology, enhances open-source models with systematic uncertainty reduction, matching proprietary agents' performance in complex information-seeking tasks.",
      "ai_keywords": [
        "LLM training",
        "DeepResearch",
        "BrowseComp",
        "reasoning pattern",
        "high-uncertainty tasks",
        "structured sampling",
        "information obfuscation",
        "RFT cold start",
        "agentic RL training",
        "Duplicating Sampling Policy Optimization",
        "DUPO"
      ]
    }
  },
  {
    "id": "56ef9e3095cf3666002acddf23dee4ac",
    "source": "huggingface",
    "type": "paper",
    "title": "ReSum: Unlocking Long-Horizon Search Intelligence via Context\n  Summarization",
    "description": "Large Language Model (LLM)-based web agents demonstrate strong performance on\nknowledge-intensive tasks but are hindered by context window limitations in\nparadigms like ReAct. Complex queries involvin...<br/>Upvotes: 70<br/>GitHub Stars: 14776<br/>Authors: Xixi Wu, Kuan Li, Yida Zhao<br/>ðŸ”— <a href=\"https://github.com/Alibaba-NLP/DeepResearch//\">GitHub</a><br/>ðŸ”— <a href=\"https://tongyi-agent.github.io/blog/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2509.13313\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2509.13313",
    "external_url": "https://github.com/Alibaba-NLP/DeepResearch//",
    "published_date": "2025-09-16T13:57:22.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2509.13313",
      "upvotes": 70,
      "github_stars": 14776,
      "github_url": "https://github.com/Alibaba-NLP/DeepResearch//",
      "project_url": "https://tongyi-agent.github.io/blog/",
      "authors": [
        "Xixi Wu",
        "Kuan Li",
        "Yida Zhao",
        "Liwen Zhang",
        "Litu Ou",
        "Huifeng Yin",
        "Zhongwang Zhang",
        "Yong Jiang",
        "Pengjun Xie",
        "Fei Huang",
        "Minhao Cheng",
        "Shuai Wang",
        "Hong Cheng",
        "Jingren Zhou"
      ],
      "summary": "Large Language Model (LLM)-based web agents demonstrate strong performance on\nknowledge-intensive tasks but are hindered by context window limitations in\nparadigms like ReAct. Complex queries involving multiple entities, intertwined\nrelationships, and high uncertainty demand extensive search cycles that rapidly\nexhaust context budgets before reaching complete solutions. To overcome this\nchallenge, we introduce ReSum, a novel paradigm that enables indefinite\nexploration through periodic context summarization. ReSum converts growing\ninteraction histories into compact reasoning states, maintaining awareness of\nprior discoveries while bypassing context constraints. For paradigm adaptation,\nwe propose ReSum-GRPO, integrating GRPO with segmented trajectory training and\nadvantage broadcasting to familiarize agents with summary-conditioned\nreasoning. Extensive experiments on web agents of varying scales across three\nbenchmarks demonstrate that ReSum delivers an average absolute improvement of\n4.5\\% over ReAct, with further gains of up to 8.2\\% following ReSum-GRPO\ntraining. Notably, with only 1K training samples, our WebResummer-30B (a\nReSum-GRPO-trained version of WebSailor-30B) achieves 33.3\\% Pass@1 on\nBrowseComp-zh and 18.3\\% on BrowseComp-en, surpassing existing open-source web\nagents.",
      "fetch_date": "2025-09-17",
      "num_comments": 5,
      "ai_summary": "ReSum, a novel paradigm with periodic context summarization, enhances web agents' performance on knowledge-intensive tasks by overcoming context window limitations, achieving significant improvements over ReAct.",
      "ai_keywords": [
        "Large Language Model",
        "LLM",
        "web agents",
        "knowledge-intensive tasks",
        "context window limitations",
        "ReAct",
        "ReSum",
        "context summarization",
        "reasoning states",
        "GRPO",
        "segmented trajectory training",
        "advantage broadcasting",
        "ReSum-GRPO",
        "WebResummer-30B",
        "WebSailor-30B",
        "BrowseComp-zh",
        "BrowseComp-en",
        "Pass@1"
      ]
    }
  },
  {
    "id": "f5dd4977eb4232fa774b69a50071202a",
    "source": "huggingface",
    "type": "paper",
    "title": "UI-S1: Advancing GUI Automation via Semi-online Reinforcement Learning",
    "description": "Graphical User Interface (GUI) agents have demonstrated remarkable progress\nin automating complex user interface interactions through reinforcement\nlearning. However, current approaches face a fundame...<br/>Upvotes: 46<br/>GitHub Stars: 5907<br/>Authors: Zhengxi Lu, Jiabo Ye, Fei Tang<br/>ðŸ”— <a href=\"https://github.com/X-PLUG/MobileAgent/tree/main/UI-S1\">GitHub</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2509.11543\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2509.11543",
    "external_url": "https://github.com/X-PLUG/MobileAgent/tree/main/UI-S1",
    "published_date": "2025-09-14T23:24:08.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2509.11543",
      "upvotes": 46,
      "github_stars": 5907,
      "github_url": "https://github.com/X-PLUG/MobileAgent/tree/main/UI-S1",
      "project_url": "",
      "authors": [
        "Zhengxi Lu",
        "Jiabo Ye",
        "Fei Tang",
        "Yongliang Shen",
        "Haiyang Xu",
        "Ziwei Zheng",
        "Weiming Lu",
        "Ming Yan",
        "Fei Huang",
        "Jun Xiao",
        "Yueting Zhuang"
      ],
      "summary": "Graphical User Interface (GUI) agents have demonstrated remarkable progress\nin automating complex user interface interactions through reinforcement\nlearning. However, current approaches face a fundamental dilemma: offline RL\nenables stable training on pre-collected trajectories, but struggles with\nmulti-step task execution for lack of trajectory-level reward signals; online\nRL captures these signals through environment interaction, but suffers from\nsparse rewards and prohibitive deployment costs. To address it, we present\nSemi-online Reinforcement Learning, a novel paradigm that simulates online RL\non offline trajectories. During each rollout process, we preserve the original\nmodel output within the multi-turn dialogue, where a Patch Module adaptively\nrecovers the divergence between rollout and expert trajectories. To capture\nlong-term training signals, Semi-online RL introduces discounted future returns\ninto the reward computation and optimizes the policy with weighted step-level\nand episode-level advantages. We further introduce Semi-Online Performance\n(SOP), a metric that aligns better with true online performance, serving as a\npractical and effective proxy for real-world evaluation. Experiments show that\nours Semi-online RL achieves SOTA performance among 7B models across four\ndynamic benchmarks, with significant gains over the base model (e.g., +12.0% on\nAndroidWorld, +23.8% on AITW), demonstrating significant progress in bridging\nthe gap between offline training efficiency and online multi-turn reasoning.\nThe code is available at https://github.com/X-PLUG/MobileAgent/tree/main/UI-S1.",
      "fetch_date": "2025-09-16",
      "num_comments": 3,
      "ai_summary": "Semi-online Reinforcement Learning addresses the limitations of offline and online RL by simulating online RL on offline trajectories, achieving state-of-the-art performance in dynamic benchmarks.",
      "ai_keywords": [
        "reinforcement learning",
        "offline RL",
        "online RL",
        "multi-step task execution",
        "trajectory-level reward signals",
        "sparse rewards",
        "deployment costs",
        "Patch Module",
        "discounted future returns",
        "step-level advantages",
        "episode-level advantages",
        "Semi-Online Performance (SOP)",
        "dynamic benchmarks",
        "AndroidWorld",
        "AITW"
      ]
    }
  }
]