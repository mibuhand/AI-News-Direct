[
  {
    "id": "b5eadc2228eda785fe3e8da98ea25174",
    "source": "huggingface",
    "type": "paper",
    "title": "Agent Learning via Early Experience",
    "description": "A long-term goal of language agents is to learn and improve through their own\nexperience, ultimately outperforming humans in complex, real-world tasks.\nHowever, training agents from experience data wi...<br/>Upvotes: 260<br/>Authors: Kai Zhang, Xiangchao Chen, Bo Liu<br/>ðŸ”— <a href=\"https://huggingface.co/papers/2510.08558\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2510.08558",
    "external_url": "",
    "published_date": "2025-10-09T13:59:17.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2510.08558",
      "upvotes": 260,
      "github_stars": 0,
      "github_url": "",
      "project_url": "",
      "authors": [
        "Kai Zhang",
        "Xiangchao Chen",
        "Bo Liu",
        "Tianci Xue",
        "Zeyi Liao",
        "Zhihan Liu",
        "Xiyao Wang",
        "Yuting Ning",
        "Zhaorun Chen",
        "Xiaohan Fu",
        "Jian Xie",
        "Yuxuan Sun",
        "Boyu Gou",
        "Qi Qi",
        "Zihang Meng",
        "Jianwei Yang",
        "Ning Zhang",
        "Xian Li",
        "Ashish Shah",
        "Dat Huynh",
        "Hengduo Li",
        "Zi Yang",
        "Sara Cao",
        "Lawrence Jang",
        "Shuyan Zhou",
        "Jiacheng Zhu",
        "Huan Sun",
        "Jason Weston",
        "Yu Su",
        "Yifan Wu"
      ],
      "summary": "A long-term goal of language agents is to learn and improve through their own\nexperience, ultimately outperforming humans in complex, real-world tasks.\nHowever, training agents from experience data with reinforcement learning\nremains difficult in many environments, which either lack verifiable rewards\n(e.g., websites) or require inefficient long-horizon rollouts (e.g., multi-turn\ntool use). As a result, most current agents rely on supervised fine-tuning on\nexpert data, which is challenging to scale and generalizes poorly. This\nlimitation stems from the nature of expert demonstrations: they capture only a\nnarrow range of scenarios and expose the agent to limited environment\ndiversity. We address this limitation with a middle-ground paradigm we call\nearly experience: interaction data generated by the agent's own actions, where\nthe resulting future states serve as supervision without reward signals. Within\nthis paradigm we study two strategies of using such data: (1) Implicit world\nmodeling, which uses collected states to ground the policy in environment\ndynamics; and (2) Self-reflection, where the agent learns from its suboptimal\nactions to improve reasoning and decision-making. We evaluate across eight\ndiverse environments and multiple model families. Our approaches consistently\nimprove effectiveness and out-of-domain generalization, highlighting the value\nof early experience. Moreover, in environments with verifiable rewards, our\nresults provide promising signals that early experience offers a strong\nfoundation for subsequent reinforcement learning, positioning it as a practical\nbridge between imitation learning and fully experience-driven agents.",
      "fetch_date": "2025-10-10",
      "num_comments": 10,
      "ai_summary": "Early experience, using agent-generated interaction data without reward signals, improves policy effectiveness and generalization, serving as a bridge between imitation learning and reinforcement learning.",
      "ai_keywords": [
        "reinforcement learning",
        "early experience",
        "implicit world modeling",
        "self-reflection",
        "out-of-domain generalization"
      ]
    }
  },
  {
    "id": "8c7bb673a7c23605ce494554bc937b12",
    "source": "huggingface",
    "type": "paper",
    "title": "Scaling Latent Reasoning via Looped Language Models",
    "description": "Modern LLMs are trained to \"think\" primarily via explicit text generation,\nsuch as chain-of-thought (CoT), which defers reasoning to post-training and\nunder-leverages pre-training data. We present and...<br/>Upvotes: 202<br/>Authors: Rui-Jie Zhu, Zixuan Wang, Kai Hua<br/>ðŸ”— <a href=\"https://ouro-llm.github.io/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2510.25741\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2510.25741",
    "external_url": "",
    "published_date": "2025-10-29T13:45:42.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2510.25741",
      "upvotes": 202,
      "github_stars": 0,
      "github_url": "",
      "project_url": "https://ouro-llm.github.io/",
      "authors": [
        "Rui-Jie Zhu",
        "Zixuan Wang",
        "Kai Hua",
        "Tianyu Zhang",
        "Ziniu Li",
        "Haoran Que",
        "Boyi Wei",
        "Zixin Wen",
        "Fan Yin",
        "He Xing",
        "Lu Li",
        "Jiajun Shi",
        "Kaijing Ma",
        "Shanda Li",
        "Taylor Kergan",
        "Andrew Smith",
        "Xingwei Qu",
        "Mude Hui",
        "Bohong Wu",
        "Qiyang Min",
        "Hongzhi Huang",
        "Xun Zhou",
        "Wei Ye",
        "Jiaheng Liu",
        "Jian Yang",
        "Yunfeng Shi",
        "Chenghua Lin",
        "Enduo Zhao",
        "Tianle Cai",
        "Ge Zhang",
        "Wenhao Huang",
        "Yoshua Bengio",
        "Jason Eshraghian"
      ],
      "summary": "Modern LLMs are trained to \"think\" primarily via explicit text generation,\nsuch as chain-of-thought (CoT), which defers reasoning to post-training and\nunder-leverages pre-training data. We present and open-source Ouro, named after\nthe recursive Ouroboros, a family of pre-trained Looped Language Models\n(LoopLM) that instead build reasoning into the pre-training phase through (i)\niterative computation in latent space, (ii) an entropy-regularized objective\nfor learned depth allocation, and (iii) scaling to 7.7T tokens. Ouro 1.4B and\n2.6B models enjoy superior performance that match the results of up to 12B SOTA\nLLMs across a wide range of benchmarks. Through controlled experiments, we show\nthis advantage stems not from increased knowledge capacity, but from superior\nknowledge manipulation capabilities. We also show that LoopLM yields reasoning\ntraces more aligned with final outputs than explicit CoT. We hope our results\nshow the potential of LoopLM as a novel scaling direction in the reasoning era.\nOur model could be found in: http://ouro-llm.github.io.",
      "fetch_date": "2025-10-30",
      "num_comments": 3,
      "ai_summary": "LoopLM, a family of pre-trained Looped Language Models, enhances reasoning by integrating iterative computation and entropy regularization during pre-training, achieving superior performance with better knowledge manipulation.",
      "ai_keywords": [
        "Looped Language Models",
        "LoopLM",
        "iterative computation",
        "latent space",
        "entropy-regularized objective",
        "learned depth allocation",
        "chain-of-thought",
        "knowledge manipulation",
        "reasoning traces"
      ]
    }
  },
  {
    "id": "f5ecd82d2d6a90e1b4eb7c0d2afe9c3e",
    "source": "huggingface",
    "type": "paper",
    "title": "QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning\n  for LLMs",
    "description": "We propose QeRL, a Quantization-enhanced Reinforcement Learning framework for\nlarge language models (LLMs). While RL is essential for LLMs' reasoning\ncapabilities, it is resource-intensive, requiring ...<br/>Upvotes: 173<br/>GitHub Stars: 419<br/>Authors: Wei Huang, Yi Ge, Shuai Yang<br/>ðŸ”— <a href=\"https://github.com/NVlabs/QeRL\">GitHub</a><br/>ðŸ”— <a href=\"https://github.com/NVlabs/QeRL\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2510.11696\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2510.11696",
    "external_url": "https://github.com/NVlabs/QeRL",
    "published_date": "2025-10-13T13:55:09.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2510.11696",
      "upvotes": 173,
      "github_stars": 419,
      "github_url": "https://github.com/NVlabs/QeRL",
      "project_url": "https://github.com/NVlabs/QeRL",
      "authors": [
        "Wei Huang",
        "Yi Ge",
        "Shuai Yang",
        "Yicheng Xiao",
        "Huizi Mao",
        "Yujun Lin",
        "Hanrong Ye",
        "Sifei Liu",
        "Ka Chun Cheung",
        "Hongxu Yin",
        "Yao Lu",
        "Xiaojuan Qi",
        "Song Han",
        "Yukang Chen"
      ],
      "summary": "We propose QeRL, a Quantization-enhanced Reinforcement Learning framework for\nlarge language models (LLMs). While RL is essential for LLMs' reasoning\ncapabilities, it is resource-intensive, requiring substantial GPU memory and\nlong rollout durations. QeRL addresses these issues by combining NVFP4\nquantization with Low-Rank Adaptation (LoRA), accelerating rollout phase of RL\nwhile reducing memory overhead. Beyond efficiency, our findings show that\nquantization noise increases policy entropy, enhancing exploration, and\nenabling the discovery of better strategies during RL. To further optimize\nexploration, QeRL introduces an Adaptive Quantization Noise (AQN) mechanism,\nwhich dynamically adjusts noise during training. Experiments demonstrate that\nQeRL delivers over 1.5 times speedup in the rollout phase. Moreover, this is\nthe first framework to enable RL training of a 32B LLM on a single H100 80GB\nGPU, while delivering overall speedups for RL training. It also achieves faster\nreward growth and higher final accuracy than 16-bit LoRA and QLoRA, while\nmatching the performance of full-parameter fine-tuning on mathematical\nbenchmarks such as GSM8K (90.8%) and MATH 500 (77.4%) in the 7B model. These\nresults establish QeRL as an efficient and effective framework for RL training\nin LLMs.",
      "fetch_date": "2025-10-14",
      "num_comments": 4,
      "ai_summary": "QeRL, a quantization-enhanced reinforcement learning framework, accelerates RL training for large language models by combining NVFP4 quantization with Low-Rank Adaptation and an Adaptive Quantization Noise mechanism, achieving significant speedups and improved performance.",
      "ai_keywords": [
        "NVFP4 quantization",
        "Low-Rank Adaptation (LoRA)",
        "Adaptive Quantization Noise (AQN)",
        "reinforcement learning",
        "large language models (LLMs)",
        "rollout phase",
        "policy entropy",
        "exploration",
        "reward growth",
        "GSM8K",
        "MATH 500"
      ]
    }
  },
  {
    "id": "9f97533076b36bd588d78f78c4352369",
    "source": "huggingface",
    "type": "paper",
    "title": "Concerto: Joint 2D-3D Self-Supervised Learning Emerges Spatial\n  Representations",
    "description": "Humans learn abstract concepts through multisensory synergy, and once formed,\nsuch representations can often be recalled from a single modality. Inspired by\nthis principle, we introduce Concerto, a mi...<br/>Upvotes: 172<br/>GitHub Stars: 377<br/>Authors: Yujia Zhang, Xiaoyang Wu, Yixing Lao<br/>ðŸ”— <a href=\"https://github.com/Pointcept/Concerto\">GitHub</a><br/>ðŸ”— <a href=\"https://pointcept.github.io/Concerto/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2510.23607\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2510.23607",
    "external_url": "https://github.com/Pointcept/Concerto",
    "published_date": "2025-10-27T13:59:59.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2510.23607",
      "upvotes": 172,
      "github_stars": 377,
      "github_url": "https://github.com/Pointcept/Concerto",
      "project_url": "https://pointcept.github.io/Concerto/",
      "authors": [
        "Yujia Zhang",
        "Xiaoyang Wu",
        "Yixing Lao",
        "Chengyao Wang",
        "Zhuotao Tian",
        "Naiyan Wang",
        "Hengshuang Zhao"
      ],
      "summary": "Humans learn abstract concepts through multisensory synergy, and once formed,\nsuch representations can often be recalled from a single modality. Inspired by\nthis principle, we introduce Concerto, a minimalist simulation of human concept\nlearning for spatial cognition, combining 3D intra-modal self-distillation with\n2D-3D cross-modal joint embedding. Despite its simplicity, Concerto learns more\ncoherent and informative spatial features, as demonstrated by zero-shot\nvisualizations. It outperforms both standalone SOTA 2D and 3D self-supervised\nmodels by 14.2% and 4.8%, respectively, as well as their feature concatenation,\nin linear probing for 3D scene perception. With full fine-tuning, Concerto sets\nnew SOTA results across multiple scene understanding benchmarks (e.g., 80.7%\nmIoU on ScanNet). We further present a variant of Concerto tailored for\nvideo-lifted point cloud spatial understanding, and a translator that linearly\nprojects Concerto representations into CLIP's language space, enabling\nopen-world perception. These results highlight that Concerto emerges spatial\nrepresentations with superior fine-grained geometric and semantic consistency.",
      "fetch_date": "2025-10-28",
      "num_comments": 4,
      "ai_summary": "Concerto, a minimalist model combining 3D self-distillation and 2D-3D joint embedding, achieves superior spatial feature learning and outperforms existing models in scene understanding and open-world perception.",
      "ai_keywords": [
        "3D intra-modal self-distillation",
        "2D-3D cross-modal joint embedding",
        "zero-shot visualizations",
        "linear probing",
        "3D scene perception",
        "ScanNet",
        "mIoU",
        "video-lifted point cloud",
        "CLIP's language space",
        "open-world perception",
        "fine-grained geometric and semantic consistency"
      ]
    }
  },
  {
    "id": "1f97c06c73ceb6a2aa9ee2bb5f344034",
    "source": "huggingface",
    "type": "paper",
    "title": "Diffusion Transformers with Representation Autoencoders",
    "description": "Latent generative modeling, where a pretrained autoencoder maps pixels into a\nlatent space for the diffusion process, has become the standard strategy for\nDiffusion Transformers (DiT); however, the au...<br/>Upvotes: 160<br/>GitHub Stars: 1498<br/>Authors: Boyang Zheng, Nanye Ma, Shengbang Tong<br/>ðŸ”— <a href=\"https://github.com/bytetriper/RAE\">GitHub</a><br/>ðŸ”— <a href=\"https://rae-dit.github.io/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2510.11690\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2510.11690",
    "external_url": "https://github.com/bytetriper/RAE",
    "published_date": "2025-10-13T13:51:39.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2510.11690",
      "upvotes": 160,
      "github_stars": 1498,
      "github_url": "https://github.com/bytetriper/RAE",
      "project_url": "https://rae-dit.github.io/",
      "authors": [
        "Boyang Zheng",
        "Nanye Ma",
        "Shengbang Tong",
        "Saining Xie"
      ],
      "summary": "Latent generative modeling, where a pretrained autoencoder maps pixels into a\nlatent space for the diffusion process, has become the standard strategy for\nDiffusion Transformers (DiT); however, the autoencoder component has barely\nevolved. Most DiTs continue to rely on the original VAE encoder, which\nintroduces several limitations: outdated backbones that compromise\narchitectural simplicity, low-dimensional latent spaces that restrict\ninformation capacity, and weak representations that result from purely\nreconstruction-based training and ultimately limit generative quality. In this\nwork, we explore replacing the VAE with pretrained representation encoders\n(e.g., DINO, SigLIP, MAE) paired with trained decoders, forming what we term\nRepresentation Autoencoders (RAEs). These models provide both high-quality\nreconstructions and semantically rich latent spaces, while allowing for a\nscalable transformer-based architecture. Since these latent spaces are\ntypically high-dimensional, a key challenge is enabling diffusion transformers\nto operate effectively within them. We analyze the sources of this difficulty,\npropose theoretically motivated solutions, and validate them empirically. Our\napproach achieves faster convergence without auxiliary representation alignment\nlosses. Using a DiT variant equipped with a lightweight, wide DDT head, we\nachieve strong image generation results on ImageNet: 1.51 FID at 256x256 (no\nguidance) and 1.13 at both 256x256 and 512x512 (with guidance). RAE offers\nclear advantages and should be the new default for diffusion transformer\ntraining.",
      "fetch_date": "2025-10-14",
      "num_comments": 5,
      "ai_summary": "Replacing VAEs with pretrained representation encoders in Diffusion Transformers enhances generative quality and convergence speed without auxiliary losses.",
      "ai_keywords": [
        "Latent generative modeling",
        "Diffusion Transformers (DiT)",
        "VAE encoder",
        "Representation Autoencoders (RAEs)",
        "pretrained representation encoders",
        "DINO",
        "SigLIP",
        "MAE",
        "high-dimensional latent spaces",
        "transformer-based architecture",
        "diffusion transformers",
        "image generation",
        "ImageNet",
        "FID"
      ]
    }
  },
  {
    "id": "b81e62a9e2bd28a9fe78c760c27308b6",
    "source": "huggingface",
    "type": "paper",
    "title": "Thinking with Video: Video Generation as a Promising Multimodal\n  Reasoning Paradigm",
    "description": "\"Thinking with Text\" and \"Thinking with Images\" paradigm significantly\nimprove the reasoning ability of large language models (LLMs) and Vision\nLanguage Models (VLMs). However, these paradigms have in...<br/>Upvotes: 145<br/>GitHub Stars: 107<br/>Authors: Jingqi Tong, Yurong Mou, Hangcheng Li<br/>ðŸ”— <a href=\"https://github.com/tongjingqi/Thinking-with-Video\">GitHub</a><br/>ðŸ”— <a href=\"https://thinking-with-video.github.io/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2511.04570\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2511.04570",
    "external_url": "https://github.com/tongjingqi/Thinking-with-Video",
    "published_date": "2025-11-06T12:25:23.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2511.04570",
      "upvotes": 145,
      "github_stars": 107,
      "github_url": "https://github.com/tongjingqi/Thinking-with-Video",
      "project_url": "https://thinking-with-video.github.io/",
      "authors": [
        "Jingqi Tong",
        "Yurong Mou",
        "Hangcheng Li",
        "Mingzhe Li",
        "Yongzhuo Yang",
        "Ming Zhang",
        "Qiguang Chen",
        "Tianyi Liang",
        "Xiaomeng Hu",
        "Yining Zheng",
        "Xinchi Chen",
        "Jun Zhao",
        "Xuanjing Huang",
        "Xipeng Qiu"
      ],
      "summary": "\"Thinking with Text\" and \"Thinking with Images\" paradigm significantly\nimprove the reasoning ability of large language models (LLMs) and Vision\nLanguage Models (VLMs). However, these paradigms have inherent limitations. (1)\nImages capture only single moments and fail to represent dynamic processes or\ncontinuous changes, and (2) The separation of text and vision as distinct\nmodalities, hindering unified multimodal understanding and generation. To\novercome these limitations, we introduce \"Thinking with Video\", a new paradigm\nthat leverages video generation models, such as Sora-2, to bridge visual and\ntextual reasoning in a unified temporal framework. To support this exploration,\nwe developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBench\nencompasses two task categories: (1) vision-centric tasks (e.g., Eyeballing\nPuzzles), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Our\nevaluation establishes Sora-2 as a capable reasoner. On vision-centric tasks,\nSora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even\nsurpasses VLMs on several tasks, such as Eyeballing Games. On text-centric\ntasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU.\nFurthermore, we systematically analyse the source of these abilities. We also\nfind that self-consistency and in-context learning can improve Sora-2's\nperformance. In summary, our findings demonstrate that the video generation\nmodel is the potential unified multimodal understanding and generation model,\npositions \"thinking with video\" as a unified multimodal reasoning paradigm.",
      "fetch_date": "2025-11-07",
      "num_comments": 2,
      "ai_summary": "The \"Thinking with Video\" paradigm enhances multimodal reasoning by integrating video generation models, demonstrated through the Video Thinking Benchmark and improved performance on both vision and text tasks.",
      "ai_keywords": [
        "Thinking with Text",
        "Thinking with Images",
        "large language models",
        "Vision Language Models",
        "Thinking with Video",
        "video generation models",
        "Video Thinking Benchmark",
        "vision-centric tasks",
        "text-centric tasks",
        "Eyeballing Puzzles",
        "GSM8K",
        "MMMU",
        "self-consistency",
        "in-context learning"
      ]
    }
  },
  {
    "id": "d98bf11f4930fd1159bb5f90e3f76afd",
    "source": "huggingface",
    "type": "paper",
    "title": "PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B\n  Ultra-Compact Vision-Language Model",
    "description": "In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model\ntailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a\ncompact yet powerful vision-language model (VLM) ...<br/>Upvotes: 85<br/>GitHub Stars: 63021<br/>Authors: Cheng Cui, Ting Sun, Suyin Liang<br/>ðŸ”— <a href=\"https://github.com/PaddlePaddle/PaddleOCR\">GitHub</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2510.14528\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2510.14528",
    "external_url": "https://github.com/PaddlePaddle/PaddleOCR",
    "published_date": "2025-10-16T06:18:48.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2510.14528",
      "upvotes": 85,
      "github_stars": 63021,
      "github_url": "https://github.com/PaddlePaddle/PaddleOCR",
      "project_url": "",
      "authors": [
        "Cheng Cui",
        "Ting Sun",
        "Suyin Liang",
        "Tingquan Gao",
        "Zelun Zhang",
        "Jiaxuan Liu",
        "Xueqing Wang",
        "Changda Zhou",
        "Hongen Liu",
        "Manhui Lin",
        "Yue Zhang",
        "Yubo Zhang",
        "Handong Zheng",
        "Jing Zhang",
        "Jun Zhang",
        "Yi Liu",
        "Dianhai Yu",
        "Yanjun Ma"
      ],
      "summary": "In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model\ntailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a\ncompact yet powerful vision-language model (VLM) that integrates a NaViT-style\ndynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to\nenable accurate element recognition. This innovative model efficiently supports\n109 languages and excels in recognizing complex elements (e.g., text, tables,\nformulas, and charts), while maintaining minimal resource consumption. Through\ncomprehensive evaluations on widely used public benchmarks and in-house\nbenchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document\nparsing and element-level recognition. It significantly outperforms existing\nsolutions, exhibits strong competitiveness against top-tier VLMs, and delivers\nfast inference speeds. These strengths make it highly suitable for practical\ndeployment in real-world scenarios.",
      "fetch_date": "2025-10-17",
      "num_comments": 5,
      "ai_summary": "PaddleOCR-VL, a vision-language model combining NaViT-style visual encoder and ERNIE-4.5 language model, achieves state-of-the-art performance in document parsing with minimal resource consumption.",
      "ai_keywords": [
        "vision-language model",
        "NaViT-style",
        "dynamic resolution visual encoder",
        "ERNIE-4.5",
        "element recognition",
        "page-level document parsing",
        "element-level recognition",
        "inference speeds"
      ]
    }
  },
  {
    "id": "4477d6ceb0f6af4c535f87fb4c3e55f5",
    "source": "huggingface",
    "type": "paper",
    "title": "BitNet Distillation",
    "description": "In this paper, we present BitNet Distillation (BitDistill), a lightweight\npipeline that fine-tunes off-the-shelf full-precision LLMs (e.g., Qwen) into\n1.58-bit precision (i.e., ternary weights {-1, 0,...<br/>Upvotes: 52<br/>GitHub Stars: 24363<br/>Authors: Xun Wu, Shaohan Huang, Wenhui Wang<br/>ðŸ”— <a href=\"https://github.com/microsoft/BitNet\">GitHub</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2510.13998\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2510.13998",
    "external_url": "https://github.com/microsoft/BitNet",
    "published_date": "2025-10-15T14:28:12.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2510.13998",
      "upvotes": 52,
      "github_stars": 24363,
      "github_url": "https://github.com/microsoft/BitNet",
      "project_url": "",
      "authors": [
        "Xun Wu",
        "Shaohan Huang",
        "Wenhui Wang",
        "Ting Song",
        "Li Dong",
        "Yan Xia",
        "Furu Wei"
      ],
      "summary": "In this paper, we present BitNet Distillation (BitDistill), a lightweight\npipeline that fine-tunes off-the-shelf full-precision LLMs (e.g., Qwen) into\n1.58-bit precision (i.e., ternary weights {-1, 0, 1}) for specific downstream\ntasks, achieving strong task-specific performance with minimal computational\ncost. Specifically, BitDistill incorporates three key techniques: the SubLN\nmodule, as introduced in BitNet; multi-head attention distillation, based on\nMiniLM; and continual pre-training, which serves as a crucial warm-up step to\nmitigate the scalability issue of the performance gap between finetuned\nfull-precision and 1.58-bit LLMs on specific tasks. Experimental results show\nthat BitDistill achieves performance comparable to the full-precision\ncounterpart models across model size, while enabling up to 10x memory savings\nand 2.65x faster inference on CPUs. Code is available at\nhttps://github.com/microsoft/BitNet.",
      "fetch_date": "2025-10-17",
      "num_comments": 4,
      "ai_summary": "BitNet Distillation fine-tunes large language models to 1.58-bit precision using SubLN, multi-head attention distillation, and continual pre-training, achieving comparable performance with significant memory and inference speed improvements.",
      "ai_keywords": [
        "BitNet Distillation",
        "BitDistill",
        "SubLN",
        "multi-head attention distillation",
        "continual pre-training",
        "LLMs",
        "Qwen",
        "ternary weights",
        "memory savings",
        "inference speed"
      ]
    }
  },
  {
    "id": "bf88f955951ceb1c5ba0121481fe4bf5",
    "source": "huggingface",
    "type": "paper",
    "title": "FAPO: Flawed-Aware Policy Optimization for Efficient and Reliable\n  Reasoning",
    "description": "Reinforcement learning with verifiable rewards (RLVR) has emerged as a\npromising paradigm for enhancing the reasoning capabilities of large language\nmodels (LLMs). In this context, models explore reas...<br/>Upvotes: 6<br/>GitHub Stars: 15235<br/>Authors: Yuyang Ding, Chi Zhang, Juntao Li<br/>ðŸ”— <a href=\"https://github.com/volcengine/verl/tree/main/recipe/fapo\">GitHub</a><br/>ðŸ”— <a href=\"https://fapo-rl.github.io/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2510.22543\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2510.22543",
    "external_url": "https://github.com/volcengine/verl/tree/main/recipe/fapo",
    "published_date": "2025-10-26T01:49:38.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2510.22543",
      "upvotes": 6,
      "github_stars": 15235,
      "github_url": "https://github.com/volcengine/verl/tree/main/recipe/fapo",
      "project_url": "https://fapo-rl.github.io/",
      "authors": [
        "Yuyang Ding",
        "Chi Zhang",
        "Juntao Li",
        "Haibin Lin",
        "Xin Liu",
        "Min Zhang"
      ],
      "summary": "Reinforcement learning with verifiable rewards (RLVR) has emerged as a\npromising paradigm for enhancing the reasoning capabilities of large language\nmodels (LLMs). In this context, models explore reasoning trajectories and\nexploit rollouts with correct answers as positive signals for policy\noptimization. However, these rollouts might involve flawed patterns such as\nanswer-guessing and jump-in-reasoning. Such flawed-positive rollouts are\nrewarded identically to fully correct ones, causing policy models to\ninternalize these unreliable reasoning patterns. In this work, we first conduct\na systematic study of flawed-positive rollouts in RL and find that they enable\nrapid capability gains during the early optimization stage, while constraining\nreasoning capability later by reinforcing unreliable patterns. Building on\nthese insights, we propose Flawed-Aware Policy Optimization (FAPO), which\npresents a parameter-free reward penalty for flawed-positive rollouts, enabling\nthe policy to leverage them as useful shortcuts in the warm-up stage, securing\nstable early gains, while gradually shifting optimization toward reliable\nreasoning in the later refinement stage. To accurately and comprehensively\ndetect flawed-positive rollouts, we introduce a generative reward model (GenRM)\nwith a process-level reward that precisely localizes reasoning errors.\nExperiments show that FAPO is effective in broad domains, improving outcome\ncorrectness, process reliability, and training stability without increasing the\ntoken budget.",
      "fetch_date": "2025-10-30",
      "num_comments": 1,
      "ai_summary": "Flawed-Aware Policy Optimization (FAPO) enhances reinforcement learning with verifiable rewards by penalizing flawed-positive rollouts, improving reasoning capability and training stability in large language models.",
      "ai_keywords": [
        "reinforcement learning",
        "verifiable rewards",
        "large language models",
        "reasoning trajectories",
        "policy optimization",
        "flawed-positive rollouts",
        "parameter-free reward penalty",
        "generative reward model",
        "process-level reward",
        "reasoning errors"
      ]
    }
  },
  {
    "id": "2431e39c5ab567b865fe3f25b69b4d35",
    "source": "huggingface",
    "type": "paper",
    "title": "RAG-Anything: All-in-One RAG Framework",
    "description": "Retrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm\nfor expanding Large Language Models beyond their static training limitations.\nHowever, a critical misalignment exists between...<br/>Upvotes: 47<br/>GitHub Stars: 10043<br/>Authors: Zirui Guo, Xubin Ren, Lingrui Xu<br/>ðŸ”— <a href=\"https://github.com/HKUDS/RAG-Anything\">GitHub</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2510.12323\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2510.12323",
    "external_url": "https://github.com/HKUDS/RAG-Anything",
    "published_date": "2025-10-14T05:25:35.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2510.12323",
      "upvotes": 47,
      "github_stars": 10043,
      "github_url": "https://github.com/HKUDS/RAG-Anything",
      "project_url": "",
      "authors": [
        "Zirui Guo",
        "Xubin Ren",
        "Lingrui Xu",
        "Jiahao Zhang",
        "Chao Huang"
      ],
      "summary": "Retrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm\nfor expanding Large Language Models beyond their static training limitations.\nHowever, a critical misalignment exists between current RAG capabilities and\nreal-world information environments. Modern knowledge repositories are\ninherently multimodal, containing rich combinations of textual content, visual\nelements, structured tables, and mathematical expressions. Yet existing RAG\nframeworks are limited to textual content, creating fundamental gaps when\nprocessing multimodal documents. We present RAG-Anything, a unified framework\nthat enables comprehensive knowledge retrieval across all modalities. Our\napproach reconceptualizes multimodal content as interconnected knowledge\nentities rather than isolated data types. The framework introduces dual-graph\nconstruction to capture both cross-modal relationships and textual semantics\nwithin a unified representation. We develop cross-modal hybrid retrieval that\ncombines structural knowledge navigation with semantic matching. This enables\neffective reasoning over heterogeneous content where relevant evidence spans\nmultiple modalities. RAG-Anything demonstrates superior performance on\nchallenging multimodal benchmarks, achieving significant improvements over\nstate-of-the-art methods. Performance gains become particularly pronounced on\nlong documents where traditional approaches fail. Our framework establishes a\nnew paradigm for multimodal knowledge access, eliminating the architectural\nfragmentation that constrains current systems. Our framework is open-sourced\nat: https://github.com/HKUDS/RAG-Anything.",
      "fetch_date": "2025-10-15",
      "num_comments": 5,
      "ai_summary": "RAG-Anything is a unified framework that enhances multimodal knowledge retrieval by integrating cross-modal relationships and semantic matching, outperforming existing methods on complex benchmarks.",
      "ai_keywords": [
        "Retrieval-Augmented Generation",
        "RAG",
        "Large Language Models",
        "multimodal",
        "textual content",
        "visual elements",
        "structured tables",
        "mathematical expressions",
        "dual-graph construction",
        "cross-modal hybrid retrieval",
        "structural knowledge navigation",
        "semantic matching",
        "multimodal benchmarks",
        "long documents"
      ]
    }
  },
  {
    "id": "32811c5aee11a368e90161a3c41bd70b",
    "source": "huggingface",
    "type": "paper",
    "title": "Chronos-2: From Univariate to Universal Forecasting",
    "description": "Pretrained time series models have enabled inference-only forecasting systems\nthat produce accurate predictions without task-specific training. However,\nexisting approaches largely focus on univariate...<br/>Upvotes: 17<br/>GitHub Stars: 4229<br/>Authors: Abdul Fatir Ansari, Oleksandr Shchur, Jaris KÃ¼ken<br/>ðŸ”— <a href=\"https://github.com/amazon-science/chronos-forecasting\">GitHub</a><br/>ðŸ”— <a href=\"https://huggingface.co/amazon/chronos-2\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2510.15821\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2510.15821",
    "external_url": "https://github.com/amazon-science/chronos-forecasting",
    "published_date": "2025-10-17T13:00:53.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2510.15821",
      "upvotes": 17,
      "github_stars": 4229,
      "github_url": "https://github.com/amazon-science/chronos-forecasting",
      "project_url": "https://huggingface.co/amazon/chronos-2",
      "authors": [
        "Abdul Fatir Ansari",
        "Oleksandr Shchur",
        "Jaris KÃ¼ken",
        "Andreas Auer",
        "Boran Han",
        "Pedro Mercado",
        "Syama Sundar Rangapuram",
        "Huibin Shen",
        "Lorenzo Stella",
        "Xiyuan Zhang",
        "Mononito Goswami",
        "Shubham Kapoor",
        "Danielle C. Maddix",
        "Pablo Guerron",
        "Tony Hu",
        "Junming Yin",
        "Nick Erickson",
        "Prateek Mutalik Desai",
        "Hao Wang",
        "Huzefa Rangwala",
        "George Karypis",
        "Yuyang Wang",
        "Michael Bohlke-Schneider"
      ],
      "summary": "Pretrained time series models have enabled inference-only forecasting systems\nthat produce accurate predictions without task-specific training. However,\nexisting approaches largely focus on univariate forecasting, limiting their\napplicability in real-world scenarios where multivariate data and covariates\nplay a crucial role. We present Chronos-2, a pretrained model capable of\nhandling univariate, multivariate, and covariate-informed forecasting tasks in\na zero-shot manner. Chronos-2 employs a group attention mechanism that\nfacilitates in-context learning (ICL) through efficient information sharing\nacross multiple time series within a group, which may represent sets of related\nseries, variates of a multivariate series, or targets and covariates in a\nforecasting task. These general capabilities are achieved through training on\nsynthetic datasets that impose diverse multivariate structures on univariate\nseries. Chronos-2 delivers state-of-the-art performance across three\ncomprehensive benchmarks: fev-bench, GIFT-Eval, and Chronos Benchmark II. On\nfev-bench, which emphasizes multivariate and covariate-informed forecasting,\nChronos-2's universal ICL capabilities lead to substantial improvements over\nexisting models. On tasks involving covariates, it consistently outperforms\nbaselines by a wide margin. Case studies in the energy and retail domains\nfurther highlight its practical advantages. The in-context learning\ncapabilities of Chronos-2 establish it as a general-purpose forecasting model\nthat can be used \"as is\" in real-world forecasting pipelines.",
      "fetch_date": "2025-10-21",
      "num_comments": 3,
      "ai_summary": "Chronos-2, a pretrained model with a group attention mechanism, achieves state-of-the-art performance in zero-shot univariate, multivariate, and covariate-informed forecasting tasks.",
      "ai_keywords": [
        "pretrained model",
        "group attention mechanism",
        "in-context learning",
        "ICL",
        "synthetic datasets",
        "multivariate structures",
        "fev-bench",
        "GIFT-Eval",
        "Chronos Benchmark II",
        "energy domain",
        "retail domain"
      ]
    }
  },
  {
    "id": "a88f84734a51625761ecb06d3d2f8410",
    "source": "huggingface",
    "type": "paper",
    "title": "Training-Free Group Relative Policy Optimization",
    "description": "Recent advances in Large Language Model (LLM) agents have demonstrated their\npromising general capabilities. However, their performance in specialized\nreal-world domains often degrades due to challeng...<br/>Upvotes: 44<br/>GitHub Stars: 3773<br/>Authors: Yuzheng Cai, Siqi Cai, Yuchen Shi<br/>ðŸ”— <a href=\"https://github.com/TencentCloudADP/youtu-agent/tree/training_free_GRPO\">GitHub</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2510.08191\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2510.08191",
    "external_url": "https://github.com/TencentCloudADP/youtu-agent/tree/training_free_GRPO",
    "published_date": "2025-10-09T09:18:17.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2510.08191",
      "upvotes": 44,
      "github_stars": 3773,
      "github_url": "https://github.com/TencentCloudADP/youtu-agent/tree/training_free_GRPO",
      "project_url": "",
      "authors": [
        "Yuzheng Cai",
        "Siqi Cai",
        "Yuchen Shi",
        "Zihan Xu",
        "Lichao Chen",
        "Yulei Qin",
        "Xiaoyu Tan",
        "Gang Li",
        "Zongyi Li",
        "Haojia Lin",
        "Yong Mao",
        "Ke Li",
        "Xing Sun"
      ],
      "summary": "Recent advances in Large Language Model (LLM) agents have demonstrated their\npromising general capabilities. However, their performance in specialized\nreal-world domains often degrades due to challenges in effectively integrating\nexternal tools and specific prompting strategies. While methods like agentic\nreinforcement learning have been proposed to address this, they typically rely\non costly parameter updates, for example, through a process that uses\nSupervised Fine-Tuning (SFT) followed by a Reinforcement Learning (RL) phase\nwith Group Relative Policy Optimization (GRPO) to alter the output\ndistribution. However, we argue that LLMs can achieve a similar effect on the\noutput distribution by learning experiential knowledge as a token prior, which\nis a far more lightweight approach that not only addresses practical data\nscarcity but also avoids the common issue of overfitting. To this end, we\npropose Training-Free Group Relative Policy Optimization (Training-Free GRPO),\na cost-effective solution that enhances LLM agent performance without any\nparameter updates. Our method leverages the group relative semantic advantage\ninstead of numerical ones within each group of rollouts, iteratively distilling\nhigh-quality experiential knowledge during multi-epoch learning on a minimal\nground-truth data. Such knowledge serves as the learned token prior, which is\nseamlessly integrated during LLM API calls to guide model behavior. Experiments\non mathematical reasoning and web searching tasks demonstrate that\nTraining-Free GRPO, when applied to DeepSeek-V3.1-Terminus, significantly\nimproves out-of-domain performance. With just a few dozen training samples,\nTraining-Free GRPO outperforms fine-tuned small LLMs with marginal training\ndata and cost.",
      "fetch_date": "2025-10-10",
      "num_comments": 2,
      "ai_summary": "Training-Free GRPO enhances LLM agent performance in specialized domains by learning experiential knowledge as a token prior without parameter updates, improving out-of-domain tasks with minimal data.",
      "ai_keywords": [
        "Large Language Model (LLM)",
        "agentic reinforcement learning",
        "Supervised Fine-Tuning (SFT)",
        "Reinforcement Learning (RL)",
        "Group Relative Policy Optimization (GRPO)",
        "token prior",
        "Training-Free GRPO",
        "group relative semantic advantage",
        "multi-epoch learning",
        "minimal ground-truth data",
        "DeepSeek-V3.1-Terminus",
        "mathematical reasoning",
        "web searching tasks"
      ]
    }
  }
]