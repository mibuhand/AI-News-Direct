[
  {
    "id": "7e43a061a51737f2587e45e69aa58ff1",
    "source": "huggingface",
    "type": "paper",
    "title": "From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence",
    "description": "Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adop...<br/>Upvotes: 264<br/>Authors: Jian Yang, Xianglong Liu, Weifeng Lv<br/>ðŸ”— <a href=\"https://huggingface.co/papers/2511.18538\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2511.18538",
    "external_url": "",
    "published_date": "2025-11-23T12:09:34.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2511.18538",
      "upvotes": 264,
      "github_stars": 0,
      "github_url": "",
      "project_url": "",
      "authors": [
        "Jian Yang",
        "Xianglong Liu",
        "Weifeng Lv",
        "Ken Deng",
        "Shawn Guo",
        "Lin Jing",
        "Yizhi Li",
        "Shark Liu",
        "Xianzhen Luo",
        "Yuyu Luo",
        "Changzai Pan",
        "Ensheng Shi",
        "Yingshui Tan",
        "Renshuai Tao",
        "Jiajun Wu",
        "Xianjie Wu",
        "Zhenhe Wu",
        "Daoguang Zan",
        "Chenchen Zhang",
        "Wei Zhang",
        "He Zhu",
        "Terry Yue Zhuo",
        "Kerui Cao",
        "Xianfu Cheng",
        "Jun Dong",
        "Shengjie Fang",
        "Zhiwei Fei",
        "Xiangyuan Guan",
        "Qipeng Guo",
        "Zhiguang Han",
        "Joseph James",
        "Tianqi Luo",
        "Renyuan Li",
        "Yuhang Li",
        "Yiming Liang",
        "Congnan Liu",
        "Jiaheng Liu",
        "Qian Liu",
        "Ruitong Liu",
        "Tyler Loakman",
        "Xiangxin Meng",
        "Chuang Peng",
        "Tianhao Peng",
        "Jiajun Shi",
        "Mingjie Tang",
        "Boyang Wang",
        "Haowen Wang",
        "Yunli Wang",
        "Fanglin Xu",
        "Zihan Xu",
        "Fei Yuan",
        "Ge Zhang",
        "Jiayi Zhang",
        "Xinhao Zhang",
        "Wangchunshu Zhou",
        "Hualei Zhu",
        "King Zhu",
        "Brown Dai",
        "Aishan Liu",
        "Zhoujun Li",
        "Chenghua Lin",
        "Tianyu Liu",
        "Chao Peng",
        "Kai Shen",
        "Libo Qin",
        "Shuangyong Song",
        "Zizheng Zhan",
        "Jiajun Zhang",
        "Jie Zhang",
        "Zhaoxiang Zhang",
        "Bo Zheng"
      ],
      "summary": "Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adoption through tools like Github Copilot (Microsoft), Cursor (Anysphere), Trae (ByteDance), and Claude Code (Anthropic). While the field has evolved dramatically from rule-based systems to Transformer-based architectures, achieving performance improvements from single-digit to over 95\\% success rates on benchmarks like HumanEval. In this work, we provide a comprehensive synthesis and practical guide (a series of analytic and probing experiments) about code LLMs, systematically examining the complete model life cycle from data curation to post-training through advanced prompting paradigms, code pre-training, supervised fine-tuning, reinforcement learning, and autonomous coding agents. We analyze the code capability of the general LLMs (GPT-4, Claude, LLaMA) and code-specialized LLMs (StarCoder, Code LLaMA, DeepSeek-Coder, and QwenCoder), critically examining the techniques, design decisions, and trade-offs. Further, we articulate the research-practice gap between academic research (e.g., benchmarks and tasks) and real-world deployment (e.g., software-related code tasks), including code correctness, security, contextual awareness of large codebases, and integration with development workflows, and map promising research directions to practical needs. Last, we conduct a series of experiments to provide a comprehensive analysis of code pre-training, supervised fine-tuning, and reinforcement learning, covering scaling law, framework selection, hyperparameter sensitivity, model architectures, and dataset comparisons.",
      "fetch_date": "2025-12-02",
      "num_comments": 13,
      "ai_summary": "A comprehensive guide to code LLMs, covering their lifecycle from data curation to deployment, including techniques, trade-offs, and research-practice gaps.",
      "ai_keywords": [
        "Transformer-based architectures",
        "HumanEval",
        "prompting paradigms",
        "code pre-training",
        "supervised fine-tuning",
        "reinforcement learning",
        "autonomous coding agents",
        "GPT-4",
        "Claude",
        "LLaMA",
        "StarCoder",
        "Code LLaMA",
        "DeepSeek-Coder",
        "QwenCoder",
        "code correctness",
        "security",
        "contextual awareness",
        "software-related code tasks",
        "scaling law",
        "framework selection",
        "hyperparameter sensitivity",
        "model architectures",
        "dataset comparisons"
      ]
    }
  },
  {
    "id": "0139ab6619114102f84bcc9c449fd176",
    "source": "huggingface",
    "type": "paper",
    "title": "Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation",
    "description": "This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kand...<br/>Upvotes: 224<br/>GitHub Stars: 575<br/>Authors: Vladimir Arkhipkin, Vladimir Korviakov, Nikolai Gerasimenko<br/>ðŸ”— <a href=\"https://github.com/kandinskylab/kandinsky-5\">GitHub</a><br/>ðŸ”— <a href=\"https://kandinskylab.ai/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2511.14993\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2511.14993",
    "external_url": "https://github.com/kandinskylab/kandinsky-5",
    "published_date": "2025-11-18T19:23:22.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2511.14993",
      "upvotes": 224,
      "github_stars": 575,
      "github_url": "https://github.com/kandinskylab/kandinsky-5",
      "project_url": "https://kandinskylab.ai/",
      "authors": [
        "Vladimir Arkhipkin",
        "Vladimir Korviakov",
        "Nikolai Gerasimenko",
        "Denis Parkhomenko",
        "Viacheslav Vasilev",
        "Alexey Letunovskiy",
        "Maria Kovaleva",
        "Nikolai Vaulin",
        "Ivan Kirillov",
        "Lev Novitskiy",
        "Denis Koposov",
        "Nikita Kiselev",
        "Alexander Varlamov",
        "Dmitrii Mikhailov",
        "Vladimir Polovnikov",
        "Andrey Shutkin",
        "Ilya Vasiliev",
        "Julia Agafonova",
        "Anastasiia Kargapoltseva",
        "Anna Dmitrienko",
        "Anastasia Maltseva",
        "Anna Averchenkova",
        "Olga Kim",
        "Tatiana Nikulina",
        "Denis Dimitrov"
      ],
      "summary": "This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kandinsky 5.0 Image Lite - a line-up of 6B parameter image generation models, Kandinsky 5.0 Video Lite - a fast and lightweight 2B parameter text-to-video and image-to-video models, and Kandinsky 5.0 Video Pro - 19B parameter models that achieves superior video generation quality. We provide a comprehensive review of the data curation lifecycle - including collection, processing, filtering and clustering - for the multi-stage training pipeline that involves extensive pre-training and incorporates quality-enhancement techniques such as self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. We also present novel architectural, training, and inference optimizations that enable Kandinsky 5.0 to achieve high generation speeds and state-of-the-art performance across various tasks, as demonstrated by human evaluation. As a large-scale, publicly available generative framework, Kandinsky 5.0 leverages the full potential of its pre-training and subsequent stages to be adapted for a wide range of generative applications. We hope that this report, together with the release of our open-source code and training checkpoints, will substantially advance the development and accessibility of high-quality generative models for the research community.",
      "fetch_date": "2025-11-20",
      "num_comments": 5,
      "ai_summary": "Kandinsky 5.0 is a family of state-of-the-art generative models for high-resolution images and short videos, featuring model lineups with varying parameters and enhanced training techniques to achieve superior quality and performance.",
      "ai_keywords": [
        "foundation models",
        "high-resolution image synthesis",
        "10-second video synthesis",
        "image generation models",
        "text-to-video models",
        "image-to-video models",
        "multi-stage training pipeline",
        "self-supervised fine-tuning",
        "reinforcement learning",
        "pre-training",
        "quality-enhancement techniques",
        "architectural optimizations",
        "training optimizations",
        "inference optimizations",
        "human evaluation",
        "generative framework",
        "open-source code",
        "training checkpoints"
      ]
    }
  },
  {
    "id": "c360b805a86862ca56d4316aea6a684d",
    "source": "huggingface",
    "type": "paper",
    "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models",
    "description": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) De...<br/>Upvotes: 213<br/>Authors: DeepSeek-AI, Aixin Liu, Aoxue Mei<br/>ðŸ”— <a href=\"https://huggingface.co/papers/2512.02556\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2512.02556",
    "external_url": "",
    "published_date": "2025-12-02T04:25:14.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2512.02556",
      "upvotes": 213,
      "github_stars": 0,
      "github_url": "",
      "project_url": "",
      "authors": [
        "DeepSeek-AI",
        "Aixin Liu",
        "Aoxue Mei",
        "Bangcai Lin",
        "Bing Xue",
        "Bingxuan Wang",
        "Bingzheng Xu",
        "Bochao Wu",
        "Bowei Zhang",
        "Chaofan Lin",
        "Chen Dong",
        "Chengda Lu",
        "Chenggang Zhao",
        "Chengqi Deng",
        "Chenhao Xu",
        "Chong Ruan",
        "Damai Dai",
        "Daya Guo",
        "Dejian Yang",
        "Deli Chen",
        "Erhang Li",
        "Fangqi Zhou",
        "Fangyun Lin",
        "Fucong Dai",
        "Guangbo Hao",
        "Guanting Chen",
        "Guowei Li",
        "H. Zhang",
        "Hanwei Xu",
        "Hao Li",
        "Haofen Liang",
        "Haoran Wei",
        "Haowei Zhang",
        "Haowen Luo",
        "Haozhe Ji",
        "Honghui Ding",
        "Hongxuan Tang",
        "Huanqi Cao",
        "Huazuo Gao",
        "Hui Qu",
        "Hui Zeng",
        "Jialiang Huang",
        "Jiashi Li",
        "Jiaxin Xu",
        "Jiewen Hu",
        "Jingchang Chen",
        "Jingting Xiang",
        "Jingyang Yuan",
        "Jingyuan Cheng",
        "Jinhua Zhu",
        "Jun Ran",
        "Junguang Jiang",
        "Junjie Qiu",
        "Junlong Li",
        "Junxiao Song",
        "Kai Dong",
        "Kaige Gao",
        "Kang Guan",
        "Kexin Huang",
        "Kexing Zhou",
        "Kezhao Huang",
        "Kuai Yu",
        "Lean Wang",
        "Lecong Zhang",
        "Lei Wang",
        "Liang Zhao",
        "Liangsheng Yin",
        "Lihua Guo",
        "Lingxiao Luo",
        "Linwang Ma",
        "Litong Wang",
        "Liyue Zhang",
        "M. S. Di",
        "M. Y Xu",
        "Mingchuan Zhang",
        "Minghua Zhang",
        "Minghui Tang",
        "Mingxu Zhou",
        "Panpan Huang",
        "Peixin Cong",
        "Peiyi Wang",
        "Qiancheng Wang",
        "Qihao Zhu",
        "Qingyang Li",
        "Qinyu Chen",
        "Qiushi Du",
        "Ruiling Xu",
        "Ruiqi Ge",
        "Ruisong Zhang",
        "Ruizhe Pan",
        "Runji Wang",
        "Runqiu Yin",
        "Runxin Xu",
        "Ruomeng Shen",
        "Ruoyu Zhang",
        "S. H. Liu",
        "Shanghao Lu",
        "Shangyan Zhou",
        "Shanhuang Chen",
        "Shaofei Cai",
        "Shaoyuan Chen",
        "Shengding Hu",
        "Shengyu Liu",
        "Shiqiang Hu",
        "Shirong Ma",
        "Shiyu Wang",
        "Shuiping Yu",
        "Shunfeng Zhou",
        "Shuting Pan",
        "Songyang Zhou",
        "Tao Ni",
        "Tao Yun",
        "Tian Pei",
        "Tian Ye",
        "Tianyuan Yue",
        "Wangding Zeng",
        "Wen Liu",
        "Wenfeng Liang",
        "Wenjie Pang",
        "Wenjing Luo",
        "Wenjun Gao",
        "Wentao Zhang",
        "Xi Gao",
        "Xiangwen Wang",
        "Xiao Bi",
        "Xiaodong Liu",
        "Xiaohan Wang",
        "Xiaokang Chen",
        "Xiaokang Zhang",
        "Xiaotao Nie",
        "Xin Cheng",
        "Xin Liu",
        "Xin Xie",
        "Xingchao Liu",
        "Xingkai Yu",
        "Xingyou Li",
        "Xinyu Yang",
        "Xinyuan Li",
        "Xu Chen",
        "Xuecheng Su",
        "Xuehai Pan",
        "Xuheng Lin",
        "Xuwei Fu",
        "Y. Q. Wang",
        "Yang Zhang",
        "Yanhong Xu",
        "Yanru Ma",
        "Yao Li",
        "Yao Li",
        "Yao Zhao",
        "Yaofeng Sun",
        "Yaohui Wang",
        "Yi Qian",
        "Yi Yu",
        "Yichao Zhang",
        "Yifan Ding",
        "Yifan Shi",
        "Yiliang Xiong",
        "Ying He",
        "Ying Zhou",
        "Yinmin Zhong",
        "Yishi Piao",
        "Yisong Wang",
        "Yixiao Chen",
        "Yixuan Tan",
        "Yixuan Wei",
        "Yiyang Ma",
        "Yiyuan Liu",
        "Yonglun Yang",
        "Yongqiang Guo",
        "Yongtong Wu",
        "Yu Wu",
        "Yuan Cheng",
        "Yuan Ou",
        "Yuanfan Xu",
        "Yuduan Wang",
        "Yue Gong",
        "Yuhan Wu",
        "Yuheng Zou",
        "Yukun Li",
        "Yunfan Xiong",
        "Yuxiang Luo",
        "Yuxiang You",
        "Yuxuan Liu",
        "Yuyang Zhou",
        "Z. F. Wu",
        "Z. Z. Ren",
        "Zehua Zhao",
        "Zehui Ren",
        "Zhangli Sha",
        "Zhe Fu",
        "Zhean Xu",
        "Zhenda Xie",
        "Zhengyan Zhang",
        "Zhewen Hao",
        "Zhibin Gou",
        "Zhicheng Ma",
        "Zhigang Yan",
        "Zhihong Shao",
        "Zhixian Huang",
        "Zhiyu Wu",
        "Zhuoshu Li",
        "Zhuping Zhang",
        "Zian Xu",
        "Zihao Wang",
        "Zihui Gu",
        "Zijia Zhu",
        "Zilin Li",
        "Zipeng Zhang",
        "Ziwei Xie",
        "Ziyi Gao",
        "Zizheng Pan",
        "Zongqing Yao",
        "Bei Feng",
        "Hui Li",
        "J. L. Cai",
        "Jiaqi Ni",
        "Lei Xu",
        "Meng Li",
        "Ning Tian",
        "R. J. Chen",
        "R. L. Jin",
        "S. S. Li",
        "Shuang Zhou",
        "Tianyu Sun",
        "X. Q. Li",
        "Xiangyue Jin",
        "Xiaojin Shen",
        "Xiaosha Chen",
        "Xinnan Song",
        "Xinyi Zhou",
        "Y. X. Zhu",
        "Yanping Huang",
        "Yaohui Li",
        "Yi Zheng",
        "Yuchen Zhu",
        "Yunxian Ma",
        "Zhen Huang",
        "Zhipeng Xu",
        "Zhongyu Zhang",
        "Dongjie Ji",
        "Jian Liang",
        "Jianzhong Guo",
        "Jin Chen",
        "Leyi Xia",
        "Miaojun Wang",
        "Mingming Li",
        "Peng Zhang",
        "Ruyi Chen",
        "Shangmian Sun",
        "Shaoqing Wu",
        "Shengfeng Ye",
        "T. Wang",
        "W. L. Xiao",
        "Wei An",
        "Xianzu Wang",
        "Xiaowen Sun",
        "Xiaoxiang Wang",
        "Ying Tang",
        "Yukun Zha",
        "Zekai Zhang",
        "Zhe Ju",
        "Zhen Zhang",
        "Zihua Qu"
      ],
      "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.",
      "fetch_date": "2025-12-03",
      "num_comments": 5,
      "ai_summary": "DeepSeek-V3.2 introduces DeepSeek Sparse Attention and a scalable reinforcement learning framework, achieving superior reasoning and performance compared to GPT-5 and Gemini-3.0-Pro in complex reasoning tasks.",
      "ai_keywords": [
        "DeepSeek Sparse Attention",
        "DSA",
        "reinforcement learning framework",
        "agentic task synthesis pipeline",
        "computational efficiency",
        "long-context scenarios",
        "gold-medal performance",
        "International Mathematical Olympiad",
        "International Olympiad in Informatics",
        "reasoning proficiency"
      ]
    }
  },
  {
    "id": "8e9429d7feaa43a78c484db81c3eddf8",
    "source": "huggingface",
    "type": "paper",
    "title": "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer",
    "description": "The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Imag...<br/>Upvotes: 199<br/>GitHub Stars: 7160<br/>Authors: Z-Image Team, Huanqia Cai, Sihan Cao<br/>ðŸ”— <a href=\"https://github.com/Tongyi-MAI/Z-Image\">GitHub</a><br/>ðŸ”— <a href=\"https://tongyi-mai.github.io/Z-Image-blog/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2511.22699\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2511.22699",
    "external_url": "https://github.com/Tongyi-MAI/Z-Image",
    "published_date": "2025-11-27T13:52:07.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2511.22699",
      "upvotes": 199,
      "github_stars": 7160,
      "github_url": "https://github.com/Tongyi-MAI/Z-Image",
      "project_url": "https://tongyi-mai.github.io/Z-Image-blog/",
      "authors": [
        "Z-Image Team",
        "Huanqia Cai",
        "Sihan Cao",
        "Ruoyi Du",
        "Peng Gao",
        "Steven Hoi",
        "Shijie Huang",
        "Zhaohui Hou",
        "Dengyang Jiang",
        "Xin Jin",
        "Liangchen Li",
        "Zhen Li",
        "Zhong-Yu Li",
        "David Liu",
        "Dongyang Liu",
        "Junhan Shi",
        "Qilong Wu",
        "Feng Yu",
        "Chi Zhang",
        "Shifeng Zhang",
        "Shilin Zhou"
      ],
      "summary": "The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the \"scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.",
      "fetch_date": "2025-12-01",
      "num_comments": 4,
      "ai_summary": "Z-Image, a 6B-parameter Scalable Single-Stream Diffusion Transformer (S3-DiT) model, achieves high-performance image generation with reduced computational cost, offering sub-second inference and compatibility with consumer hardware.",
      "ai_keywords": [
        "Scalable Single-Stream Diffusion Transformer",
        "S3-DiT",
        "diffusion transformer",
        "omni-pre-training",
        "instruction-following capabilities",
        "photorealistic image generation",
        "bilingual text rendering",
        "distillation scheme",
        "reward post-training",
        "H800 GPU",
        "VRAM"
      ]
    }
  },
  {
    "id": "bdd6e066d5b32b9c61996386576f3682",
    "source": "huggingface",
    "type": "paper",
    "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length",
    "description": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audi...<br/>Upvotes: 166<br/>GitHub Stars: 1036<br/>Authors: Yubo Huang, Hailong Guo, Fangtai Wu<br/>ðŸ”— <a href=\"https://github.com/Alibaba-Quark/LiveAvatar\">GitHub</a><br/>ðŸ”— <a href=\"https://liveavatar.github.io/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2512.04677\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2512.04677",
    "external_url": "https://github.com/Alibaba-Quark/LiveAvatar",
    "published_date": "2025-12-04T06:11:24.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2512.04677",
      "upvotes": 166,
      "github_stars": 1036,
      "github_url": "https://github.com/Alibaba-Quark/LiveAvatar",
      "project_url": "https://liveavatar.github.io/",
      "authors": [
        "Yubo Huang",
        "Hailong Guo",
        "Fangtai Wu",
        "Shifeng Zhang",
        "Shijie Huang",
        "Qijun Gan",
        "Lin Liu",
        "Sirui Zhao",
        "Enhong Chen",
        "Jiaming Liu",
        "Steven Hoi"
      ],
      "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.",
      "fetch_date": "2025-12-05",
      "num_comments": 4,
      "ai_summary": "Live Avatar uses a 14-billion-parameter diffusion model with Timestep-forcing Pipeline Parallelism and Rolling Sink Frame Mechanism to achieve real-time, high-fidelity avatar generation.",
      "ai_keywords": [
        "diffusion-based video generation",
        "sequential computation",
        "long-horizon inconsistency",
        "real-time",
        "streaming audio-driven avatar synthesis",
        "Timestep-forcing Pipeline Parallelism",
        "distributed inference paradigm",
        "pipeline parallelism",
        "denoising steps",
        "autoregressive bottleneck",
        "low-latency real-time streaming",
        "Rolling Sink Frame Mechanism",
        "sequence fidelity",
        "Self-Forcing Distribution Matching Distillation",
        "causal",
        "streamable adaptation",
        "visual quality",
        "end-to-end generation",
        "H800 GPUs"
      ]
    }
  },
  {
    "id": "0a60cb0f535685cc7eb12b1b01de62bc",
    "source": "huggingface",
    "type": "paper",
    "title": "MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling",
    "description": "We present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or co...<br/>Upvotes: 161<br/>GitHub Stars: 1319<br/>Authors: MiroMind Team, Song Bai, Lidong Bing<br/>ðŸ”— <a href=\"https://github.com/MiroMindAI/MiroThinker\">GitHub</a><br/>ðŸ”— <a href=\"https://dr.miromind.ai/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2511.11793\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2511.11793",
    "external_url": "https://github.com/MiroMindAI/MiroThinker",
    "published_date": "2025-11-14T13:52:07.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2511.11793",
      "upvotes": 161,
      "github_stars": 1319,
      "github_url": "https://github.com/MiroMindAI/MiroThinker",
      "project_url": "https://dr.miromind.ai/",
      "authors": [
        "MiroMind Team",
        "Song Bai",
        "Lidong Bing",
        "Carson Chen",
        "Guanzheng Chen",
        "Yuntao Chen",
        "Zhe Chen",
        "Ziyi Chen",
        "Jifeng Dai",
        "Xuan Dong",
        "Yue Deng",
        "Yunjie Fu",
        "Junqi Ge",
        "Chenxia Han",
        "Tammy Huang",
        "Zhenhang Huang",
        "Jerry Jiao",
        "Shilei Jiang",
        "Tianyu Jiao",
        "Xiaoqi Jian",
        "Lei Lei",
        "Ruilin Li",
        "Ryan Luo",
        "Tiantong Li",
        "Xiang Lin",
        "Ziyuan Liu",
        "Zhiqi Li",
        "Jie Ni",
        "Qiang Ren",
        "Pax Sun",
        "Shiqian Su",
        "Chenxin Tao",
        "Bin Wang",
        "Hellen Wang",
        "Haonan Wang",
        "James Wang",
        "Jin Wang",
        "Jojo Wang",
        "Letian Wang",
        "Shizun Wang",
        "Weizhi Wang",
        "Zixuan Wang",
        "Jinfan Xu",
        "Sen Xing",
        "Chenyu Yang",
        "Hai Ye",
        "Jiaheng Yu",
        "Yue Yu",
        "Muyan Zhong",
        "Tianchen Zhao",
        "Xizhou Zhu",
        "Yanpeng Zhou",
        "Yifan Zhang",
        "Zhi Zhu"
      ],
      "summary": "We present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or context length, MiroThinker explores interaction scaling at the model level, systematically training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement. Unlike LLM test-time scaling, which operates in isolation and risks degradation with longer reasoning chains, interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories. Through reinforcement learning, the model achieves efficient interaction scaling: with a 256K context window, it can perform up to 600 tool calls per task, enabling sustained multi-turn reasoning and complex real-world research workflows. Across four representative benchmarks-GAIA, HLE, BrowseComp, and BrowseComp-ZH-the 72B variant achieves up to 81.9%, 37.7%, 47.1%, and 55.6% accuracy respectively, surpassing previous open-source agents and approaching commercial counterparts such as GPT-5-high. Our analysis reveals that MiroThinker benefits from interactive scaling consistently: research performance improves predictably as the model engages in deeper and more frequent agent-environment interactions, demonstrating that interaction depth exhibits scaling behaviors analogous to model size and context length. These findings establish interaction scaling as a third critical dimension for building next-generation open research agents, complementing model capacity and context windows.",
      "fetch_date": "2025-11-18",
      "num_comments": 4,
      "ai_summary": "",
      "ai_keywords": []
    }
  },
  {
    "id": "5b6ff13ede65701e5f764a0267eca8df",
    "source": "huggingface",
    "type": "paper",
    "title": "Qwen3-VL Technical Report",
    "description": "We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks. It natively supports interleaved...<br/>Upvotes: 126<br/>GitHub Stars: 17221<br/>Authors: Shuai Bai, Yuxuan Cai, Ruizhe Chen<br/>ðŸ”— <a href=\"https://github.com/QwenLM/Qwen3-VL\">GitHub</a><br/>ðŸ”— <a href=\"https://chat.qwen.ai\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2511.21631\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2511.21631",
    "external_url": "https://github.com/QwenLM/Qwen3-VL",
    "published_date": "2025-11-26T12:59:08.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2511.21631",
      "upvotes": 126,
      "github_stars": 17221,
      "github_url": "https://github.com/QwenLM/Qwen3-VL",
      "project_url": "https://chat.qwen.ai",
      "authors": [
        "Shuai Bai",
        "Yuxuan Cai",
        "Ruizhe Chen",
        "Keqin Chen",
        "Xionghui Chen",
        "Zesen Cheng",
        "Lianghao Deng",
        "Wei Ding",
        "Chang Gao",
        "Chunjiang Ge",
        "Wenbin Ge",
        "Zhifang Guo",
        "Qidong Huang",
        "Jie Huang",
        "Fei Huang",
        "Binyuan Hui",
        "Shutong Jiang",
        "Zhaohai Li",
        "Mingsheng Li",
        "Mei Li",
        "Kaixin Li",
        "Zicheng Lin",
        "Junyang Lin",
        "Xuejing Liu",
        "Jiawei Liu",
        "Chenglong Liu",
        "Yang Liu",
        "Dayiheng Liu",
        "Shixuan Liu",
        "Dunjie Lu",
        "Ruilin Luo",
        "Chenxu Lv",
        "Rui Men",
        "Lingchen Meng",
        "Xuancheng Ren",
        "Xingzhang Ren",
        "Sibo Song",
        "Yuchong Sun",
        "Jun Tang",
        "Jianhong Tu",
        "Jianqiang Wan",
        "Peng Wang",
        "Pengfei Wang",
        "Qiuyue Wang",
        "Yuxuan Wang",
        "Tianbao Xie",
        "Yiheng Xu",
        "Haiyang Xu",
        "Jin Xu",
        "Zhibo Yang",
        "Mingkun Yang",
        "Jianxin Yang",
        "An Yang",
        "Bowen Yu",
        "Fei Zhang",
        "Hang Zhang",
        "Xi Zhang",
        "Bo Zheng",
        "Humen Zhong",
        "Jingren Zhou",
        "Fan Zhou",
        "Jing Zhou",
        "Yuanzhi Zhu",
        "Ke Zhu"
      ],
      "summary": "We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks. It natively supports interleaved contexts of up to 256K tokens, seamlessly integrating text, images, and video. The model family includes both dense (2B/4B/8B/32B) and mixture-of-experts (30B-A3B/235B-A22B) variants to accommodate diverse latency-quality trade-offs. Qwen3-VL delivers three core pillars: (i) markedly stronger pure-text understanding, surpassing comparable text-only backbones in several cases; (ii) robust long-context comprehension with a native 256K-token window for both text and interleaved multimodal inputs, enabling faithful retention, retrieval, and cross-referencing across long documents and videos; and (iii) advanced multimodal reasoning across single-image, multi-image, and video tasks, demonstrating leading performance on comprehensive evaluations such as MMMU and visual-math benchmarks (e.g., MathVista and MathVision). Architecturally, we introduce three key upgrades: (i) an enhanced interleaved-MRoPE for stronger spatial-temporal modeling across images and video; (ii) DeepStack integration, which effectively leverages multi-level ViT features to tighten vision-language alignment; and (iii) text-based time alignment for video, evolving from T-RoPE to explicit textual timestamp alignment for more precise temporal grounding. Under comparable token budgets and latency constraints, Qwen3-VL achieves superior performance in both dense and Mixture-of-Experts (MoE) architectures. We envision Qwen3-VL serving as a foundational engine for image-grounded reasoning, agentic decision-making, and multimodal code intelligence in real-world workflows.",
      "fetch_date": "2025-12-04",
      "num_comments": 3,
      "ai_summary": "Qwen3-VL, a vision-language model, excels in text and multimodal understanding through advanced architectures and larger contexts, achieving superior performance across benchmarks.",
      "ai_keywords": [
        "vision-language model",
        "interleaved contexts",
        "multimodal benchmarks",
        "dense variants",
        "mixture-of-experts",
        "pure-text understanding",
        "long-context comprehension",
        "multimodal reasoning",
        "MMMU",
        "visual-math benchmarks",
        "interleaved-MRoPE",
        "DeepStack",
        "text-based time alignment",
        "T-RoPE",
        "explicit textual timestamp alignment",
        "vision-language alignment",
        "image-grounded reasoning",
        "agentic decision-making",
        "multimodal code intelligence"
      ]
    }
  },
  {
    "id": "84e0c9ab3b32b3eab139bdd727fe4f8c",
    "source": "huggingface",
    "type": "paper",
    "title": "DeepCode: Open Agentic Coding",
    "description": "Recent advances in large language models (LLMs) have given rise to powerful coding agents, making it possible for code assistants to evolve into code engineers. However, existing methods still face si...<br/>Upvotes: 27<br/>GitHub Stars: 12619<br/>Authors: Zongwei Li, Zhonghang Li, Zirui Guo<br/>ðŸ”— <a href=\"https://github.com/HKUDS/DeepCode\">GitHub</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2512.07921\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2512.07921",
    "external_url": "https://github.com/HKUDS/DeepCode",
    "published_date": "2025-12-08T11:07:13.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2512.07921",
      "upvotes": 27,
      "github_stars": 12619,
      "github_url": "https://github.com/HKUDS/DeepCode",
      "project_url": "",
      "authors": [
        "Zongwei Li",
        "Zhonghang Li",
        "Zirui Guo",
        "Xubin Ren",
        "Chao Huang"
      ],
      "summary": "Recent advances in large language models (LLMs) have given rise to powerful coding agents, making it possible for code assistants to evolve into code engineers. However, existing methods still face significant challenges in achieving high-fidelity document-to-codebase synthesis--such as scientific papers to code--primarily due to a fundamental conflict between information overload and the context bottlenecks of LLMs. In this work, we introduce DeepCode, a fully autonomous framework that fundamentally addresses this challenge through principled information-flow management. By treating repository synthesis as a channel optimization problem, DeepCode seamlessly orchestrates four information operations to maximize task-relevant signals under finite context budgets: source compression via blueprint distillation, structured indexing using stateful code memory, conditional knowledge injection via retrieval-augmented generation, and closed-loop error correction. Extensive evaluations on the PaperBench benchmark demonstrate that DeepCode achieves state-of-the-art performance, decisively outperforming leading commercial agents such as Cursor and Claude Code, and crucially, surpassing PhD-level human experts from top institutes on key reproduction metrics. By systematically transforming paper specifications into production-grade implementations comparable to human expert quality, this work establishes new foundations for autonomous scientific reproduction that can accelerate research evaluation and discovery.",
      "fetch_date": "2025-12-10",
      "num_comments": 2,
      "ai_summary": "DeepCode, a fully autonomous framework, addresses the challenges of document-to-codebase synthesis by optimizing information flow through source compression, structured indexing, knowledge injection, and error correction, achieving state-of-the-art performance and surpassing human experts.",
      "ai_keywords": [
        "large language models",
        "coding agents",
        "document-to-codebase synthesis",
        "information overload",
        "context bottlenecks",
        "DeepCode",
        "channel optimization",
        "blueprint distillation",
        "stateful code memory",
        "retrieval-augmented generation",
        "closed-loop error correction",
        "PaperBench",
        "autonomous scientific reproduction"
      ]
    }
  },
  {
    "id": "98c3fb1b14f6bd881158862a97b49d56",
    "source": "huggingface",
    "type": "paper",
    "title": "Agent READMEs: An Empirical Study of Context Files for Agentic Coding",
    "description": "Agentic coding tools receive goals written in natural language as input, break them down into specific tasks, and write or execute the actual code with minimal human intervention. Central to this proc...<br/>Upvotes: 7<br/>GitHub Stars: 12166<br/>Authors: Worawalan Chatlatanagulchai, Hao Li, Yutaro Kashiwa<br/>ðŸ”— <a href=\"https://github.com/openai/agents.md\">GitHub</a><br/>ðŸ”— <a href=\"https://agents.md\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2511.12884\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2511.12884",
    "external_url": "https://github.com/openai/agents.md",
    "published_date": "2025-11-16T21:18:55.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2511.12884",
      "upvotes": 7,
      "github_stars": 12166,
      "github_url": "https://github.com/openai/agents.md",
      "project_url": "https://agents.md",
      "authors": [
        "Worawalan Chatlatanagulchai",
        "Hao Li",
        "Yutaro Kashiwa",
        "Brittany Reid",
        "Kundjanasith Thonglek",
        "Pattara Leelaprute",
        "Arnon Rungsawang",
        "Bundit Manaskasemsak",
        "Bram Adams",
        "Ahmed E. Hassan",
        "Hajimu Iida"
      ],
      "summary": "Agentic coding tools receive goals written in natural language as input, break them down into specific tasks, and write or execute the actual code with minimal human intervention. Central to this process are agent context files (\"READMEs for agents\") that provide persistent, project-level instructions. In this paper, we conduct the first large-scale empirical study of 2,303 agent context files from 1,925 repositories to characterize their structure, maintenance, and content. We find that these files are not static documentation but complex, difficult-to-read artifacts that evolve like configuration code, maintained through frequent, small additions. Our content analysis of 16 instruction types shows that developers prioritize functional context, such as build and run commands (62.3%), implementation details (69.9%), and architecture (67.7%). We also identify a significant gap: non-functional requirements like security (14.5%) and performance (14.5%) are rarely specified. These findings indicate that while developers use context files to make agents functional, they provide few guardrails to ensure that agent-written code is secure or performant, highlighting the need for improved tooling and practices.",
      "fetch_date": "2025-11-19",
      "num_comments": 2,
      "ai_summary": "",
      "ai_keywords": []
    }
  },
  {
    "id": "4a2bfe8d09c1cb8d7f0252ad4b16ac1e",
    "source": "huggingface",
    "type": "paper",
    "title": "Soft Adaptive Policy Optimization",
    "description": "Reinforcement learning (RL) plays an increasingly important role in enhancing the reasoning capabilities of large language models (LLMs), yet stable and performant policy optimization remains challeng...<br/>Upvotes: 40<br/>GitHub Stars: 11695<br/>Authors: Chang Gao, Chujie Zheng, Xiong-Hui Chen<br/>ðŸ”— <a href=\"https://github.com/modelscope/ms-swift\">GitHub</a><br/>ðŸ”— <a href=\"https://swift.readthedocs.io/en/latest/Instruction/GRPO/AdvancedResearch/SAPO.html\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2511.20347\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2511.20347",
    "external_url": "https://github.com/modelscope/ms-swift",
    "published_date": "2025-11-25T09:25:19.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2511.20347",
      "upvotes": 40,
      "github_stars": 11695,
      "github_url": "https://github.com/modelscope/ms-swift",
      "project_url": "https://swift.readthedocs.io/en/latest/Instruction/GRPO/AdvancedResearch/SAPO.html",
      "authors": [
        "Chang Gao",
        "Chujie Zheng",
        "Xiong-Hui Chen",
        "Kai Dang",
        "Shixuan Liu",
        "Bowen Yu",
        "An Yang",
        "Shuai Bai",
        "Jingren Zhou",
        "Junyang Lin"
      ],
      "summary": "Reinforcement learning (RL) plays an increasingly important role in enhancing the reasoning capabilities of large language models (LLMs), yet stable and performant policy optimization remains challenging. Token-level importance ratios often exhibit high variance-a phenomenon exacerbated in Mixture-of-Experts models-leading to unstable updates. Existing group-based policy optimization methods, such as GSPO and GRPO, alleviate this problem via hard clipping, making it difficult to maintain both stability and effective learning. We propose Soft Adaptive Policy Optimization (SAPO), which replaces hard clipping with a smooth, temperature-controlled gate that adaptively attenuates off-policy updates while preserving useful learning signals. Compared with GSPO and GRPO, SAPO is both sequence-coherent and token-adaptive. Like GSPO, SAPO maintains sequence-level coherence, but its soft gating forms a continuous trust region that avoids the brittle hard clipping band used in GSPO. When a sequence contains a few highly off-policy tokens, GSPO suppresses all gradients for that sequence, whereas SAPO selectively down-weights only the offending tokens and preserves the learning signal from the near-on-policy ones, improving sample efficiency. Relative to GRPO, SAPO replaces hard token-level clipping with smooth, temperature-controlled scaling, enabling more informative and stable updates. Empirical results on mathematical reasoning benchmarks indicate that SAPO exhibits improved training stability and higher Pass@1 performance under comparable training budgets. Moreover, we employ SAPO to train the Qwen3-VL model series, demonstrating that SAPO yields consistent performance gains across diverse tasks and different model sizes. Overall, SAPO provides a more reliable, scalable, and effective optimization strategy for RL training of LLMs.",
      "fetch_date": "2025-11-26",
      "num_comments": 6,
      "ai_summary": "Soft Adaptive Policy Optimization (SAPO) enhances the stability and performance of reinforcement learning in large language models by adaptively attenuating off-policy updates with a smooth, temperature-controlled gate, leading to improved training stability and performance.",
      "ai_keywords": [
        "reinforcement learning",
        "large language models",
        "policy optimization",
        "token-level importance ratios",
        "Mixture-of-Experts models",
        "GSPO",
        "GRPO",
        "Soft Adaptive Policy Optimization",
        "sequence-coherent",
        "token-adaptive",
        "sequence-level coherence",
        "off-policy updates",
        "sample efficiency",
        "Pass@1 performance",
        "Qwen3-VL model series"
      ]
    }
  },
  {
    "id": "f37a636c7cc5f515c6866d60bda27643",
    "source": "huggingface",
    "type": "paper",
    "title": "UFO^3: Weaving the Digital Agent Galaxy",
    "description": "Large language model (LLM)-powered agents are transforming digital devices from passive tools into proactive intelligent collaborators. However, most existing frameworks remain confined to a single OS...<br/>Upvotes: 18<br/>GitHub Stars: 7813<br/>Authors: Chaoyun Zhang, Liqun Li, He Huang<br/>ðŸ”— <a href=\"https://github.com/microsoft/UFO/\">GitHub</a><br/>ðŸ”— <a href=\"https://microsoft.github.io/UFO/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2511.11332\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2511.11332",
    "external_url": "https://github.com/microsoft/UFO/",
    "published_date": "2025-11-14T09:05:31.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2511.11332",
      "upvotes": 18,
      "github_stars": 7813,
      "github_url": "https://github.com/microsoft/UFO/",
      "project_url": "https://microsoft.github.io/UFO/",
      "authors": [
        "Chaoyun Zhang",
        "Liqun Li",
        "He Huang",
        "Chiming Ni",
        "Bo Qiao",
        "Si Qin",
        "Yu Kang",
        "Minghua Ma",
        "Qingwei Lin",
        "Saravan Rajmohan",
        "Dongmei Zhang"
      ],
      "summary": "Large language model (LLM)-powered agents are transforming digital devices from passive tools into proactive intelligent collaborators. However, most existing frameworks remain confined to a single OS or device, making cross-device workflows brittle and largely manual. We present UFO^3, a system that unifies heterogeneous endpoints, desktops, servers, mobile devices, and edge, into a single orchestration fabric. UFO^3 models each user request as a mutable TaskConstellation: a distributed DAG of atomic subtasks (TaskStars) with explicit control and data dependencies (TaskStarLines). The TaskConstellation continuously evolves as results stream in from distributed devices, enabling asynchronous execution, adaptive recovery, and dynamic optimization. A Constellation Orchestrator} executes tasks safely and asynchronously while applying dynamic DAG updates, and the Agent Interaction Protocol (AIP) provides persistent, low-latency channels for reliable task dispatch and result streaming. These designs dissolve the traditional boundaries between devices and platforms, allowing agents to collaborate seamlessly and amplify their collective intelligence.\n  We evaluate UFO^3 on NebulaBench, a benchmark of 55 cross-device tasks across 5 machines and 10 categories. UFO^3 achieves 83.3% subtask completion, 70.9% task success, exposes parallelism with an average width of 1.72, and reduces end-to-end latency by 31% relative to a sequential baseline. Fault-injection experiments demonstrate graceful degradation and recovery under transient and permanent agent failures. These results show that UFO^3 achieves accurate, efficient, and resilient task orchestration across heterogeneous devices, uniting isolated agents into a coherent, adaptive computing fabric that extends across the landscape of ubiquitous computing.",
      "fetch_date": "2025-11-18",
      "num_comments": 3,
      "ai_summary": "UFO$^3$ unifies heterogeneous devices into a single orchestration fabric, enabling seamless task collaboration and dynamic optimization across distributed environments.",
      "ai_keywords": [
        "TaskConstellation",
        "TaskStars",
        "TaskStarLines",
        "DAG",
        "Constellation Orchestrator",
        "Agent Interaction Protocol",
        "NebulaBench",
        "parallelism",
        "end-to-end latency",
        "fault-injection",
        "adaptive computing fabric",
        "ubiquitous computing"
      ]
    }
  },
  {
    "id": "18503dda198b1fcbf7bb01dbf7296d31",
    "source": "huggingface",
    "type": "paper",
    "title": "Decoupled DMD: CFG Augmentation as the Spear, Distribution Matching as the Shield",
    "description": "Diffusion model distillation has emerged as a powerful technique for creating efficient few-step and single-step generators. Among these, Distribution Matching Distillation (DMD) and its variants stan...<br/>Upvotes: 27<br/>GitHub Stars: 7169<br/>Authors: Dongyang Liu, Peng Gao, David Liu<br/>ðŸ”— <a href=\"https://github.com/Tongyi-MAI/Z-Image/tree/main\">GitHub</a><br/>ðŸ”— <a href=\"https://tongyi-mai.github.io/Z-Image-blog/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2511.22677\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2511.22677",
    "external_url": "https://github.com/Tongyi-MAI/Z-Image/tree/main",
    "published_date": "2025-11-27T13:24:28.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2511.22677",
      "upvotes": 27,
      "github_stars": 7169,
      "github_url": "https://github.com/Tongyi-MAI/Z-Image/tree/main",
      "project_url": "https://tongyi-mai.github.io/Z-Image-blog/",
      "authors": [
        "Dongyang Liu",
        "Peng Gao",
        "David Liu",
        "Ruoyi Du",
        "Zhen Li",
        "Qilong Wu",
        "Xin Jin",
        "Sihan Cao",
        "Shifeng Zhang",
        "Hongsheng Li",
        "Steven Hoi"
      ],
      "summary": "Diffusion model distillation has emerged as a powerful technique for creating efficient few-step and single-step generators. Among these, Distribution Matching Distillation (DMD) and its variants stand out for their impressive performance, which is widely attributed to their core mechanism of matching the student's output distribution to that of a pre-trained teacher model. In this work, we challenge this conventional understanding. Through a rigorous decomposition of the DMD training objective, we reveal that in complex tasks like text-to-image generation, where CFG is typically required for desirable few-step performance, the primary driver of few-step distillation is not distribution matching, but a previously overlooked component we identify as CFG Augmentation (CA). We demonstrate that this term acts as the core ``engine'' of distillation, while the Distribution Matching (DM) term functions as a ``regularizer'' that ensures training stability and mitigates artifacts. We further validate this decoupling by demonstrating that while the DM term is a highly effective regularizer, it is not unique; simpler non-parametric constraints or GAN-based objectives can serve the same stabilizing function, albeit with different trade-offs. This decoupling of labor motivates a more principled analysis of the properties of both terms, leading to a more systematic and in-depth understanding. This new understanding further enables us to propose principled modifications to the distillation process, such as decoupling the noise schedules for the engine and the regularizer, leading to further performance gains. Notably, our method has been adopted by the Z-Image ( https://github.com/Tongyi-MAI/Z-Image ) project to develop a top-tier 8-step image generation model, empirically validating the generalization and robustness of our findings.",
      "fetch_date": "2025-12-01",
      "num_comments": 2,
      "ai_summary": "The study reveals that in text-to-image generation, CFG Augmentation is the primary driver of few-step distillation in Distribution Matching Distillation (DMD), while the distribution matching term acts as a regularizer.",
      "ai_keywords": [
        "diffusion model distillation",
        "Distribution Matching Distillation (DMD)",
        "CFG Augmentation (CA)",
        "text-to-image generation",
        "configuration (CFG)",
        "distribution matching",
        "noise schedules",
        "Z-Image"
      ]
    }
  }
]