[
  {
    "id": "2442f667afe5130cc9a066a427c61d55",
    "source": "huggingface",
    "type": "paper",
    "title": "OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-training in Every Iteration",
    "description": "As high-quality public text approaches exhaustion, a phenomenon known as the Data Wall, pre-training is shifting from more tokens to better tokens. However, existing methods either rely on heuristic s...<br/>Upvotes: 317<br/>Authors: Shaobo Wang, Xuan Ouyang, Tianyi Xu<br/>ðŸ”— <a href=\"https://huggingface.co/papers/2602.05400\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2602.05400",
    "external_url": "",
    "published_date": "2026-02-05T02:34:23.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2602.05400",
      "upvotes": 317,
      "github_stars": 0,
      "github_url": "",
      "project_url": "",
      "authors": [
        "Shaobo Wang",
        "Xuan Ouyang",
        "Tianyi Xu",
        "Yuzheng Hu",
        "Jialin Liu",
        "Guo Chen",
        "Tianyu Zhang",
        "Junhao Zheng",
        "Kexin Yang",
        "Xingzhang Ren",
        "Dayiheng Liu",
        "Linfeng Zhang"
      ],
      "summary": "As high-quality public text approaches exhaustion, a phenomenon known as the Data Wall, pre-training is shifting from more tokens to better tokens. However, existing methods either rely on heuristic static filters that ignore training dynamics, or use dynamic yet optimizer-agnostic criteria based on raw gradients. We propose OPUS (Optimizer-induced Projected Utility Selection), a dynamic data selection framework that defines utility in the optimizer-induced update space. OPUS scores candidates by projecting their effective updates, shaped by modern optimizers, onto a target direction derived from a stable, in-distribution proxy. To ensure scalability, we employ Ghost technique with CountSketch for computational efficiency, and Boltzmann sampling for data diversity, incurring only 4.7\\% additional compute overhead. OPUS achieves remarkable results across diverse corpora, quality tiers, optimizers, and model scales. In pre-training of GPT-2 Large/XL on FineWeb and FineWeb-Edu with 30B tokens, OPUS outperforms industrial-level baselines and even full 200B-token training. Moreover, when combined with industrial-level static filters, OPUS further improves pre-training efficiency, even with lower-quality data. Furthermore, in continued pre-training of Qwen3-8B-Base on SciencePedia, OPUS achieves superior performance using only 0.5B tokens compared to full training with 3B tokens, demonstrating significant data efficiency gains in specialized domains.",
      "fetch_date": "2026-02-11",
      "num_comments": 3,
      "ai_summary": "OPUS is a dynamic data selection framework that improves pre-training efficiency by scoring data candidates based on optimizer-induced update projections in a stable proxy-derived target space, achieving superior performance with reduced computational overhead.",
      "ai_keywords": [
        "data selection",
        "optimizer-induced update space",
        "effective updates",
        "stable in-distribution proxy",
        "Ghost technique",
        "CountSketch",
        "Boltzmann sampling",
        "pre-training",
        "GPT-2",
        "Qwen3-8B-Base",
        "FineWeb",
        "FineWeb-Edu",
        "SciencePedia"
      ]
    }
  },
  {
    "id": "60666ed538be4ebda8058b1f954547e6",
    "source": "huggingface",
    "type": "paper",
    "title": "Green-VLA: Staged Vision-Language-Action Model for Generalist Robots",
    "description": "We introduce Green-VLA, a staged Vision-Language-Action (VLA) framework for real-world deployment on the Green humanoid robot while maintaining generalization across diverse embodiments. Green-VLA fol...<br/>Upvotes: 280<br/>GitHub Stars: 36<br/>Authors: I. Apanasevich, M. Artemyev, R. Babakyan<br/>ðŸ”— <a href=\"https://github.com/greenvla/GreenVLA\">GitHub</a><br/>ðŸ”— <a href=\"https://greenvla.github.io\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2602.00919\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2602.00919",
    "external_url": "https://github.com/greenvla/GreenVLA",
    "published_date": "2026-01-31T17:13:23.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2602.00919",
      "upvotes": 280,
      "github_stars": 36,
      "github_url": "https://github.com/greenvla/GreenVLA",
      "project_url": "https://greenvla.github.io",
      "authors": [
        "I. Apanasevich",
        "M. Artemyev",
        "R. Babakyan",
        "P. Fedotova",
        "D. Grankin",
        "E. Kupryashin",
        "A. Misailidi",
        "D. Nerus",
        "A. Nutalapati",
        "G. Sidorov",
        "I. Efremov",
        "M. Gerasyov",
        "D. Pikurov",
        "Y. Senchenko",
        "S. Davidenko",
        "D. Kulikov",
        "M. Sultankin",
        "K. Askarbek",
        "O. Shamanin",
        "D. Statovoy",
        "E. Zalyaev",
        "I. Zorin",
        "A. Letkin",
        "E. Rusakov",
        "A. Silchenko",
        "V. Vorobyov",
        "S. Sobolnikov",
        "A. Postnikov"
      ],
      "summary": "We introduce Green-VLA, a staged Vision-Language-Action (VLA) framework for real-world deployment on the Green humanoid robot while maintaining generalization across diverse embodiments. Green-VLA follows a five stage curriculum: (L0) foundational VLMs, (L1) multimodal grounding, (R0) multi-embodiment pretraining, (R1) embodiment-specific adaptation, and (R2) reinforcement-learning (RL) policy alignment. We couple a scalable data-processing pipeline (3,000 hours of demonstrations) with temporal alignment and quality filtering, and use a unified, embodiment-aware action interface enabling a single policy to control humanoids, mobile manipulators, and fixed-base arms. At inference, the VLA controller is enhanced with episode-progress prediction, out-of-distribution detection, and joint-prediction-based guidance to improve safety and precise target selection. Experiments on Simpler BRIDGE WidowX and CALVIN ABC-D, as well as real-robot evaluations, show strong generalization and performance gains from RL alignment in success rate, robustness, and long-horizon efficiency.",
      "fetch_date": "2026-02-03",
      "num_comments": 7,
      "ai_summary": "Green-VLA is a five-stage vision-language-action framework for real-world robot deployment that achieves generalization across different robot embodiments through multimodal training and reinforcement learning.",
      "ai_keywords": [
        "Vision-Language-Action",
        "multimodal grounding",
        "multi-embodiment pretraining",
        "embodiment-specific adaptation",
        "reinforcement-learning",
        "episode-progress prediction",
        "out-of-distribution detection",
        "joint-prediction-based guidance"
      ]
    }
  },
  {
    "id": "59257a8c7ec0c63e71909f450abc636c",
    "source": "huggingface",
    "type": "paper",
    "title": "Weak-Driven Learning: How Weak Agents make Strong Agents Stronger",
    "description": "As post-training optimization becomes central to improving large language models, we observe a persistent saturation bottleneck: once models grow highly confident, further training yields diminishing ...<br/>Upvotes: 256<br/>GitHub Stars: 83<br/>Authors: Zehao Chen, Gongxun Li, Tianxiang Ai<br/>ðŸ”— <a href=\"https://github.com/chenzehao82/Weak-Driven-Learning\">GitHub</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2602.08222\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2602.08222",
    "external_url": "https://github.com/chenzehao82/Weak-Driven-Learning",
    "published_date": "2026-02-08T21:50:40.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2602.08222",
      "upvotes": 256,
      "github_stars": 83,
      "github_url": "https://github.com/chenzehao82/Weak-Driven-Learning",
      "project_url": "",
      "authors": [
        "Zehao Chen",
        "Gongxun Li",
        "Tianxiang Ai",
        "Yifei Li",
        "Zixuan Huang",
        "Wang Zhou",
        "Fuzhen Zhuang",
        "Xianglong Liu",
        "Jianxin Li",
        "Deqing Wang",
        "Yikun Ban"
      ],
      "summary": "As post-training optimization becomes central to improving large language models, we observe a persistent saturation bottleneck: once models grow highly confident, further training yields diminishing returns. While existing methods continue to reinforce target predictions, we find that informative supervision signals remain latent in models' own historical weak states. Motivated by this observation, we propose WMSS (Weak Agents Can Make Strong Agents Stronger), a post-training paradigm that leverages weak checkpoints to guide continued optimization. By identifying recoverable learning gaps via entropy dynamics and reinforcing them through compensatory learning, WMSS enables strong agents to improve beyond conventional post-training saturation. Experiments on mathematical reasoning and code generation datasets show that agents trained with our approach achieve effective performance improvements, while incurring zero additional inference cost.",
      "fetch_date": "2026-02-10",
      "num_comments": 5,
      "ai_summary": "WMSS is a post-training paradigm that uses weak model checkpoints to identify and fill learning gaps, enabling continued improvement beyond conventional saturation points in large language models.",
      "ai_keywords": [
        "post-training optimization",
        "large language models",
        "saturation bottleneck",
        "weak checkpoints",
        "entropy dynamics",
        "compensatory learning",
        "learning gaps"
      ]
    }
  },
  {
    "id": "b1cff772289aab941c862bdff282297c",
    "source": "huggingface",
    "type": "paper",
    "title": "ERNIE 5.0 Technical Report",
    "description": "In this report, we introduce ERNIE 5.0, a natively autoregressive foundation model desinged for unified multimodal understanding and generation across text, image, video, and audio. All modalities are...<br/>Upvotes: 251<br/>Authors: Haifeng Wang, Hua Wu, Tian Wu<br/>ðŸ”— <a href=\"https://huggingface.co/papers/2602.04705\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2602.04705",
    "external_url": "",
    "published_date": "2026-02-04T11:18:15.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2602.04705",
      "upvotes": 251,
      "github_stars": 0,
      "github_url": "",
      "project_url": "",
      "authors": [
        "Haifeng Wang",
        "Hua Wu",
        "Tian Wu",
        "Yu Sun",
        "Jing Liu",
        "Dianhai Yu",
        "Yanjun Ma",
        "Jingzhou He",
        "Zhongjun He",
        "Dou Hong",
        "Qiwen Liu",
        "Shuohuan Wang",
        "Junyuan Shang",
        "Zhenyu Zhang",
        "Yuchen Ding",
        "Jinle Zeng",
        "Jiabin Yang",
        "Liang Shen",
        "Ruibiao Chen",
        "Weichong Yin",
        "Siyu Ding",
        "Dai Dai",
        "Shikun Feng",
        "Siqi Bao",
        "Bolei He",
        "Yan Chen",
        "Zhenyu Jiao",
        "Ruiqing Zhang",
        "Zeyu Chen",
        "Qingqing Dang",
        "Kaipeng Deng",
        "Jiajun Jiang",
        "Enlei Gong",
        "Guoxia Wang",
        "Yanlin Sha",
        "Yi Liu",
        "Yehan Zheng",
        "Weijian Xu",
        "Jiaxiang Liu",
        "Zengfeng Zeng",
        "Yingqi Qu",
        "Zhongli Li",
        "Zhengkun Zhang",
        "Xiyang Wang",
        "Zixiang Xu",
        "Xinchao Xu",
        "Zhengjie Huang",
        "Dong Wang",
        "Bingjin Chen",
        "Yue Chang",
        "Xing Yuan",
        "Shiwei Huang",
        "Qiao Zhao",
        "Xinzhe Ding",
        "Shuangshuang Qiao",
        "Baoshan Yang",
        "Bihong Tang",
        "Bin Li",
        "Bingquan Wang",
        "Binhan Tang",
        "Binxiong Zheng",
        "Bo Cui",
        "Bo Ke",
        "Bo Zhang",
        "Bowen Zhang",
        "Boyan Zhang",
        "Boyang Liu",
        "Caiji Zhang",
        "Can Li",
        "Chang Xu",
        "Chao Pang",
        "Chao Zhang",
        "Chaoyi Yuan",
        "Chen Chen",
        "Cheng Cui",
        "Chenlin Yin",
        "Chun Gan",
        "Chunguang Chai",
        "Chuyu Fang",
        "Cuiyun Han",
        "Dan Zhang",
        "Danlei Feng",
        "Danxiang Zhu",
        "Dong Sun",
        "Dongbo Li",
        "Dongdong Li",
        "Dongdong Liu",
        "Dongxue Liu",
        "Fan Ding",
        "Fan Hu",
        "Fan Li",
        "Fan Mo",
        "Feisheng Wu",
        "Fengwei Liu",
        "Gangqiang Hu",
        "Gaofeng Lu",
        "Gaopeng Yong",
        "Gexiao Tian",
        "Guan Wang",
        "Guangchen Ni",
        "Guangshuo Wu",
        "Guanzhong Wang",
        "Guihua Liu",
        "Guishun Li",
        "Haibin Li",
        "Haijian Liang",
        "Haipeng Ming",
        "Haisu Wang",
        "Haiyang Lu",
        "Haiye Lin",
        "Han Zhou",
        "Hangting Lou",
        "Hanwen Du",
        "Hanzhi Zhang",
        "Hao Chen",
        "Hao Du",
        "Hao Liu",
        "Hao Zhou",
        "Haochen Jiang",
        "Haodong Tian",
        "Haoshuang Wang",
        "Haozhe Geng",
        "Heju Yin",
        "Hong Chen",
        "Hongchen Xue",
        "Hongen Liu",
        "Honggeng Zhang",
        "Hongji Xu",
        "Hongwei Chen",
        "Hongyang Zhang",
        "Hongyuan Zhang",
        "Hua Lu",
        "Huan Chen",
        "Huan Wang",
        "Huang He",
        "Hui Liu",
        "Hui Zhong",
        "Huibin Ruan",
        "Jiafeng Lu",
        "Jiage Liang",
        "Jiahao Hu",
        "Jiahao Hu",
        "Jiajie Yang",
        "Jialin Li",
        "Jian Chen",
        "Jian Wu",
        "Jianfeng Yang",
        "Jianguang Jiang",
        "Jianhua Wang",
        "Jianye Chen",
        "Jiaodi Liu",
        "Jiarui Zhou",
        "Jiawei Lv",
        "Jiaxin Zhou",
        "Jiaxuan Liu",
        "Jie Han",
        "Jie Sun",
        "Jiefan Fang",
        "Jihan Liu",
        "Jihua Liu",
        "Jing Hu",
        "Jing Qian",
        "Jing Yan",
        "Jingdong Du",
        "Jingdong Wang",
        "Jingjing Wu",
        "Jingyong Li",
        "Jinheng Wang",
        "Jinjin Li",
        "Jinliang Lu",
        "Jinlin Yu",
        "Jinnan Liu",
        "Jixiang Feng",
        "Jiyi Huang",
        "Jiyuan Zhang",
        "Jun Liang",
        "Jun Xia",
        "Jun Yu",
        "Junda Chen",
        "Junhao Feng",
        "Junhong Xiang",
        "Junliang Li",
        "Kai Liu",
        "Kailun Chen",
        "Kairan Su",
        "Kang Hu",
        "Kangkang Zhou",
        "Ke Chen",
        "Ke Wei",
        "Kui Huang",
        "Kun Wu",
        "Kunbin Chen",
        "Lei Han",
        "Lei Sun",
        "Lei Wen",
        "Linghui Meng",
        "Linhao Yu",
        "Liping Ouyang",
        "Liwen Zhang",
        "Longbin Ji",
        "Longzhi Wang",
        "Meng Sun",
        "Meng Tian",
        "Mengfei Li",
        "Mengqi Zeng",
        "Mengyu Zhang",
        "Ming Hong",
        "Mingcheng Zhou",
        "Mingming Huang",
        "Mingxin Chen",
        "Mingzhu Cai",
        "Naibin Gu",
        "Nemin Qiu",
        "Nian Wang",
        "Peng Qiu",
        "Peng Zhao",
        "Pengyu Zou",
        "Qi Wang",
        "Qi Xin",
        "Qian Wang",
        "Qiang Zhu",
        "Qianhui Luo",
        "Qianwei Yang",
        "Qianyue He",
        "Qifei Wu",
        "Qinrui Li",
        "Qiwen Bao",
        "Quan Zhang",
        "Quanxiang Liu",
        "Qunyi Xie",
        "Rongrui Zhan",
        "Rufeng Dai",
        "Rui Peng",
        "Ruian Liu",
        "Ruihao Xu",
        "Ruijie Wang",
        "Ruixi Zhang",
        "Ruixuan Liu",
        "Runsheng Shi",
        "Ruting Wang",
        "Senbo Kang",
        "Shan Lu",
        "Shaofei Yu",
        "Shaotian Gong",
        "Shenwei Hu",
        "Shifeng Zheng",
        "Shihao Guo",
        "Shilong Fan",
        "Shiqin Liu",
        "Shiwei Gu",
        "Shixi Zhang",
        "Shuai Yao",
        "Shuang Zhang",
        "Shuangqiao Liu",
        "Shuhao Liang",
        "Shuwei He",
        "Shuwen Yang",
        "Sijun He",
        "Siming Dai",
        "Siming Wu",
        "Siyi Long",
        "Songhe Deng",
        "Suhui Dong",
        "Suyin Liang",
        "Teng Hu",
        "Tianchan Xu",
        "Tianliang Lv",
        "Tianmeng Yang",
        "Tianyi Wei",
        "Tiezhu Gao",
        "Ting Sun",
        "Ting Zhang",
        "Tingdan Luo",
        "Wei He",
        "Wei Luan",
        "Wei Yin",
        "Wei Zhang",
        "Wei Zhou",
        "Weibao Gong",
        "Weibin Li",
        "Weicheng Huang",
        "Weichong Dang",
        "Weiguo Zhu",
        "Weilong Zhang",
        "Weiqi Tan",
        "Wen Huang",
        "Wenbin Chang",
        "Wenjing Du",
        "Wenlong Miao",
        "Wenpei Luo",
        "Wenquan Wu",
        "Xi Shi",
        "Xi Zhao",
        "Xiang Gao",
        "Xiangguo Zhang",
        "Xiangrui Yu",
        "Xiangsen Wang",
        "Xiangzhe Wang",
        "Xianlong Luo",
        "Xianying Ma",
        "Xiao Tan",
        "Xiaocong Lin",
        "Xiaofei Wang",
        "Xiaofeng Peng",
        "Xiaofeng Wu",
        "Xiaojian Xu",
        "Xiaolan Yuan",
        "Xiaopeng Cui",
        "Xiaotian Han",
        "Xiaoxiong Liu",
        "Xiaoxu Fei",
        "Xiaoxuan Wu",
        "Xiaoyu Wang",
        "Xiaoyu Zhang",
        "Xin Sun",
        "Xin Wang",
        "Xinhui Huang",
        "Xinming Zhu",
        "Xintong Yu",
        "Xinyi Xu",
        "Xinyu Wang",
        "Xiuxian Li",
        "XuanShi Zhu",
        "Xue Xu",
        "Xueying Lv",
        "Xuhong Li",
        "Xulong Wei",
        "Xuyi Chen",
        "Yabing Shi",
        "Yafeng Wang",
        "Yamei Li",
        "Yan Liu",
        "Yanfu Cheng",
        "Yang Gao",
        "Yang Liang",
        "Yang Wang",
        "Yang Wang",
        "Yang Yang",
        "Yanlong Liu",
        "Yannian Fu",
        "Yanpeng Wang",
        "Yanzheng Lin",
        "Yao Chen",
        "Yaozong Shen",
        "Yaqian Han",
        "Yehua Yang",
        "Yekun Chai",
        "Yesong Wang",
        "Yi Song",
        "Yichen Zhang",
        "Yifei Wang",
        "Yifeng Guo",
        "Yifeng Kou",
        "Yilong Chen",
        "Yilong Guo",
        "Yiming Wang",
        "Ying Chen",
        "Ying Wang",
        "Yingsheng Wu",
        "Yingzhan Lin",
        "Yinqi Yang",
        "Yiran Xing",
        "Yishu Lei",
        "Yixiang Tu",
        "Yiyan Chen",
        "Yong Zhang",
        "Yonghua Li",
        "Yongqiang Ma",
        "Yongxing Dai",
        "Yongyue Zhang",
        "Yu Ran",
        "Yu Sun",
        "Yu-Wen Michael Zhang",
        "Yuang Liu",
        "Yuanle Liu",
        "Yuanyuan Zhou",
        "Yubo Zhang",
        "Yuchen Han",
        "Yucheng Wang",
        "Yude Gao",
        "Yuedong Luo",
        "Yuehu Dong",
        "Yufeng Hu",
        "Yuhui Cao",
        "Yuhui Yun",
        "Yukun Chen",
        "Yukun Gao",
        "Yukun Li",
        "Yumeng Zhang",
        "Yun Fan",
        "Yun Ma",
        "Yunfei Zhang",
        "Yunshen Xie",
        "Yuping Xu",
        "Yuqin Zhang",
        "Yuqing Liu",
        "Yurui Li",
        "Yuwen Wang",
        "Yuxiang Lu",
        "Zefeng Cai",
        "Zelin Zhao",
        "Zelun Zhang",
        "Zenan Lin",
        "Zezhao Dong",
        "Zhaowu Pan",
        "Zhaoyu Liu",
        "Zhe Dong",
        "Zhe Zhang",
        "Zhen Zhang",
        "Zhengfan Wu",
        "Zhengrui Wei",
        "Zhengsheng Ning",
        "Zhenxing Li",
        "Zhenyu Li",
        "Zhenyu Qian",
        "Zhenyun Li",
        "Zhi Li",
        "Zhichao Chen",
        "Zhicheng Dong",
        "Zhida Feng",
        "Zhifan Feng",
        "Zhihao Deng",
        "Zhijin Yu",
        "Zhiyang Chen",
        "Zhonghui Zheng",
        "Zhuangzhuang Guo",
        "Zhujun Zhang",
        "Zhuo Sun",
        "Zichang Liu",
        "Zihan Lin",
        "Zihao Huang",
        "Zihe Zhu",
        "Ziheng Zhao",
        "Ziping Chen",
        "Zixuan Zhu",
        "Ziyang Xu",
        "Ziyi Liang",
        "Ziyuan Gao"
      ],
      "summary": "In this report, we introduce ERNIE 5.0, a natively autoregressive foundation model desinged for unified multimodal understanding and generation across text, image, video, and audio. All modalities are trained from scratch under a unified next-group-of-tokens prediction objective, based on an ultra-sparse mixture-of-experts (MoE) architecture with modality-agnostic expert routing. To address practical challenges in large-scale deployment under diverse resource constraints, ERNIE 5.0 adopts a novel elastic training paradigm. Within a single pre-training run, the model learns a family of sub-models with varying depths, expert capacities, and routing sparsity, enabling flexible trade-offs among performance, model size, and inference latency in memory- or time-constrained scenarios. Moreover, we systematically address the challenges of scaling reinforcement learning to unified foundation models, thereby guaranteeing efficient and stable post-training under ultra-sparse MoE architectures and diverse multimodal settings. Extensive experiments demonstrate that ERNIE 5.0 achieves strong and balanced performance across multiple modalities. To the best of our knowledge, among publicly disclosed models, ERNIE 5.0 represents the first production-scale realization of a trillion-parameter unified autoregressive model that supports both multimodal understanding and generation. To facilitate further research, we present detailed visualizations of modality-agnostic expert routing in the unified model, alongside comprehensive empirical analysis of elastic training, aiming to offer profound insights to the community.",
      "fetch_date": "2026-02-05",
      "num_comments": 4,
      "ai_summary": "ERNIE 5.0 is a production-scale trillion-parameter autoregressive model that unifies multimodal understanding and generation through sparse MoE architecture and elastic training.",
      "ai_keywords": [
        "autoregressive foundation model",
        "unified multimodal understanding",
        "unified next-group-of-tokens prediction objective",
        "mixture-of-experts",
        "modality-agnostic expert routing",
        "elastic training paradigm",
        "reinforcement learning",
        "sparse MoE architecture"
      ]
    }
  },
  {
    "id": "4f9739b510c1386a39502a9995b593ed",
    "source": "huggingface",
    "type": "paper",
    "title": "Kimi K2.5: Visual Agentic Intelligence",
    "description": "We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhan...<br/>Upvotes: 234<br/>GitHub Stars: 1054<br/>Authors: Kimi Team, Tongtong Bai, Yifan Bai<br/>ðŸ”— <a href=\"https://github.com/MoonshotAI/Kimi-K2.5\">GitHub</a><br/>ðŸ”— <a href=\"https://www.kimi.com/blog/kimi-k2-5.html\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2602.02276\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2602.02276",
    "external_url": "https://github.com/MoonshotAI/Kimi-K2.5",
    "published_date": "2026-02-02T11:17:38.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2602.02276",
      "upvotes": 234,
      "github_stars": 1054,
      "github_url": "https://github.com/MoonshotAI/Kimi-K2.5",
      "project_url": "https://www.kimi.com/blog/kimi-k2-5.html",
      "authors": [
        "Kimi Team",
        "Tongtong Bai",
        "Yifan Bai",
        "Yiping Bao",
        "S. H. Cai",
        "Yuan Cao",
        "Y. Charles",
        "H. S. Che",
        "Cheng Chen",
        "Guanduo Chen",
        "Huarong Chen",
        "Jia Chen",
        "Jiahao Chen",
        "Jianlong Chen",
        "Jun Chen",
        "Kefan Chen",
        "Liang Chen",
        "Ruijue Chen",
        "Xinhao Chen",
        "Yanru Chen",
        "Yanxu Chen",
        "Yicun Chen",
        "Yimin Chen",
        "Yingjiang Chen",
        "Yuankun Chen",
        "Yujie Chen",
        "Yutian Chen",
        "Zhirong Chen",
        "Ziwei Chen",
        "Dazhi Cheng",
        "Minghan Chu",
        "Jialei Cui",
        "Jiaqi Deng",
        "Muxi Diao",
        "Hao Ding",
        "Mengfan Dong",
        "Mengnan Dong",
        "Yuxin Dong",
        "Yuhao Dong",
        "Angang Du",
        "Chenzhuang Du",
        "Dikang Du",
        "Lingxiao Du",
        "Yulun Du",
        "Yu Fan",
        "Shengjun Fang",
        "Qiulin Feng",
        "Yichen Feng",
        "Garimugai Fu",
        "Kelin Fu",
        "Hongcheng Gao",
        "Tong Gao",
        "Yuyao Ge",
        "Shangyi Geng",
        "Chengyang Gong",
        "Xiaochen Gong",
        "Zhuoma Gongque",
        "Qizheng Gu",
        "Xinran Gu",
        "Yicheng Gu",
        "Longyu Guan",
        "Yuanying Guo",
        "Xiaoru Hao",
        "Weiran He",
        "Wenyang He",
        "Yunjia He",
        "Chao Hong",
        "Hao Hu",
        "Jiaxi Hu",
        "Yangyang Hu",
        "Zhenxing Hu",
        "Ke Huang",
        "Ruiyuan Huang",
        "Weixiao Huang",
        "Zhiqi Huang",
        "Tao Jiang",
        "Zhejun Jiang",
        "Xinyi Jin",
        "Yu Jing",
        "Guokun Lai",
        "Aidi Li",
        "C. Li",
        "Cheng Li",
        "Fang Li",
        "Guanghe Li",
        "Guanyu Li",
        "Haitao Li",
        "Haoyang Li",
        "Jia Li",
        "Jingwei Li",
        "Junxiong Li",
        "Lincan Li",
        "Mo Li",
        "Weihong Li",
        "Wentao Li",
        "Xinhang Li",
        "Xinhao Li",
        "Yang Li",
        "Yanhao Li",
        "Yiwei Li",
        "Yuxiao Li",
        "Zhaowei Li",
        "Zheming Li",
        "Weilong Liao",
        "Jiawei Lin",
        "Xiaohan Lin",
        "Zhishan Lin",
        "Zichao Lin",
        "Cheng Liu",
        "Chenyu Liu",
        "Hongzhang Liu",
        "Liang Liu",
        "Shaowei Liu",
        "Shudong Liu",
        "Shuran Liu",
        "Tianwei Liu",
        "Tianyu Liu",
        "Weizhou Liu",
        "Xiangyan Liu",
        "Yangyang Liu",
        "Yanming Liu",
        "Yibo Liu",
        "Yuanxin Liu",
        "Yue Liu",
        "Zhengying Liu",
        "Zhongnuo Liu",
        "Enzhe Lu",
        "Haoyu Lu",
        "Zhiyuan Lu",
        "Junyu Luo",
        "Tongxu Luo",
        "Yashuo Luo",
        "Long Ma",
        "Yingwei Ma",
        "Shaoguang Mao",
        "Yuan Mei",
        "Xin Men",
        "Fanqing Meng",
        "Zhiyong Meng",
        "Yibo Miao",
        "Minqing Ni",
        "Kun Ouyang",
        "Siyuan Pan",
        "Bo Pang",
        "Yuchao Qian",
        "Ruoyu Qin",
        "Zeyu Qin",
        "Jiezhong Qiu",
        "Bowen Qu",
        "Zeyu Shang",
        "Youbo Shao",
        "Tianxiao Shen",
        "Zhennan Shen",
        "Juanfeng Shi",
        "Lidong Shi",
        "Shengyuan Shi",
        "Feifan Song",
        "Pengwei Song",
        "Tianhui Song",
        "Xiaoxi Song",
        "Hongjin Su",
        "Jianlin Su",
        "Zhaochen Su",
        "Lin Sui",
        "Jinsong Sun",
        "Junyao Sun",
        "Tongyu Sun",
        "Flood Sung",
        "Yunpeng Tai",
        "Chuning Tang",
        "Heyi Tang",
        "Xiaojuan Tang",
        "Zhengyang Tang",
        "Jiawen Tao",
        "Shiyuan Teng",
        "Chaoran Tian",
        "Pengfei Tian",
        "Ao Wang",
        "Bowen Wang",
        "Chensi Wang",
        "Chuang Wang",
        "Congcong Wang",
        "Dingkun Wang",
        "Dinglu Wang",
        "Dongliang Wang",
        "Feng Wang",
        "Hailong Wang",
        "Haiming Wang",
        "Hengzhi Wang",
        "Huaqing Wang",
        "Hui Wang",
        "Jiahao Wang",
        "Jinhong Wang",
        "Jiuzheng Wang",
        "Kaixin Wang",
        "Linian Wang",
        "Qibin Wang",
        "Shengjie Wang",
        "Shuyi Wang",
        "Si Wang",
        "Wei Wang",
        "Xiaochen Wang",
        "Xinyuan Wang",
        "Yao Wang",
        "Yejie Wang",
        "Yipu Wang",
        "Yiqin Wang",
        "Yucheng Wang",
        "Yuzhi Wang",
        "Zhaoji Wang",
        "Zhaowei Wang",
        "Zhengtao Wang",
        "Zhexu Wang",
        "Zihan Wang",
        "Zizhe Wang",
        "Chu Wei",
        "Ming Wei",
        "Chuan Wen",
        "Zichen Wen",
        "Chengjie Wu",
        "Haoning Wu",
        "Junyan Wu",
        "Rucong Wu",
        "Wenhao Wu",
        "Yuefeng Wu",
        "Yuhao Wu",
        "Yuxin Wu",
        "Zijian Wu",
        "Chenjun Xiao",
        "Jin Xie",
        "Xiaotong Xie",
        "Yuchong Xie",
        "Yifei Xin",
        "Bowei Xing",
        "Boyu Xu",
        "Jianfan Xu",
        "Jing Xu",
        "Jinjing Xu",
        "L. H. Xu",
        "Lin Xu",
        "Suting Xu",
        "Weixin Xu",
        "Xinbo Xu",
        "Xinran Xu",
        "Yangchuan Xu",
        "Yichang Xu",
        "Yuemeng Xu",
        "Zelai Xu",
        "Ziyao Xu",
        "Junjie Yan",
        "Yuzi Yan",
        "Guangyao Yang",
        "Hao Yang",
        "Junwei Yang",
        "Kai Yang",
        "Ningyuan Yang",
        "Ruihan Yang",
        "Xiaofei Yang",
        "Xinlong Yang",
        "Ying Yang",
        "Yi Yang",
        "Yi Yang",
        "Zhen Yang",
        "Zhilin Yang",
        "Zonghan Yang",
        "Haotian Yao",
        "Dan Ye",
        "Wenjie Ye",
        "Zhuorui Ye",
        "Bohong Yin",
        "Chengzhen Yu",
        "Longhui Yu",
        "Tao Yu",
        "Tianxiang Yu",
        "Enming Yuan",
        "Mengjie Yuan",
        "Xiaokun Yuan",
        "Yang Yue",
        "Weihao Zeng",
        "Dunyuan Zha",
        "Haobing Zhan",
        "Dehao Zhang",
        "Hao Zhang",
        "Jin Zhang",
        "Puqi Zhang",
        "Qiao Zhang",
        "Rui Zhang",
        "Xiaobin Zhang",
        "Y. Zhang",
        "Yadong Zhang",
        "Yangkun Zhang",
        "Yichi Zhang",
        "Yizhi Zhang",
        "Yongting Zhang",
        "Yu Zhang",
        "Yushun Zhang",
        "Yutao Zhang",
        "Yutong Zhang",
        "Zheng Zhang",
        "Chenguang Zhao",
        "Feifan Zhao",
        "Jinxiang Zhao",
        "Shuai Zhao",
        "Xiangyu Zhao",
        "Yikai Zhao",
        "Zijia Zhao",
        "Huabin Zheng",
        "Ruihan Zheng",
        "Shaojie Zheng",
        "Tengyang Zheng",
        "Junfeng Zhong",
        "Longguang Zhong",
        "Weiming Zhong",
        "M. Zhou",
        "Runjie Zhou",
        "Xinyu Zhou",
        "Zaida Zhou",
        "Jinguo Zhu",
        "Liya Zhu",
        "Xinhao Zhu",
        "Yuxuan Zhu",
        "Zhen Zhu",
        "Jingze Zhuang",
        "Weiyu Zhuang",
        "Ying Zou",
        "Xinxing Zu"
      ],
      "summary": "We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhance each other. This includes a series of techniques such as joint text-vision pre-training, zero-vision SFT, and joint text-vision reinforcement learning. Building on this multimodal foundation, K2.5 introduces Agent Swarm, a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. Extensive evaluations show that Kimi K2.5 achieves state-of-the-art results across various domains including coding, vision, reasoning, and agentic tasks. Agent Swarm also reduces latency by up to 4.5times over single-agent baselines. We release the post-trained Kimi K2.5 model checkpoint to facilitate future research and real-world applications of agentic intelligence.",
      "fetch_date": "2026-02-03",
      "num_comments": 3,
      "ai_summary": "Kimi K2.5 is an open-source multimodal agentic model that enhances text and vision processing through joint optimization techniques and introduces Agent Swarm for parallel task execution.",
      "ai_keywords": [
        "multimodal agentic model",
        "joint text-vision pre-training",
        "zero-vision SFT",
        "joint text-vision reinforcement learning",
        "Agent Swarm",
        "self-directed parallel agent orchestration framework",
        "heterogeneous sub-problems"
      ]
    }
  },
  {
    "id": "6752926501a77920d0fc0489405fae26",
    "source": "huggingface",
    "type": "paper",
    "title": "Less is Enough: Synthesizing Diverse Data in Feature Space of LLMs",
    "description": "The diversity of post-training data is critical for effective downstream performance in large language models (LLMs). Many existing approaches to constructing post-training data quantify diversity usi...<br/>Upvotes: 213<br/>GitHub Stars: 95<br/>Authors: Zhongzhi Li, Xuansheng Wu, Yijiang Li<br/>ðŸ”— <a href=\"https://github.com/Zhongzhi660/FAC-Synthesis\">GitHub</a><br/>ðŸ”— <a href=\"https://website-sigma-three-35.vercel.app/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2602.10388\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2602.10388",
    "external_url": "https://github.com/Zhongzhi660/FAC-Synthesis",
    "published_date": "2026-02-10T19:23:13.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2602.10388",
      "upvotes": 213,
      "github_stars": 95,
      "github_url": "https://github.com/Zhongzhi660/FAC-Synthesis",
      "project_url": "https://website-sigma-three-35.vercel.app/",
      "authors": [
        "Zhongzhi Li",
        "Xuansheng Wu",
        "Yijiang Li",
        "Lijie Hu",
        "Ninghao Liu"
      ],
      "summary": "The diversity of post-training data is critical for effective downstream performance in large language models (LLMs). Many existing approaches to constructing post-training data quantify diversity using text-based metrics that capture linguistic variation, but such metrics provide only weak signals for the task-relevant features that determine downstream performance. In this work, we introduce Feature Activation Coverage (FAC) which measures data diversity in an interpretable feature space. Building upon this metric, we further propose a diversity-driven data synthesis framework, named FAC Synthesis, that first uses a sparse autoencoder to identify missing features from a seed dataset, and then generates synthetic samples that explicitly reflect these features. Experiments show that our approach consistently improves both data diversity and downstream performance on various tasks, including instruction following, toxicity detection, reward modeling, and behavior steering. Interestingly, we identify a shared, interpretable feature space across model families (i.e., LLaMA, Mistral, and Qwen), enabling cross-model knowledge transfer. Our work provides a solid and practical methodology for exploring data-centric optimization of LLMs.",
      "fetch_date": "2026-02-16",
      "num_comments": 5,
      "ai_summary": "Feature Activation Coverage measures data diversity in an interpretable feature space and enables diversity-driven data synthesis that improves downstream performance across multiple language model architectures.",
      "ai_keywords": [
        "Feature Activation Coverage",
        "sparse autoencoder",
        "data diversity",
        "downstream performance",
        "instruction following",
        "toxicity detection",
        "reward modeling",
        "behavior steering",
        "cross-model knowledge transfer",
        "data-centric optimization"
      ]
    }
  },
  {
    "id": "e6cd663cc8eaf53e2ed564e037a90af3",
    "source": "huggingface",
    "type": "paper",
    "title": "Qwen3-TTS Technical Report",
    "description": "In this report, we present the Qwen3-TTS series, a family of advanced multilingual, controllable, robust, and streaming text-to-speech models. Qwen3-TTS supports state-of-the-art 3-second voice clonin...<br/>Upvotes: 67<br/>GitHub Stars: 7959<br/>Authors: Hangrui Hu, Xinfa Zhu, Ting He<br/>ðŸ”— <a href=\"https://github.com/QwenLM/Qwen3-TTS\">GitHub</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2601.15621\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2601.15621",
    "external_url": "https://github.com/QwenLM/Qwen3-TTS",
    "published_date": "2026-01-21T22:51:43.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2601.15621",
      "upvotes": 67,
      "github_stars": 7959,
      "github_url": "https://github.com/QwenLM/Qwen3-TTS",
      "project_url": "",
      "authors": [
        "Hangrui Hu",
        "Xinfa Zhu",
        "Ting He",
        "Dake Guo",
        "Bin Zhang",
        "Xiong Wang",
        "Zhifang Guo",
        "Ziyue Jiang",
        "Hongkun Hao",
        "Zishan Guo",
        "Xinyu Zhang",
        "Pei Zhang",
        "Baosong Yang",
        "Jin Xu",
        "Jingren Zhou",
        "Junyang Lin"
      ],
      "summary": "In this report, we present the Qwen3-TTS series, a family of advanced multilingual, controllable, robust, and streaming text-to-speech models. Qwen3-TTS supports state-of-the-art 3-second voice cloning and description-based control, allowing both the creation of entirely novel voices and fine-grained manipulation over the output speech. Trained on over 5 million hours of speech data spanning 10 languages, Qwen3-TTS adopts a dual-track LM architecture for real-time synthesis, coupled with two speech tokenizers: 1) Qwen-TTS-Tokenizer-25Hz is a single-codebook codec emphasizing semantic content, which offers seamlessly integration with Qwen-Audio and enables streaming waveform reconstruction via a block-wise DiT. 2) Qwen-TTS-Tokenizer-12Hz achieves extreme bitrate reduction and ultra-low-latency streaming, enabling immediate first-packet emission (97,ms) through its 12.5 Hz, 16-layer multi-codebook design and a lightweight causal ConvNet. Extensive experiments indicate state-of-the-art performance across diverse objective and subjective benchmark (e.g., TTS multilingual test set, InstructTTSEval, and our long speech test set). To facilitate community research and development, we release both tokenizers and models under the Apache 2.0 license.",
      "fetch_date": "2026-01-23",
      "num_comments": 1,
      "ai_summary": "The Qwen3-TTS series presents advanced multilingual text-to-speech models with voice cloning and controllable speech generation capabilities, utilizing dual-track LM architecture and specialized speech tokenizers for efficient streaming synthesis.",
      "ai_keywords": [
        "text-to-speech",
        "voice cloning",
        "dual-track LM architecture",
        "speech tokenizers",
        "Qwen-TTS-Tokenizer-25Hz",
        "Qwen-TTS-Tokenizer-12Hz",
        "DiT",
        "ConvNet",
        "streaming waveform reconstruction",
        "multilingual",
        "controllable speech generation"
      ]
    }
  },
  {
    "id": "3ca1c9448a1392dc62dcb1e40f0a07ae",
    "source": "huggingface",
    "type": "paper",
    "title": "PaperBanana: Automating Academic Illustration for AI Scientists",
    "description": "Despite rapid advances in autonomous AI scientists powered by language models, generating publication-ready illustrations remains a labor-intensive bottleneck in the research workflow. To lift this bu...<br/>Upvotes: 188<br/>GitHub Stars: 3706<br/>Authors: Dawei Zhu, Rui Meng, Yale Song<br/>ðŸ”— <a href=\"https://github.com/dwzhu-pku/PaperBanana\">GitHub</a><br/>ðŸ”— <a href=\"https://dwzhu-pku.github.io/PaperBanana/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2601.23265\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2601.23265",
    "external_url": "https://github.com/dwzhu-pku/PaperBanana",
    "published_date": "2026-01-30T13:33:37.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2601.23265",
      "upvotes": 188,
      "github_stars": 3706,
      "github_url": "https://github.com/dwzhu-pku/PaperBanana",
      "project_url": "https://dwzhu-pku.github.io/PaperBanana/",
      "authors": [
        "Dawei Zhu",
        "Rui Meng",
        "Yale Song",
        "Xiyu Wei",
        "Sujian Li",
        "Tomas Pfister",
        "Jinsung Yoon"
      ],
      "summary": "Despite rapid advances in autonomous AI scientists powered by language models, generating publication-ready illustrations remains a labor-intensive bottleneck in the research workflow. To lift this burden, we introduce PaperBanana, an agentic framework for automated generation of publication-ready academic illustrations. Powered by state-of-the-art VLMs and image generation models, PaperBanana orchestrates specialized agents to retrieve references, plan content and style, render images, and iteratively refine via self-critique. To rigorously evaluate our framework, we introduce PaperBananaBench, comprising 292 test cases for methodology diagrams curated from NeurIPS 2025 publications, covering diverse research domains and illustration styles. Comprehensive experiments demonstrate that PaperBanana consistently outperforms leading baselines in faithfulness, conciseness, readability, and aesthetics. We further show that our method effectively extends to the generation of high-quality statistical plots. Collectively, PaperBanana paves the way for the automated generation of publication-ready illustrations.",
      "fetch_date": "2026-02-02",
      "num_comments": 12,
      "ai_summary": "_paperbanana is an agentic framework that automates the creation of publication-ready academic illustrations using advanced vision-language models and image generation techniques.",
      "ai_keywords": [
        "VLMs",
        "image generation models",
        "agentic framework",
        "publication-ready illustrations",
        "methodology diagrams",
        "PaperBananaBench",
        "self-critique",
        "statistical plots"
      ]
    }
  },
  {
    "id": "de0ab4deaf169a5f7442c17a4444ef3f",
    "source": "huggingface",
    "type": "paper",
    "title": "Advancing Open-source World Models",
    "description": "We present LingBot-World, an open-sourced world simulator stemming from video generation. Positioned as a top-tier world model, LingBot-World offers the following features. (1) It maintains high fidel...<br/>Upvotes: 127<br/>GitHub Stars: 2890<br/>Authors: Robbyant Team, Zelin Gao, Qiuyu Wang<br/>ðŸ”— <a href=\"https://github.com/Robbyant/lingbot-world\">GitHub</a><br/>ðŸ”— <a href=\"https://technology.robbyant.com/lingbot-world\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2601.20540\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2601.20540",
    "external_url": "https://github.com/Robbyant/lingbot-world",
    "published_date": "2026-01-28T07:37:01.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2601.20540",
      "upvotes": 127,
      "github_stars": 2890,
      "github_url": "https://github.com/Robbyant/lingbot-world",
      "project_url": "https://technology.robbyant.com/lingbot-world",
      "authors": [
        "Robbyant Team",
        "Zelin Gao",
        "Qiuyu Wang",
        "Yanhong Zeng",
        "Jiapeng Zhu",
        "Ka Leong Cheng",
        "Yixuan Li",
        "Hanlin Wang",
        "Yinghao Xu",
        "Shuailei Ma",
        "Yihang Chen",
        "Jie Liu",
        "Yansong Cheng",
        "Yao Yao",
        "Jiayi Zhu",
        "Yihao Meng",
        "Kecheng Zheng",
        "Qingyan Bai",
        "Jingye Chen",
        "Zehong Shen",
        "Yue Yu",
        "Xing Zhu",
        "Yujun Shen",
        "Hao Ouyang"
      ],
      "summary": "We present LingBot-World, an open-sourced world simulator stemming from video generation. Positioned as a top-tier world model, LingBot-World offers the following features. (1) It maintains high fidelity and robust dynamics in a broad spectrum of environments, including realism, scientific contexts, cartoon styles, and beyond. (2) It enables a minute-level horizon while preserving contextual consistency over time, which is also known as \"long-term memory\". (3) It supports real-time interactivity, achieving a latency of under 1 second when producing 16 frames per second. We provide public access to the code and model in an effort to narrow the divide between open-source and closed-source technologies. We believe our release will empower the community with practical applications across areas like content creation, gaming, and robot learning.",
      "fetch_date": "2026-01-29",
      "num_comments": 2,
      "ai_summary": "LingBot-World is an open-source world simulator with high-fidelity dynamics, long-term memory capabilities, and real-time interactivity for diverse environments.",
      "ai_keywords": [
        "world simulator",
        "video generation",
        "world model",
        "long-term memory",
        "real-time interactivity"
      ]
    }
  },
  {
    "id": "e77f7ec067148dac222fa8934786f0dd",
    "source": "huggingface",
    "type": "paper",
    "title": "WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning",
    "description": "Recent advancements in Large Language Models (LLMs) have largely focused on depth scaling, where a single agent solves long-horizon problems with multi-turn reasoning and tool use. However, as tasks g...<br/>Upvotes: 93<br/>GitHub Stars: 2492<br/>Authors: Zelai Xu, Zhexuan Xu, Ruize Zhang<br/>ðŸ”— <a href=\"https://github.com/RLinf/RLinf\">GitHub</a><br/>ðŸ”— <a href=\"https://wideseek-r1.github.io/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2602.04634\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2602.04634",
    "external_url": "https://github.com/RLinf/RLinf",
    "published_date": "2026-02-04T10:05:12.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2602.04634",
      "upvotes": 93,
      "github_stars": 2492,
      "github_url": "https://github.com/RLinf/RLinf",
      "project_url": "https://wideseek-r1.github.io/",
      "authors": [
        "Zelai Xu",
        "Zhexuan Xu",
        "Ruize Zhang",
        "Chunyang Zhu",
        "Shi Yu",
        "Weilin Liu",
        "Quanlu Zhang",
        "Wenbo Ding",
        "Chao Yu",
        "Yu Wang"
      ],
      "summary": "Recent advancements in Large Language Models (LLMs) have largely focused on depth scaling, where a single agent solves long-horizon problems with multi-turn reasoning and tool use. However, as tasks grow broader, the key bottleneck shifts from individual competence to organizational capability. In this work, we explore a complementary dimension of width scaling with multi-agent systems to address broad information seeking. Existing multi-agent systems often rely on hand-crafted workflows and turn-taking interactions that fail to parallelize work effectively. To bridge this gap, we propose WideSeek-R1, a lead-agent-subagent framework trained via multi-agent reinforcement learning (MARL) to synergize scalable orchestration and parallel execution. By utilizing a shared LLM with isolated contexts and specialized tools, WideSeek-R1 jointly optimizes the lead agent and parallel subagents on a curated dataset of 20k broad information-seeking tasks. Extensive experiments show that WideSeek-R1-4B achieves an item F1 score of 40.0% on the WideSearch benchmark, which is comparable to the performance of single-agent DeepSeek-R1-671B. Furthermore, WideSeek-R1-4B exhibits consistent performance gains as the number of parallel subagents increases, highlighting the effectiveness of width scaling.",
      "fetch_date": "2026-02-05",
      "num_comments": 4,
      "ai_summary": "Multi-agent systems using reinforcement learning enable parallel information seeking with scalable orchestration, achieving performance comparable to larger single agents.",
      "ai_keywords": [
        "Large Language Models",
        "multi-agent systems",
        "multi-agent reinforcement learning",
        "lead-agent-subagent framework",
        "parallel execution",
        "information seeking",
        "WideSearch benchmark",
        "F1 score"
      ]
    }
  },
  {
    "id": "b157736c15947e5fb8a4be69ee79b1ab",
    "source": "huggingface",
    "type": "paper",
    "title": "GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning",
    "description": "Vision-language-action (VLA) models that directly predict multi-step action chunks from current observations face inherent limitations due to constrained scene understanding and weak future anticipati...<br/>Upvotes: 55<br/>GitHub Stars: 2323<br/>Authors: GigaBrain Team, Boyuan Wang, Chaojun Ni<br/>ðŸ”— <a href=\"https://github.com/open-gigaai/giga-brain-0\">GitHub</a><br/>ðŸ”— <a href=\"https://gigabrain05m.github.io/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2602.12099\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2602.12099",
    "external_url": "https://github.com/open-gigaai/giga-brain-0",
    "published_date": "2026-02-12T10:55:19.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2602.12099",
      "upvotes": 55,
      "github_stars": 2323,
      "github_url": "https://github.com/open-gigaai/giga-brain-0",
      "project_url": "https://gigabrain05m.github.io/",
      "authors": [
        "GigaBrain Team",
        "Boyuan Wang",
        "Chaojun Ni",
        "Guan Huang",
        "Guosheng Zhao",
        "Hao Li",
        "Jie Li",
        "Jindi Lv",
        "Jingyu Liu",
        "Lv Feng",
        "Mingming Yu",
        "Peng Li",
        "Qiuping Deng",
        "Tianze Liu",
        "Xinyu Zhou",
        "Xinze Chen",
        "Xiaofeng Wang",
        "Yang Wang",
        "Yifan Li",
        "Yifei Nie",
        "Yilong Li",
        "Yukun Zhou",
        "Yun Ye",
        "Zhichao Liu",
        "Zheng Zhu"
      ],
      "summary": "Vision-language-action (VLA) models that directly predict multi-step action chunks from current observations face inherent limitations due to constrained scene understanding and weak future anticipation capabilities. In contrast, video world models pre-trained on web-scale video corpora exhibit robust spatiotemporal reasoning and accurate future prediction, making them a natural foundation for enhancing VLA learning. Therefore, we propose GigaBrain-0.5M*, a VLA model trained via world model-based reinforcement learning. Built upon GigaBrain-0.5, which is pre-trained on over 10,000 hours of robotic manipulation data, whose intermediate version currently ranks first on the international RoboChallenge benchmark. GigaBrain-0.5M* further integrates world model-based reinforcement learning via RAMP (Reinforcement leArning via world Model-conditioned Policy) to enable robust cross-task adaptation. Empirical results demonstrate that RAMP achieves substantial performance gains over the RECAP baseline, yielding improvements of approximately 30\\% on challenging tasks including Laundry Folding, Box Packing, and Espresso Preparation. Critically, GigaBrain-0.5M^* exhibits reliable long-horizon execution, consistently accomplishing complex manipulation tasks without failure as validated by real-world deployment videos on our https://gigabrain05m.github.io{project page}.",
      "fetch_date": "2026-02-13",
      "num_comments": 2,
      "ai_summary": "A vision-language-action model enhanced with world model-based reinforcement learning demonstrates improved performance and long-horizon execution capabilities for robotic manipulation tasks.",
      "ai_keywords": [
        "Vision-language-action models",
        "world models",
        "reinforcement learning",
        "cross-task adaptation",
        "RAMP",
        "RoboChallenge benchmark",
        "robotic manipulation"
      ]
    }
  },
  {
    "id": "886e15bc16a3ed2f216c7a525f3e9086",
    "source": "huggingface",
    "type": "paper",
    "title": "DeepSeek-OCR 2: Visual Causal Flow",
    "description": "We present DeepSeek-OCR 2 to investigate the feasibility of a novel encoder-DeepEncoder V2-capable of dynamically reordering visual tokens upon image semantics. Conventional vision-language models (VL...<br/>Upvotes: 60<br/>GitHub Stars: 2292<br/>Authors: Haoran Wei, Yaofeng Sun, Yukun Li<br/>ðŸ”— <a href=\"https://github.com/deepseek-ai/DeepSeek-OCR-2\">GitHub</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2601.20552\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2601.20552",
    "external_url": "https://github.com/deepseek-ai/DeepSeek-OCR-2",
    "published_date": "2026-01-28T07:46:07.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2601.20552",
      "upvotes": 60,
      "github_stars": 2292,
      "github_url": "https://github.com/deepseek-ai/DeepSeek-OCR-2",
      "project_url": "",
      "authors": [
        "Haoran Wei",
        "Yaofeng Sun",
        "Yukun Li"
      ],
      "summary": "We present DeepSeek-OCR 2 to investigate the feasibility of a novel encoder-DeepEncoder V2-capable of dynamically reordering visual tokens upon image semantics. Conventional vision-language models (VLMs) invariably process visual tokens in a rigid raster-scan order (top-left to bottom-right) with fixed positional encoding when fed into LLMs. However, this contradicts human visual perception, which follows flexible yet semantically coherent scanning patterns driven by inherent logical structures. Particularly for images with complex layouts, human vision exhibits causally-informed sequential processing. Inspired by this cognitive mechanism, DeepEncoder V2 is designed to endow the encoder with causal reasoning capabilities, enabling it to intelligently reorder visual tokens prior to LLM-based content interpretation. This work explores a novel paradigm: whether 2D image understanding can be effectively achieved through two-cascaded 1D causal reasoning structures, thereby offering a new architectural approach with the potential to achieve genuine 2D reasoning. Codes and model weights are publicly accessible at http://github.com/deepseek-ai/DeepSeek-OCR-2.",
      "fetch_date": "2026-01-29",
      "num_comments": 4,
      "ai_summary": "DeepSeek-OCR 2 introduces DeepEncoder V2 that dynamically reorders visual tokens based on semantic content, enabling more human-like causal reasoning in 2D image understanding through cascaded 1D causal structures.",
      "ai_keywords": [
        "encoder-DeepEncoder V2",
        "visual tokens",
        "vision-language models",
        "raster-scan order",
        "positional encoding",
        "causal reasoning",
        "2D image understanding",
        "cascaded 1D causal reasoning"
      ]
    }
  }
]