[
  {
    "id": "3d55a02b14bdc7a04256ce32475eb075",
    "source": "huggingface",
    "type": "paper",
    "title": "Less is More: Recursive Reasoning with Tiny Networks",
    "description": "Hierarchical Reasoning Model (HRM) is a novel approach using two small neural\nnetworks recursing at different frequencies. This biologically inspired method\nbeats Large Language models (LLMs) on hard ...<br/>Upvotes: 461<br/>GitHub Stars: 5336<br/>Authors: Alexia Jolicoeur-Martineau<br/>ðŸ”— <a href=\"https://github.com/SamsungSAILMontreal/TinyRecursiveModels\">GitHub</a><br/>ðŸ”— <a href=\"https://alexiajm.github.io/2025/09/29/tiny_recursive_models.html#\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2510.04871\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2510.04871",
    "external_url": "https://github.com/SamsungSAILMontreal/TinyRecursiveModels",
    "published_date": "2025-10-06T10:58:08.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2510.04871",
      "upvotes": 461,
      "github_stars": 5336,
      "github_url": "https://github.com/SamsungSAILMontreal/TinyRecursiveModels",
      "project_url": "https://alexiajm.github.io/2025/09/29/tiny_recursive_models.html#",
      "authors": [
        "Alexia Jolicoeur-Martineau"
      ],
      "summary": "Hierarchical Reasoning Model (HRM) is a novel approach using two small neural\nnetworks recursing at different frequencies. This biologically inspired method\nbeats Large Language models (LLMs) on hard puzzle tasks such as Sudoku, Maze,\nand ARC-AGI while trained with small models (27M parameters) on small data\n(around 1000 examples). HRM holds great promise for solving hard problems with\nsmall networks, but it is not yet well understood and may be suboptimal. We\npropose Tiny Recursive Model (TRM), a much simpler recursive reasoning approach\nthat achieves significantly higher generalization than HRM, while using a\nsingle tiny network with only 2 layers. With only 7M parameters, TRM obtains\n45% test-accuracy on ARC-AGI-1 and 8% on ARC-AGI-2, higher than most LLMs\n(e.g., Deepseek R1, o3-mini, Gemini 2.5 Pro) with less than 0.01% of the\nparameters.",
      "fetch_date": "2025-10-08",
      "num_comments": 41,
      "ai_summary": "Tiny Recursive Model (TRM) achieves high generalization on complex puzzle tasks using a small, two-layer network with minimal parameters, outperforming larger language models.",
      "ai_keywords": [
        "Hierarchical Reasoning Model",
        "HRM",
        "Tiny Recursive Model",
        "TRM",
        "recursive reasoning",
        "neural networks",
        "ARC-AGI",
        "Deepseek R1",
        "o3-mini",
        "Gemini 2.5 Pro"
      ]
    }
  },
  {
    "id": "b5eadc2228eda785fe3e8da98ea25174",
    "source": "huggingface",
    "type": "paper",
    "title": "Agent Learning via Early Experience",
    "description": "A long-term goal of language agents is to learn and improve through their own\nexperience, ultimately outperforming humans in complex, real-world tasks.\nHowever, training agents from experience data wi...<br/>Upvotes: 255<br/>Authors: Kai Zhang, Xiangchao Chen, Bo Liu<br/>ðŸ”— <a href=\"https://huggingface.co/papers/2510.08558\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2510.08558",
    "external_url": "",
    "published_date": "2025-10-09T13:59:17.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2510.08558",
      "upvotes": 255,
      "github_stars": 0,
      "github_url": "",
      "project_url": "",
      "authors": [
        "Kai Zhang",
        "Xiangchao Chen",
        "Bo Liu",
        "Tianci Xue",
        "Zeyi Liao",
        "Zhihan Liu",
        "Xiyao Wang",
        "Yuting Ning",
        "Zhaorun Chen",
        "Xiaohan Fu",
        "Jian Xie",
        "Yuxuan Sun",
        "Boyu Gou",
        "Qi Qi",
        "Zihang Meng",
        "Jianwei Yang",
        "Ning Zhang",
        "Xian Li",
        "Ashish Shah",
        "Dat Huynh",
        "Hengduo Li",
        "Zi Yang",
        "Sara Cao",
        "Lawrence Jang",
        "Shuyan Zhou",
        "Jiacheng Zhu",
        "Huan Sun",
        "Jason Weston",
        "Yu Su",
        "Yifan Wu"
      ],
      "summary": "A long-term goal of language agents is to learn and improve through their own\nexperience, ultimately outperforming humans in complex, real-world tasks.\nHowever, training agents from experience data with reinforcement learning\nremains difficult in many environments, which either lack verifiable rewards\n(e.g., websites) or require inefficient long-horizon rollouts (e.g., multi-turn\ntool use). As a result, most current agents rely on supervised fine-tuning on\nexpert data, which is challenging to scale and generalizes poorly. This\nlimitation stems from the nature of expert demonstrations: they capture only a\nnarrow range of scenarios and expose the agent to limited environment\ndiversity. We address this limitation with a middle-ground paradigm we call\nearly experience: interaction data generated by the agent's own actions, where\nthe resulting future states serve as supervision without reward signals. Within\nthis paradigm we study two strategies of using such data: (1) Implicit world\nmodeling, which uses collected states to ground the policy in environment\ndynamics; and (2) Self-reflection, where the agent learns from its suboptimal\nactions to improve reasoning and decision-making. We evaluate across eight\ndiverse environments and multiple model families. Our approaches consistently\nimprove effectiveness and out-of-domain generalization, highlighting the value\nof early experience. Moreover, in environments with verifiable rewards, our\nresults provide promising signals that early experience offers a strong\nfoundation for subsequent reinforcement learning, positioning it as a practical\nbridge between imitation learning and fully experience-driven agents.",
      "fetch_date": "2025-10-10",
      "num_comments": 10,
      "ai_summary": "Early experience, using agent-generated interaction data without reward signals, improves policy effectiveness and generalization, serving as a bridge between imitation learning and reinforcement learning.",
      "ai_keywords": [
        "reinforcement learning",
        "early experience",
        "implicit world modeling",
        "self-reflection",
        "out-of-domain generalization"
      ]
    }
  },
  {
    "id": "8c7bb673a7c23605ce494554bc937b12",
    "source": "huggingface",
    "type": "paper",
    "title": "Scaling Latent Reasoning via Looped Language Models",
    "description": "Modern LLMs are trained to \"think\" primarily via explicit text generation,\nsuch as chain-of-thought (CoT), which defers reasoning to post-training and\nunder-leverages pre-training data. We present and...<br/>Upvotes: 173<br/>Authors: Rui-Jie Zhu, Zixuan Wang, Kai Hua<br/>ðŸ”— <a href=\"https://ouro-llm.github.io/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2510.25741\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2510.25741",
    "external_url": "",
    "published_date": "2025-10-29T13:45:42.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2510.25741",
      "upvotes": 173,
      "github_stars": 0,
      "github_url": "",
      "project_url": "https://ouro-llm.github.io/",
      "authors": [
        "Rui-Jie Zhu",
        "Zixuan Wang",
        "Kai Hua",
        "Tianyu Zhang",
        "Ziniu Li",
        "Haoran Que",
        "Boyi Wei",
        "Zixin Wen",
        "Fan Yin",
        "He Xing",
        "Lu Li",
        "Jiajun Shi",
        "Kaijing Ma",
        "Shanda Li",
        "Taylor Kergan",
        "Andrew Smith",
        "Xingwei Qu",
        "Mude Hui",
        "Bohong Wu",
        "Qiyang Min",
        "Hongzhi Huang",
        "Xun Zhou",
        "Wei Ye",
        "Jiaheng Liu",
        "Jian Yang",
        "Yunfeng Shi",
        "Chenghua Lin",
        "Enduo Zhao",
        "Tianle Cai",
        "Ge Zhang",
        "Wenhao Huang",
        "Yoshua Bengio",
        "Jason Eshraghian"
      ],
      "summary": "Modern LLMs are trained to \"think\" primarily via explicit text generation,\nsuch as chain-of-thought (CoT), which defers reasoning to post-training and\nunder-leverages pre-training data. We present and open-source Ouro, named after\nthe recursive Ouroboros, a family of pre-trained Looped Language Models\n(LoopLM) that instead build reasoning into the pre-training phase through (i)\niterative computation in latent space, (ii) an entropy-regularized objective\nfor learned depth allocation, and (iii) scaling to 7.7T tokens. Ouro 1.4B and\n2.6B models enjoy superior performance that match the results of up to 12B SOTA\nLLMs across a wide range of benchmarks. Through controlled experiments, we show\nthis advantage stems not from increased knowledge capacity, but from superior\nknowledge manipulation capabilities. We also show that LoopLM yields reasoning\ntraces more aligned with final outputs than explicit CoT. We hope our results\nshow the potential of LoopLM as a novel scaling direction in the reasoning era.\nOur model could be found in: http://ouro-llm.github.io.",
      "fetch_date": "2025-10-30",
      "num_comments": 2,
      "ai_summary": "",
      "ai_keywords": []
    }
  },
  {
    "id": "f5ecd82d2d6a90e1b4eb7c0d2afe9c3e",
    "source": "huggingface",
    "type": "paper",
    "title": "QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning\n  for LLMs",
    "description": "We propose QeRL, a Quantization-enhanced Reinforcement Learning framework for\nlarge language models (LLMs). While RL is essential for LLMs' reasoning\ncapabilities, it is resource-intensive, requiring ...<br/>Upvotes: 168<br/>GitHub Stars: 384<br/>Authors: Wei Huang, Yi Ge, Shuai Yang<br/>ðŸ”— <a href=\"https://github.com/NVlabs/QeRL\">GitHub</a><br/>ðŸ”— <a href=\"https://github.com/NVlabs/QeRL\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2510.11696\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2510.11696",
    "external_url": "https://github.com/NVlabs/QeRL",
    "published_date": "2025-10-13T13:55:09.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2510.11696",
      "upvotes": 168,
      "github_stars": 384,
      "github_url": "https://github.com/NVlabs/QeRL",
      "project_url": "https://github.com/NVlabs/QeRL",
      "authors": [
        "Wei Huang",
        "Yi Ge",
        "Shuai Yang",
        "Yicheng Xiao",
        "Huizi Mao",
        "Yujun Lin",
        "Hanrong Ye",
        "Sifei Liu",
        "Ka Chun Cheung",
        "Hongxu Yin",
        "Yao Lu",
        "Xiaojuan Qi",
        "Song Han",
        "Yukang Chen"
      ],
      "summary": "We propose QeRL, a Quantization-enhanced Reinforcement Learning framework for\nlarge language models (LLMs). While RL is essential for LLMs' reasoning\ncapabilities, it is resource-intensive, requiring substantial GPU memory and\nlong rollout durations. QeRL addresses these issues by combining NVFP4\nquantization with Low-Rank Adaptation (LoRA), accelerating rollout phase of RL\nwhile reducing memory overhead. Beyond efficiency, our findings show that\nquantization noise increases policy entropy, enhancing exploration, and\nenabling the discovery of better strategies during RL. To further optimize\nexploration, QeRL introduces an Adaptive Quantization Noise (AQN) mechanism,\nwhich dynamically adjusts noise during training. Experiments demonstrate that\nQeRL delivers over 1.5 times speedup in the rollout phase. Moreover, this is\nthe first framework to enable RL training of a 32B LLM on a single H100 80GB\nGPU, while delivering overall speedups for RL training. It also achieves faster\nreward growth and higher final accuracy than 16-bit LoRA and QLoRA, while\nmatching the performance of full-parameter fine-tuning on mathematical\nbenchmarks such as GSM8K (90.8%) and MATH 500 (77.4%) in the 7B model. These\nresults establish QeRL as an efficient and effective framework for RL training\nin LLMs.",
      "fetch_date": "2025-10-14",
      "num_comments": 4,
      "ai_summary": "QeRL, a quantization-enhanced reinforcement learning framework, accelerates RL training for large language models by combining NVFP4 quantization with Low-Rank Adaptation and an Adaptive Quantization Noise mechanism, achieving significant speedups and improved performance.",
      "ai_keywords": [
        "NVFP4 quantization",
        "Low-Rank Adaptation (LoRA)",
        "Adaptive Quantization Noise (AQN)",
        "reinforcement learning",
        "large language models (LLMs)",
        "rollout phase",
        "policy entropy",
        "exploration",
        "reward growth",
        "GSM8K",
        "MATH 500"
      ]
    }
  },
  {
    "id": "9f97533076b36bd588d78f78c4352369",
    "source": "huggingface",
    "type": "paper",
    "title": "Concerto: Joint 2D-3D Self-Supervised Learning Emerges Spatial\n  Representations",
    "description": "Humans learn abstract concepts through multisensory synergy, and once formed,\nsuch representations can often be recalled from a single modality. Inspired by\nthis principle, we introduce Concerto, a mi...<br/>Upvotes: 166<br/>GitHub Stars: 2592<br/>Authors: Yujia Zhang, Xiaoyang Wu, Yixing Lao<br/>ðŸ”— <a href=\"https://github.com/Pointcept/Pointcept\">GitHub</a><br/>ðŸ”— <a href=\"https://pointcept.github.io/Concerto/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2510.23607\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2510.23607",
    "external_url": "https://github.com/Pointcept/Pointcept",
    "published_date": "2025-10-27T13:59:59.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2510.23607",
      "upvotes": 166,
      "github_stars": 2592,
      "github_url": "https://github.com/Pointcept/Pointcept",
      "project_url": "https://pointcept.github.io/Concerto/",
      "authors": [
        "Yujia Zhang",
        "Xiaoyang Wu",
        "Yixing Lao",
        "Chengyao Wang",
        "Zhuotao Tian",
        "Naiyan Wang",
        "Hengshuang Zhao"
      ],
      "summary": "Humans learn abstract concepts through multisensory synergy, and once formed,\nsuch representations can often be recalled from a single modality. Inspired by\nthis principle, we introduce Concerto, a minimalist simulation of human concept\nlearning for spatial cognition, combining 3D intra-modal self-distillation with\n2D-3D cross-modal joint embedding. Despite its simplicity, Concerto learns more\ncoherent and informative spatial features, as demonstrated by zero-shot\nvisualizations. It outperforms both standalone SOTA 2D and 3D self-supervised\nmodels by 14.2% and 4.8%, respectively, as well as their feature concatenation,\nin linear probing for 3D scene perception. With full fine-tuning, Concerto sets\nnew SOTA results across multiple scene understanding benchmarks (e.g., 80.7%\nmIoU on ScanNet). We further present a variant of Concerto tailored for\nvideo-lifted point cloud spatial understanding, and a translator that linearly\nprojects Concerto representations into CLIP's language space, enabling\nopen-world perception. These results highlight that Concerto emerges spatial\nrepresentations with superior fine-grained geometric and semantic consistency.",
      "fetch_date": "2025-10-28",
      "num_comments": 4,
      "ai_summary": "Concerto, a minimalist model combining 3D self-distillation and 2D-3D joint embedding, achieves superior spatial feature learning and outperforms existing models in scene understanding and open-world perception.",
      "ai_keywords": [
        "3D intra-modal self-distillation",
        "2D-3D cross-modal joint embedding",
        "zero-shot visualizations",
        "linear probing",
        "3D scene perception",
        "ScanNet",
        "mIoU",
        "video-lifted point cloud",
        "CLIP's language space",
        "open-world perception",
        "fine-grained geometric and semantic consistency"
      ]
    }
  },
  {
    "id": "1f97c06c73ceb6a2aa9ee2bb5f344034",
    "source": "huggingface",
    "type": "paper",
    "title": "Diffusion Transformers with Representation Autoencoders",
    "description": "Latent generative modeling, where a pretrained autoencoder maps pixels into a\nlatent space for the diffusion process, has become the standard strategy for\nDiffusion Transformers (DiT); however, the au...<br/>Upvotes: 160<br/>GitHub Stars: 1437<br/>Authors: Boyang Zheng, Nanye Ma, Shengbang Tong<br/>ðŸ”— <a href=\"https://github.com/bytetriper/RAE\">GitHub</a><br/>ðŸ”— <a href=\"https://rae-dit.github.io/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2510.11690\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2510.11690",
    "external_url": "https://github.com/bytetriper/RAE",
    "published_date": "2025-10-13T13:51:39.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2510.11690",
      "upvotes": 160,
      "github_stars": 1437,
      "github_url": "https://github.com/bytetriper/RAE",
      "project_url": "https://rae-dit.github.io/",
      "authors": [
        "Boyang Zheng",
        "Nanye Ma",
        "Shengbang Tong",
        "Saining Xie"
      ],
      "summary": "Latent generative modeling, where a pretrained autoencoder maps pixels into a\nlatent space for the diffusion process, has become the standard strategy for\nDiffusion Transformers (DiT); however, the autoencoder component has barely\nevolved. Most DiTs continue to rely on the original VAE encoder, which\nintroduces several limitations: outdated backbones that compromise\narchitectural simplicity, low-dimensional latent spaces that restrict\ninformation capacity, and weak representations that result from purely\nreconstruction-based training and ultimately limit generative quality. In this\nwork, we explore replacing the VAE with pretrained representation encoders\n(e.g., DINO, SigLIP, MAE) paired with trained decoders, forming what we term\nRepresentation Autoencoders (RAEs). These models provide both high-quality\nreconstructions and semantically rich latent spaces, while allowing for a\nscalable transformer-based architecture. Since these latent spaces are\ntypically high-dimensional, a key challenge is enabling diffusion transformers\nto operate effectively within them. We analyze the sources of this difficulty,\npropose theoretically motivated solutions, and validate them empirically. Our\napproach achieves faster convergence without auxiliary representation alignment\nlosses. Using a DiT variant equipped with a lightweight, wide DDT head, we\nachieve strong image generation results on ImageNet: 1.51 FID at 256x256 (no\nguidance) and 1.13 at both 256x256 and 512x512 (with guidance). RAE offers\nclear advantages and should be the new default for diffusion transformer\ntraining.",
      "fetch_date": "2025-10-14",
      "num_comments": 5,
      "ai_summary": "Replacing VAEs with pretrained representation encoders in Diffusion Transformers enhances generative quality and convergence speed without auxiliary losses.",
      "ai_keywords": [
        "Latent generative modeling",
        "Diffusion Transformers (DiT)",
        "VAE encoder",
        "Representation Autoencoders (RAEs)",
        "pretrained representation encoders",
        "DINO",
        "SigLIP",
        "MAE",
        "high-dimensional latent spaces",
        "transformer-based architecture",
        "diffusion transformers",
        "image generation",
        "ImageNet",
        "FID"
      ]
    }
  },
  {
    "id": "d98bf11f4930fd1159bb5f90e3f76afd",
    "source": "huggingface",
    "type": "paper",
    "title": "PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B\n  Ultra-Compact Vision-Language Model",
    "description": "In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model\ntailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a\ncompact yet powerful vision-language model (VLM) ...<br/>Upvotes: 78<br/>GitHub Stars: 62204<br/>Authors: Cheng Cui, Ting Sun, Suyin Liang<br/>ðŸ”— <a href=\"https://github.com/PaddlePaddle/PaddleOCR\">GitHub</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2510.14528\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2510.14528",
    "external_url": "https://github.com/PaddlePaddle/PaddleOCR",
    "published_date": "2025-10-16T06:18:48.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2510.14528",
      "upvotes": 78,
      "github_stars": 62204,
      "github_url": "https://github.com/PaddlePaddle/PaddleOCR",
      "project_url": "",
      "authors": [
        "Cheng Cui",
        "Ting Sun",
        "Suyin Liang",
        "Tingquan Gao",
        "Zelun Zhang",
        "Jiaxuan Liu",
        "Xueqing Wang",
        "Changda Zhou",
        "Hongen Liu",
        "Manhui Lin",
        "Yue Zhang",
        "Yubo Zhang",
        "Handong Zheng",
        "Jing Zhang",
        "Jun Zhang",
        "Yi Liu",
        "Dianhai Yu",
        "Yanjun Ma"
      ],
      "summary": "In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model\ntailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a\ncompact yet powerful vision-language model (VLM) that integrates a NaViT-style\ndynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to\nenable accurate element recognition. This innovative model efficiently supports\n109 languages and excels in recognizing complex elements (e.g., text, tables,\nformulas, and charts), while maintaining minimal resource consumption. Through\ncomprehensive evaluations on widely used public benchmarks and in-house\nbenchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document\nparsing and element-level recognition. It significantly outperforms existing\nsolutions, exhibits strong competitiveness against top-tier VLMs, and delivers\nfast inference speeds. These strengths make it highly suitable for practical\ndeployment in real-world scenarios.",
      "fetch_date": "2025-10-17",
      "num_comments": 5,
      "ai_summary": "PaddleOCR-VL, a vision-language model combining NaViT-style visual encoder and ERNIE-4.5 language model, achieves state-of-the-art performance in document parsing with minimal resource consumption.",
      "ai_keywords": [
        "vision-language model",
        "NaViT-style",
        "dynamic resolution visual encoder",
        "ERNIE-4.5",
        "element recognition",
        "page-level document parsing",
        "element-level recognition",
        "inference speeds"
      ]
    }
  },
  {
    "id": "4477d6ceb0f6af4c535f87fb4c3e55f5",
    "source": "huggingface",
    "type": "paper",
    "title": "BitNet Distillation",
    "description": "In this paper, we present BitNet Distillation (BitDistill), a lightweight\npipeline that fine-tunes off-the-shelf full-precision LLMs (e.g., Qwen) into\n1.58-bit precision (i.e., ternary weights {-1, 0,...<br/>Upvotes: 52<br/>GitHub Stars: 24332<br/>Authors: Xun Wu, Shaohan Huang, Wenhui Wang<br/>ðŸ”— <a href=\"https://github.com/microsoft/BitNet\">GitHub</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2510.13998\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2510.13998",
    "external_url": "https://github.com/microsoft/BitNet",
    "published_date": "2025-10-15T14:28:12.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2510.13998",
      "upvotes": 52,
      "github_stars": 24332,
      "github_url": "https://github.com/microsoft/BitNet",
      "project_url": "",
      "authors": [
        "Xun Wu",
        "Shaohan Huang",
        "Wenhui Wang",
        "Ting Song",
        "Li Dong",
        "Yan Xia",
        "Furu Wei"
      ],
      "summary": "In this paper, we present BitNet Distillation (BitDistill), a lightweight\npipeline that fine-tunes off-the-shelf full-precision LLMs (e.g., Qwen) into\n1.58-bit precision (i.e., ternary weights {-1, 0, 1}) for specific downstream\ntasks, achieving strong task-specific performance with minimal computational\ncost. Specifically, BitDistill incorporates three key techniques: the SubLN\nmodule, as introduced in BitNet; multi-head attention distillation, based on\nMiniLM; and continual pre-training, which serves as a crucial warm-up step to\nmitigate the scalability issue of the performance gap between finetuned\nfull-precision and 1.58-bit LLMs on specific tasks. Experimental results show\nthat BitDistill achieves performance comparable to the full-precision\ncounterpart models across model size, while enabling up to 10x memory savings\nand 2.65x faster inference on CPUs. Code is available at\nhttps://github.com/microsoft/BitNet.",
      "fetch_date": "2025-10-17",
      "num_comments": 4,
      "ai_summary": "BitNet Distillation fine-tunes large language models to 1.58-bit precision using SubLN, multi-head attention distillation, and continual pre-training, achieving comparable performance with significant memory and inference speed improvements.",
      "ai_keywords": [
        "BitNet Distillation",
        "BitDistill",
        "SubLN",
        "multi-head attention distillation",
        "continual pre-training",
        "LLMs",
        "Qwen",
        "ternary weights",
        "memory savings",
        "inference speed"
      ]
    }
  },
  {
    "id": "bf88f955951ceb1c5ba0121481fe4bf5",
    "source": "huggingface",
    "type": "paper",
    "title": "FAPO: Flawed-Aware Policy Optimization for Efficient and Reliable\n  Reasoning",
    "description": "Reinforcement learning with verifiable rewards (RLVR) has emerged as a\npromising paradigm for enhancing the reasoning capabilities of large language\nmodels (LLMs). In this context, models explore reas...<br/>Upvotes: 4<br/>GitHub Stars: 14990<br/>Authors: Yuyang Ding, Chi Zhang, Juntao Li<br/>ðŸ”— <a href=\"https://github.com/volcengine/verl/tree/main/recipe/fapo\">GitHub</a><br/>ðŸ”— <a href=\"https://fapo-rl.github.io/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2510.22543\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2510.22543",
    "external_url": "https://github.com/volcengine/verl/tree/main/recipe/fapo",
    "published_date": "2025-10-26T01:49:38.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2510.22543",
      "upvotes": 4,
      "github_stars": 14990,
      "github_url": "https://github.com/volcengine/verl/tree/main/recipe/fapo",
      "project_url": "https://fapo-rl.github.io/",
      "authors": [
        "Yuyang Ding",
        "Chi Zhang",
        "Juntao Li",
        "Haibin Lin",
        "Xin Liu",
        "Min Zhang"
      ],
      "summary": "Reinforcement learning with verifiable rewards (RLVR) has emerged as a\npromising paradigm for enhancing the reasoning capabilities of large language\nmodels (LLMs). In this context, models explore reasoning trajectories and\nexploit rollouts with correct answers as positive signals for policy\noptimization. However, these rollouts might involve flawed patterns such as\nanswer-guessing and jump-in-reasoning. Such flawed-positive rollouts are\nrewarded identically to fully correct ones, causing policy models to\ninternalize these unreliable reasoning patterns. In this work, we first conduct\na systematic study of flawed-positive rollouts in RL and find that they enable\nrapid capability gains during the early optimization stage, while constraining\nreasoning capability later by reinforcing unreliable patterns. Building on\nthese insights, we propose Flawed-Aware Policy Optimization (FAPO), which\npresents a parameter-free reward penalty for flawed-positive rollouts, enabling\nthe policy to leverage them as useful shortcuts in the warm-up stage, securing\nstable early gains, while gradually shifting optimization toward reliable\nreasoning in the later refinement stage. To accurately and comprehensively\ndetect flawed-positive rollouts, we introduce a generative reward model (GenRM)\nwith a process-level reward that precisely localizes reasoning errors.\nExperiments show that FAPO is effective in broad domains, improving outcome\ncorrectness, process reliability, and training stability without increasing the\ntoken budget.",
      "fetch_date": "2025-10-30",
      "num_comments": 1,
      "ai_summary": "Flawed-Aware Policy Optimization (FAPO) improves reinforcement learning with verifiable rewards by penalizing flawed-positive rollouts, enhancing reasoning capability and training stability without increasing computational cost.",
      "ai_keywords": [
        "reinforcement learning",
        "verifiable rewards",
        "large language models",
        "reasoning trajectories",
        "policy optimization",
        "flawed-positive rollouts",
        "parameter-free reward penalty",
        "generative reward model",
        "process-level reward",
        "reasoning errors",
        "outcome correctness",
        "process reliability",
        "training stability"
      ]
    }
  },
  {
    "id": "2fbd109d587f03077296ef20d0d7459e",
    "source": "huggingface",
    "type": "paper",
    "title": "JoyAgent-JDGenie: Technical Report on the GAIA",
    "description": "Large Language Models are increasingly deployed as autonomous agents for\ncomplex real-world tasks, yet existing systems often focus on isolated\nimprovements without a unifying design for robustness an...<br/>Upvotes: 3<br/>GitHub Stars: 10610<br/>Authors: Jiarun Liu, Shiyue Xu, Shangkun Liu<br/>ðŸ”— <a href=\"https://github.com/jd-opensource/joyagent-jdgenie\">GitHub</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2510.00510\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2510.00510",
    "external_url": "https://github.com/jd-opensource/joyagent-jdgenie",
    "published_date": "2025-10-01T00:41:58.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2510.00510",
      "upvotes": 3,
      "github_stars": 10610,
      "github_url": "https://github.com/jd-opensource/joyagent-jdgenie",
      "project_url": "",
      "authors": [
        "Jiarun Liu",
        "Shiyue Xu",
        "Shangkun Liu",
        "Yang Li",
        "Wen Liu",
        "Min Liu",
        "Xiaoqing Zhou",
        "Hanmin Wang",
        "Shilin Jia",
        "zhen Wang",
        "Shaohua Tian",
        "Hanhao Li",
        "Junbo Zhang",
        "Yongli Yu",
        "Peng Cao",
        "Haofen Wang"
      ],
      "summary": "Large Language Models are increasingly deployed as autonomous agents for\ncomplex real-world tasks, yet existing systems often focus on isolated\nimprovements without a unifying design for robustness and adaptability. We\npropose a generalist agent architecture that integrates three core components:\na collective multi-agent framework combining planning and execution agents with\ncritic model voting, a hierarchical memory system spanning working, semantic,\nand procedural layers, and a refined tool suite for search, code execution, and\nmultimodal parsing. Evaluated on a comprehensive benchmark, our framework\nconsistently outperforms open-source baselines and approaches the performance\nof proprietary systems. These results demonstrate the importance of\nsystem-level integration and highlight a path toward scalable, resilient, and\nadaptive AI assistants capable of operating across diverse domains and tasks.",
      "fetch_date": "2025-10-02",
      "num_comments": 2,
      "ai_summary": "A generalist agent architecture combining multi-agent planning, hierarchical memory, and a refined tool suite outperforms existing systems in diverse tasks.",
      "ai_keywords": [
        "multi-agent framework",
        "planning agents",
        "execution agents",
        "critic model voting",
        "hierarchical memory system",
        "working memory",
        "semantic memory",
        "procedural memory",
        "tool suite",
        "search",
        "code execution",
        "multimodal parsing"
      ]
    }
  },
  {
    "id": "2431e39c5ab567b865fe3f25b69b4d35",
    "source": "huggingface",
    "type": "paper",
    "title": "RAG-Anything: All-in-One RAG Framework",
    "description": "Retrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm\nfor expanding Large Language Models beyond their static training limitations.\nHowever, a critical misalignment exists between...<br/>Upvotes: 42<br/>GitHub Stars: 9817<br/>Authors: Zirui Guo, Xubin Ren, Lingrui Xu<br/>ðŸ”— <a href=\"https://github.com/HKUDS/RAG-Anything\">GitHub</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2510.12323\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2510.12323",
    "external_url": "https://github.com/HKUDS/RAG-Anything",
    "published_date": "2025-10-14T05:25:35.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2510.12323",
      "upvotes": 42,
      "github_stars": 9817,
      "github_url": "https://github.com/HKUDS/RAG-Anything",
      "project_url": "",
      "authors": [
        "Zirui Guo",
        "Xubin Ren",
        "Lingrui Xu",
        "Jiahao Zhang",
        "Chao Huang"
      ],
      "summary": "Retrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm\nfor expanding Large Language Models beyond their static training limitations.\nHowever, a critical misalignment exists between current RAG capabilities and\nreal-world information environments. Modern knowledge repositories are\ninherently multimodal, containing rich combinations of textual content, visual\nelements, structured tables, and mathematical expressions. Yet existing RAG\nframeworks are limited to textual content, creating fundamental gaps when\nprocessing multimodal documents. We present RAG-Anything, a unified framework\nthat enables comprehensive knowledge retrieval across all modalities. Our\napproach reconceptualizes multimodal content as interconnected knowledge\nentities rather than isolated data types. The framework introduces dual-graph\nconstruction to capture both cross-modal relationships and textual semantics\nwithin a unified representation. We develop cross-modal hybrid retrieval that\ncombines structural knowledge navigation with semantic matching. This enables\neffective reasoning over heterogeneous content where relevant evidence spans\nmultiple modalities. RAG-Anything demonstrates superior performance on\nchallenging multimodal benchmarks, achieving significant improvements over\nstate-of-the-art methods. Performance gains become particularly pronounced on\nlong documents where traditional approaches fail. Our framework establishes a\nnew paradigm for multimodal knowledge access, eliminating the architectural\nfragmentation that constrains current systems. Our framework is open-sourced\nat: https://github.com/HKUDS/RAG-Anything.",
      "fetch_date": "2025-10-15",
      "num_comments": 5,
      "ai_summary": "RAG-Anything is a unified framework that enhances multimodal knowledge retrieval by integrating cross-modal relationships and semantic matching, outperforming existing methods on complex benchmarks.",
      "ai_keywords": [
        "Retrieval-Augmented Generation",
        "RAG",
        "Large Language Models",
        "multimodal",
        "textual content",
        "visual elements",
        "structured tables",
        "mathematical expressions",
        "dual-graph construction",
        "cross-modal hybrid retrieval",
        "structural knowledge navigation",
        "semantic matching",
        "multimodal benchmarks",
        "long documents"
      ]
    }
  },
  {
    "id": "5c99dbab179702139c4bd03a805804f8",
    "source": "huggingface",
    "type": "paper",
    "title": "The Unreasonable Effectiveness of Scaling Agents for Computer Use",
    "description": "Computer-use agents (CUAs) hold promise for automating everyday digital\ntasks, but their unreliability and high variance hinder their application to\nlong-horizon, complex tasks. We introduce Behavior ...<br/>Upvotes: 23<br/>GitHub Stars: 7833<br/>Authors: Gonzalo Gonzalez-Pumariega, Vincent Tu, Chih-Lun Lee<br/>ðŸ”— <a href=\"https://github.com/simular-ai/Agent-S/\">GitHub</a><br/>ðŸ”— <a href=\"https://www.simular.ai/articles/agent-s3\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2510.02250\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2510.02250",
    "external_url": "https://github.com/simular-ai/Agent-S/",
    "published_date": "2025-10-02T13:37:08.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2510.02250",
      "upvotes": 23,
      "github_stars": 7833,
      "github_url": "https://github.com/simular-ai/Agent-S/",
      "project_url": "https://www.simular.ai/articles/agent-s3",
      "authors": [
        "Gonzalo Gonzalez-Pumariega",
        "Vincent Tu",
        "Chih-Lun Lee",
        "Jiachen Yang",
        "Ang Li",
        "Xin Eric Wang"
      ],
      "summary": "Computer-use agents (CUAs) hold promise for automating everyday digital\ntasks, but their unreliability and high variance hinder their application to\nlong-horizon, complex tasks. We introduce Behavior Best-of-N (bBoN), a method\nthat scales over agents by generating multiple rollouts and selecting among\nthem using behavior narratives that describe the agents' rollouts. It enables\nboth wide exploration and principled trajectory selection, substantially\nimproving robustness and success rates. On OSWorld, our bBoN scaling method\nestablishes a new state of the art (SoTA) at 69.9%, significantly outperforming\nprior methods and approaching human-level performance at 72%, with\ncomprehensive ablations validating key design choices. We further demonstrate\nstrong generalization results to different operating systems on\nWindowsAgentArena and AndroidWorld. Crucially, our results highlight the\nunreasonable effectiveness of scaling CUAs, when you do it right: effective\nscaling requires structured trajectory understanding and selection, and bBoN\nprovides a practical framework to achieve this.",
      "fetch_date": "2025-10-03",
      "num_comments": 2,
      "ai_summary": "Behavior Best-of-N (bBoN) improves the reliability and success rates of computer-use agents by generating and selecting among multiple rollouts using behavior narratives, achieving state-of-the-art performance on OSWorld and strong generalization to different operating systems.",
      "ai_keywords": [
        "Behavior Best-of-N",
        "bBoN",
        "rollouts",
        "behavior narratives",
        "trajectory selection",
        "OSWorld",
        "WindowsAgentArena",
        "AndroidWorld"
      ]
    }
  }
]