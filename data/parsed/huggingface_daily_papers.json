[
  {
    "id": "2d9fe809957806c0d1ec27e6e37053aa",
    "source": "huggingface",
    "type": "paper",
    "title": "mHC: Manifold-Constrained Hyper-Connections",
    "description": "Recently, studies exemplified by Hyper-Connections (HC) have extended the ubiquitous residual connection paradigm established over the past decade by expanding the residual stream width and diversifyi...<br/>Upvotes: 238<br/>Authors: Zhenda Xie, Yixuan Wei, Huanqi Cao<br/>ðŸ”— <a href=\"https://huggingface.co/papers/2512.24880\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2512.24880",
    "external_url": "",
    "published_date": "2025-12-31T09:16:26.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2512.24880",
      "upvotes": 238,
      "github_stars": 0,
      "github_url": "",
      "project_url": "",
      "authors": [
        "Zhenda Xie",
        "Yixuan Wei",
        "Huanqi Cao",
        "Chenggang Zhao",
        "Chengqi Deng",
        "Jiashi Li",
        "Damai Dai",
        "Huazuo Gao",
        "Jiang Chang",
        "Liang Zhao",
        "Shangyan Zhou",
        "Zhean Xu",
        "Zhengyan Zhang",
        "Wangding Zeng",
        "Shengding Hu",
        "Yuqing Wang",
        "Jingyang Yuan",
        "Lean Wang",
        "Wenfeng Liang"
      ],
      "summary": "Recently, studies exemplified by Hyper-Connections (HC) have extended the ubiquitous residual connection paradigm established over the past decade by expanding the residual stream width and diversifying connectivity patterns. While yielding substantial performance gains, this diversification fundamentally compromises the identity mapping property intrinsic to the residual connection, which causes severe training instability and restricted scalability, and additionally incurs notable memory access overhead. To address these challenges, we propose Manifold-Constrained Hyper-Connections (mHC), a general framework that projects the residual connection space of HC onto a specific manifold to restore the identity mapping property, while incorporating rigorous infrastructure optimization to ensure efficiency. Empirical experiments demonstrate that mHC is effective for training at scale, offering tangible performance improvements and superior scalability. We anticipate that mHC, as a flexible and practical extension of HC, will contribute to a deeper understanding of topological architecture design and suggest promising directions for the evolution of foundational models.",
      "fetch_date": "2026-01-01",
      "num_comments": 6,
      "ai_summary": "Manifold-Constrained Hyper-Connections (mHC) stabilize and scale residual connection architectures by restoring identity mapping properties through manifold projection and infrastructure optimization.",
      "ai_keywords": [
        "Hyper-Connections (HC)",
        "Manifold-Constrained Hyper-Connections (mHC)",
        "residual connections",
        "residual stream width",
        "connectivity patterns",
        "identity mapping property",
        "training instability",
        "memory access overhead",
        "manifold projection",
        "infrastructure optimization",
        "scalability"
      ]
    }
  },
  {
    "id": "adcafb2025f84cc88e13db2e967d246f",
    "source": "huggingface",
    "type": "paper",
    "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI",
    "description": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current pra...<br/>Upvotes: 203<br/>GitHub Stars: 2302<br/>Authors: Hao Liang, Xiaochen Ma, Zhou Liu<br/>ðŸ”— <a href=\"https://github.com/OpenDCAI/DataFlow\">GitHub</a><br/>ðŸ”— <a href=\"https://github.com/OpenDCAI/DataFlow\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2512.16676\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2512.16676",
    "external_url": "https://github.com/OpenDCAI/DataFlow",
    "published_date": "2025-12-18T10:46:15.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2512.16676",
      "upvotes": 203,
      "github_stars": 2302,
      "github_url": "https://github.com/OpenDCAI/DataFlow",
      "project_url": "https://github.com/OpenDCAI/DataFlow",
      "authors": [
        "Hao Liang",
        "Xiaochen Ma",
        "Zhou Liu",
        "Zhen Hao Wong",
        "Zhengyang Zhao",
        "Zimo Meng",
        "Runming He",
        "Chengyu Shen",
        "Qifeng Cai",
        "Zhaoyang Han",
        "Meiyi Qiang",
        "Yalin Feng",
        "Tianyi Bai",
        "Zewei Pan",
        "Ziyi Guo",
        "Yizhen Jiang",
        "Jingwen Deng",
        "Qijie You",
        "Peichao Lai",
        "Tianyu Guo",
        "Chi Hsu Tsai",
        "Hengyi Feng",
        "Rui Hu",
        "Wenkai Yu",
        "Junbo Niu",
        "Bohan Zeng",
        "Ruichuan An",
        "Lu Ma",
        "Jihao Huang",
        "Yaowei Zheng",
        "Conghui He",
        "Linpeng Tang",
        "Bin Cui",
        "Weinan E",
        "Wentao Zhang"
      ],
      "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.",
      "fetch_date": "2025-12-23",
      "num_comments": 4,
      "ai_summary": "DataFlow is an LLM-driven data preparation framework that enhances data quality and reproducibility for various tasks, improving LLM performance with automatically generated pipelines.",
      "ai_keywords": [
        "DataFlow",
        "Large Language Models (LLMs)",
        "data preparation pipelines",
        "system-level abstractions",
        "PyTorch-style pipeline construction API",
        "reusable operators",
        "domain-general pipelines",
        "Text-to-SQL",
        "agentic RAG",
        "large-scale knowledge extraction",
        "DataFlow-Agent",
        "operator synthesis",
        "pipeline planning",
        "iterative verification"
      ]
    }
  },
  {
    "id": "8328813bd23f18dae6c4e2c63c9de3c9",
    "source": "huggingface",
    "type": "paper",
    "title": "Kling-Omni Technical Report",
    "description": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bri...<br/>Upvotes: 164<br/>Authors: Kling Team, Jialu Chen, Yuanzheng Ci<br/>ðŸ”— <a href=\"https://huggingface.co/papers/2512.16776\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2512.16776",
    "external_url": "",
    "published_date": "2025-12-18T12:08:12.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2512.16776",
      "upvotes": 164,
      "github_stars": 0,
      "github_url": "",
      "project_url": "",
      "authors": [
        "Kling Team",
        "Jialu Chen",
        "Yuanzheng Ci",
        "Xiangyu Du",
        "Zipeng Feng",
        "Kun Gai",
        "Sainan Guo",
        "Feng Han",
        "Jingbin He",
        "Kang He",
        "Xiao Hu",
        "Xiaohua Hu",
        "Boyuan Jiang",
        "Fangyuan Kong",
        "Hang Li",
        "Jie Li",
        "Qingyu Li",
        "Shen Li",
        "Xiaohan Li",
        "Yan Li",
        "Jiajun Liang",
        "Borui Liao",
        "Yiqiao Liao",
        "Weihong Lin",
        "Quande Liu",
        "Xiaokun Liu",
        "Yilun Liu",
        "Yuliang Liu",
        "Shun Lu",
        "Hangyu Mao",
        "Yunyao Mao",
        "Haodong Ouyang",
        "Wenyu Qin",
        "Wanqi Shi",
        "Xiaoyu Shi",
        "Lianghao Su",
        "Haozhi Sun",
        "Peiqin Sun",
        "Pengfei Wan",
        "Chao Wang",
        "Chenyu Wang",
        "Meng Wang",
        "Qiulin Wang",
        "Runqi Wang",
        "Xintao Wang",
        "Xuebo Wang",
        "Zekun Wang",
        "Min Wei",
        "Tiancheng Wen",
        "Guohao Wu",
        "Xiaoshi Wu",
        "Zhenhua Wu",
        "Da Xie",
        "Yingtong Xiong",
        "Yulong Xu",
        "Sile Yang",
        "Zikang Yang",
        "Weicai Ye",
        "Ziyang Yuan",
        "Shenglong Zhang",
        "Shuaiyu Zhang",
        "Yuanxing Zhang",
        "Yufan Zhang",
        "Wenzheng Zhao",
        "Ruiliang Zhou",
        "Yan Zhou",
        "Guosheng Zhu",
        "Yongjie Zhu"
      ],
      "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.",
      "fetch_date": "2025-12-19",
      "num_comments": 6,
      "ai_summary": "Kling-Omni is a versatile generative framework that synthesizes high-quality videos from multimodal inputs, integrating generation, editing, and reasoning into a unified system.",
      "ai_keywords": [
        "generative framework",
        "multimodal visual language inputs",
        "end-to-end",
        "video generation",
        "editing",
        "intelligent reasoning",
        "unified multimodal representation",
        "cinematic-quality",
        "efficient large-scale pre-training",
        "inference optimizations",
        "in-context generation",
        "reasoning-based editing",
        "multimodal instruction following",
        "multimodal world simulators"
      ]
    }
  },
  {
    "id": "9f022f923749f1889fd7797edcd67136",
    "source": "huggingface",
    "type": "paper",
    "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
    "description": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To ach...<br/>Upvotes: 150<br/>GitHub Stars: 164<br/>Authors: Shih-Yang Liu, Xin Dong, Ximing Lu<br/>ðŸ”— <a href=\"https://github.com/NVlabs/GDPO\">GitHub</a><br/>ðŸ”— <a href=\"https://nvlabs.github.io/GDPO/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2601.05242\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2601.05242",
    "external_url": "https://github.com/NVlabs/GDPO",
    "published_date": "2026-01-08T13:59:24.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2601.05242",
      "upvotes": 150,
      "github_stars": 164,
      "github_url": "https://github.com/NVlabs/GDPO",
      "project_url": "https://nvlabs.github.io/GDPO/",
      "authors": [
        "Shih-Yang Liu",
        "Xin Dong",
        "Ximing Lu",
        "Shizhe Diao",
        "Peter Belcak",
        "Mingjie Liu",
        "Min-Hung Chen",
        "Hongxu Yin",
        "Yu-Chiang Frank Wang",
        "Kwang-Ting Cheng",
        "Yejin Choi",
        "Jan Kautz",
        "Pavlo Molchanov"
      ],
      "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.",
      "fetch_date": "2026-01-09",
      "num_comments": 6,
      "ai_summary": "Multi-reward reinforcement learning suffers from reward normalization collapse in GRPO, which GDPO addresses by decoupling reward normalization for improved training stability and performance across reasoning tasks.",
      "ai_keywords": [
        "Reinforcement learning",
        "Group Relative Policy Optimization",
        "multi-reward setting",
        "policy optimization",
        "Group reward-Decoupled Normalization Policy Optimization",
        "reward normalization",
        "advantage values",
        "training stability",
        "multi-reward reinforcement learning"
      ]
    }
  },
  {
    "id": "7e91c360c930c55d2bb71f49e0548368",
    "source": "huggingface",
    "type": "paper",
    "title": "Memory in the Age of AI Agents",
    "description": "Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As research on agent memory rapidly expands and attracts unprecedented attention, the field has als...<br/>Upvotes: 136<br/>GitHub Stars: 748<br/>Authors: Yuyang Hu, Shichun Liu, Yanwei Yue<br/>ðŸ”— <a href=\"https://github.com/Shichun-Liu/Agent-Memory-Paper-List\">GitHub</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2512.13564\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2512.13564",
    "external_url": "https://github.com/Shichun-Liu/Agent-Memory-Paper-List",
    "published_date": "2025-12-15T12:22:34.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2512.13564",
      "upvotes": 136,
      "github_stars": 748,
      "github_url": "https://github.com/Shichun-Liu/Agent-Memory-Paper-List",
      "project_url": "",
      "authors": [
        "Yuyang Hu",
        "Shichun Liu",
        "Yanwei Yue",
        "Guibin Zhang",
        "Boyang Liu",
        "Fangyi Zhu",
        "Jiahang Lin",
        "Honglin Guo",
        "Shihan Dou",
        "Zhiheng Xi",
        "Senjie Jin",
        "Jiejun Tan",
        "Yanbin Yin",
        "Jiongnan Liu",
        "Zeyu Zhang",
        "Zhongxiang Sun",
        "Yutao Zhu",
        "Hao Sun",
        "Boci Peng",
        "Zhenrong Cheng",
        "Xuanbo Fan",
        "Jiaxin Guo",
        "Xinlei Yu",
        "Zhenhong Zhou",
        "Zewen Hu",
        "Jiahao Huo",
        "Junhao Wang",
        "Yuwei Niu",
        "Yu Wang",
        "Zhenfei Yin",
        "Xiaobin Hu",
        "Yue Liao",
        "Qiankun Li",
        "Kun Wang",
        "Wangchunshu Zhou",
        "Yixin Liu",
        "Dawei Cheng",
        "Qi Zhang",
        "Tao Gui",
        "Shirui Pan",
        "Yan Zhang",
        "Philip Torr",
        "Zhicheng Dou",
        "Ji-Rong Wen",
        "Xuanjing Huang",
        "Yu-Gang Jiang",
        "Shuicheng Yan"
      ],
      "summary": "Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As research on agent memory rapidly expands and attracts unprecedented attention, the field has also become increasingly fragmented. Existing works that fall under the umbrella of agent memory often differ substantially in their motivations, implementations, and evaluation protocols, while the proliferation of loosely defined memory terminologies has further obscured conceptual clarity. Traditional taxonomies such as long/short-term memory have proven insufficient to capture the diversity of contemporary agent memory systems. This work aims to provide an up-to-date landscape of current agent memory research. We begin by clearly delineating the scope of agent memory and distinguishing it from related concepts such as LLM memory, retrieval augmented generation (RAG), and context engineering. We then examine agent memory through the unified lenses of forms, functions, and dynamics. From the perspective of forms, we identify three dominant realizations of agent memory, namely token-level, parametric, and latent memory. From the perspective of functions, we propose a finer-grained taxonomy that distinguishes factual, experiential, and working memory. From the perspective of dynamics, we analyze how memory is formed, evolved, and retrieved over time. To support practical development, we compile a comprehensive summary of memory benchmarks and open-source frameworks. Beyond consolidation, we articulate a forward-looking perspective on emerging research frontiers, including memory automation, reinforcement learning integration, multimodal memory, multi-agent memory, and trustworthiness issues. We hope this survey serves not only as a reference for existing work, but also as a conceptual foundation for rethinking memory as a first-class primitive in the design of future agentic intelligence.",
      "fetch_date": "2025-12-16",
      "num_comments": 5,
      "ai_summary": "This survey provides an updated overview of agent memory research, distinguishing its forms, functions, and dynamics, and highlights emerging research directions.",
      "ai_keywords": [
        "agent memory",
        "LLM memory",
        "retrieval augmented generation (RAG)",
        "context engineering",
        "token-level memory",
        "parametric memory",
        "latent memory",
        "factual memory",
        "experiential memory",
        "working memory",
        "memory benchmarks",
        "open-source frameworks",
        "memory automation",
        "reinforcement learning integration",
        "multimodal memory",
        "multi-agent memory",
        "trustworthiness issues"
      ]
    }
  },
  {
    "id": "137390ef0a270b1435b825365ec94a3a",
    "source": "huggingface",
    "type": "paper",
    "title": "Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models",
    "description": "We introduce Youtu-LLM, a lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence. Unlike typical small models that rely on distillation,...<br/>Upvotes: 129<br/>GitHub Stars: 327<br/>Authors: Junru Lu, Jiarui Qin, Lingfeng Qiao<br/>ðŸ”— <a href=\"https://github.com/TencentCloudADP/youtu-tip\">GitHub</a><br/>ðŸ”— <a href=\"https://youtu-tip.com/#llm\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2512.24618\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2512.24618",
    "external_url": "https://github.com/TencentCloudADP/youtu-tip",
    "published_date": "2025-12-30T23:25:11.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2512.24618",
      "upvotes": 129,
      "github_stars": 327,
      "github_url": "https://github.com/TencentCloudADP/youtu-tip",
      "project_url": "https://youtu-tip.com/#llm",
      "authors": [
        "Junru Lu",
        "Jiarui Qin",
        "Lingfeng Qiao",
        "Yinghui Li",
        "Xinyi Dai",
        "Bo Ke",
        "Jianfeng He",
        "Ruizhi Qiao",
        "Di Yin",
        "Xing Sun",
        "Yunsheng Wu",
        "Yinsong Liu",
        "Shuangyin Liu",
        "Mingkong Tang",
        "Haodong Lin",
        "Jiayi Kuang",
        "Fanxu Meng",
        "Xiaojuan Tang",
        "Yunjia Xi",
        "Junjie Huang",
        "Haotong Yang",
        "Zhenyi Shen",
        "Yangning Li",
        "Qianwen Zhang",
        "Yifei Yu",
        "Siyu An",
        "Junnan Dong",
        "Qiufeng Wang",
        "Jie Wang",
        "Keyu Chen",
        "Wei Wen",
        "Taian Guo",
        "Zhifeng Shen",
        "Daohai Yu",
        "Jiahao Li",
        "Ke Li",
        "Zongyi Li",
        "Xiaoyu Tan"
      ],
      "summary": "We introduce Youtu-LLM, a lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence. Unlike typical small models that rely on distillation, Youtu-LLM (1.96B) is pre-trained from scratch to systematically cultivate reasoning and planning capabilities. The key technical advancements are as follows: (1) Compact Architecture with Long-Context Support: Built on a dense Multi-Latent Attention (MLA) architecture with a novel STEM-oriented vocabulary, Youtu-LLM supports a 128k context window. This design enables robust long-context reasoning and state tracking within a minimal memory footprint, making it ideal for long-horizon agent and reasoning tasks. (2) Principled \"Commonsense-STEM-Agent\" Curriculum: We curated a massive corpus of approximately 11T tokens and implemented a multi-stage training strategy. By progressively shifting the pre-training data distribution from general commonsense to complex STEM and agentic tasks, we ensure the model acquires deep cognitive abilities rather than superficial alignment. (3) Scalable Agentic Mid-training: Specifically for the agentic mid-training, we employ diverse data construction schemes to synthesize rich and varied trajectories across math, coding, and tool-use domains. This high-quality data enables the model to internalize planning and reflection behaviors effectively. Extensive evaluations show that Youtu-LLM sets a new state-of-the-art for sub-2B LLMs. On general benchmarks, it achieves competitive performance against larger models, while on agent-specific tasks, it significantly surpasses existing SOTA baselines, demonstrating that lightweight models can possess strong intrinsic agentic capabilities.",
      "fetch_date": "2026-01-01",
      "num_comments": 3,
      "ai_summary": "Youtu-LLM is a lightweight language model optimized for computational efficiency and agentic intelligence through a compact architecture, STEM-focused training curriculum, and scalable mid-training strategies for planning and reasoning tasks.",
      "ai_keywords": [
        "Multi-Latent Attention (MLA) architecture",
        "STEM-oriented vocabulary",
        "128k context window",
        "Commonsense-STEM-Agent Curriculum",
        "multi-stage training strategy",
        "agentic mid-training",
        "data construction schemes",
        "planning and reflection behaviors",
        "long-context reasoning",
        "state tracking",
        "agentic capabilities"
      ]
    }
  },
  {
    "id": "b0feeb7f591514204e557e85f0c2b6ca",
    "source": "huggingface",
    "type": "paper",
    "title": "Sharp Monocular View Synthesis in Less Than a Second",
    "description": "We present SHARP, an approach to photorealistic view synthesis from a single image. Given a single photograph, SHARP regresses the parameters of a 3D Gaussian representation of the depicted scene. Thi...<br/>Upvotes: 26<br/>GitHub Stars: 6753<br/>Authors: Lars Mescheder, Wei Dong, Shiwei Li<br/>ðŸ”— <a href=\"https://github.com/apple/ml-sharp\">GitHub</a><br/>ðŸ”— <a href=\"https://apple.github.io/ml-sharp/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2512.10685\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2512.10685",
    "external_url": "https://github.com/apple/ml-sharp",
    "published_date": "2025-12-11T09:34:11.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2512.10685",
      "upvotes": 26,
      "github_stars": 6753,
      "github_url": "https://github.com/apple/ml-sharp",
      "project_url": "https://apple.github.io/ml-sharp/",
      "authors": [
        "Lars Mescheder",
        "Wei Dong",
        "Shiwei Li",
        "Xuyang Bai",
        "Marcel Santos",
        "Peiyun Hu",
        "Bruno Lecouat",
        "Mingmin Zhen",
        "AmaÃ«l Delaunoy",
        "Tian Fang",
        "Yanghai Tsin",
        "Stephan R. Richter",
        "Vladlen Koltun"
      ],
      "summary": "We present SHARP, an approach to photorealistic view synthesis from a single image. Given a single photograph, SHARP regresses the parameters of a 3D Gaussian representation of the depicted scene. This is done in less than a second on a standard GPU via a single feedforward pass through a neural network. The 3D Gaussian representation produced by SHARP can then be rendered in real time, yielding high-resolution photorealistic images for nearby views. The representation is metric, with absolute scale, supporting metric camera movements. Experimental results demonstrate that SHARP delivers robust zero-shot generalization across datasets. It sets a new state of the art on multiple datasets, reducing LPIPS by 25-34% and DISTS by 21-43% versus the best prior model, while lowering the synthesis time by three orders of magnitude. Code and weights are provided at https://github.com/apple/ml-sharp",
      "fetch_date": "2025-12-15",
      "num_comments": 2,
      "ai_summary": "SHARP synthesizes photorealistic views from a single image using a 3D Gaussian representation, achieving state-of-the-art results with rapid processing.",
      "ai_keywords": [
        "photorealistic view synthesis",
        "3D Gaussian representation",
        "neural network",
        "feedforward pass",
        "real-time rendering",
        "metric representation",
        "zero-shot generalization",
        "LPIPS",
        "DISTS"
      ]
    }
  },
  {
    "id": "4abe591dec20d02c68570faea169dfcb",
    "source": "huggingface",
    "type": "paper",
    "title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization",
    "description": "Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual ef...<br/>Upvotes: 108<br/>GitHub Stars: 4186<br/>Authors: Yuchen Shi, Yuzheng Cai, Siqi Cai<br/>ðŸ”— <a href=\"https://github.com/TencentCloudADP/youtu-agent\">GitHub</a><br/>ðŸ”— <a href=\"https://tencentcloudadp.github.io/youtu-agent/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2512.24615\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2512.24615",
    "external_url": "https://github.com/TencentCloudADP/youtu-agent",
    "published_date": "2025-12-30T23:17:36.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2512.24615",
      "upvotes": 108,
      "github_stars": 4186,
      "github_url": "https://github.com/TencentCloudADP/youtu-agent",
      "project_url": "https://tencentcloudadp.github.io/youtu-agent/",
      "authors": [
        "Yuchen Shi",
        "Yuzheng Cai",
        "Siqi Cai",
        "Zihan Xu",
        "Lichao Chen",
        "Yulei Qin",
        "Zhijian Zhou",
        "Xiang Fei",
        "Chaofan Qiu",
        "Xiaoyu Tan",
        "Gang Li",
        "Zongyi Li",
        "Haojia Lin",
        "Guocan Cai",
        "Yong Mao",
        "Yunsheng Wu",
        "Ke Li",
        "Xing Sun"
      ],
      "summary": "Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose Youtu-Agent, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a Workflow mode for standard tasks and a Meta-Agent mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an Agent Practice module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an Agent RL module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\\%) and GAIA (72.8\\%) using open-weight models. Our automated generation pipeline achieves over 81\\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\\% and +5.4\\% respectively. Moreover, our Agent RL training achieves 40\\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\\% and 21\\% on Maths and general/multi-hop QA benchmarks.",
      "fetch_date": "2026-01-05",
      "num_comments": 5,
      "ai_summary": "",
      "ai_keywords": []
    }
  },
  {
    "id": "5bb778f47e493a1d7bcf12aaaec0e25e",
    "source": "huggingface",
    "type": "paper",
    "title": "TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times",
    "description": "We introduce TurboDiffusion, a video generation acceleration framework that can speed up end-to-end diffusion generation by 100-200x while maintaining video quality. TurboDiffusion mainly relies on se...<br/>Upvotes: 93<br/>GitHub Stars: 3158<br/>Authors: Jintao Zhang, Kaiwen Zheng, Kai Jiang<br/>ðŸ”— <a href=\"https://github.com/thu-ml/TurboDiffusion\">GitHub</a><br/>ðŸ”— <a href=\"https://github.com/thu-ml/TurboDiffusion\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2512.16093\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2512.16093",
    "external_url": "https://github.com/thu-ml/TurboDiffusion",
    "published_date": "2025-12-17T21:21:30.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2512.16093",
      "upvotes": 93,
      "github_stars": 3158,
      "github_url": "https://github.com/thu-ml/TurboDiffusion",
      "project_url": "https://github.com/thu-ml/TurboDiffusion",
      "authors": [
        "Jintao Zhang",
        "Kaiwen Zheng",
        "Kai Jiang",
        "Haoxu Wang",
        "Ion Stoica",
        "Joseph E. Gonzalez",
        "Jianfei Chen",
        "Jun Zhu"
      ],
      "summary": "We introduce TurboDiffusion, a video generation acceleration framework that can speed up end-to-end diffusion generation by 100-200x while maintaining video quality. TurboDiffusion mainly relies on several components for acceleration: (1) Attention acceleration: TurboDiffusion uses low-bit SageAttention and trainable Sparse-Linear Attention (SLA) to speed up attention computation. (2) Step distillation: TurboDiffusion adopts rCM for efficient step distillation. (3) W8A8 quantization: TurboDiffusion quantizes model parameters and activations to 8 bits to accelerate linear layers and compress the model. In addition, TurboDiffusion incorporates several other engineering optimizations.\n  We conduct experiments on the Wan2.2-I2V-14B-720P, Wan2.1-T2V-1.3B-480P, Wan2.1-T2V-14B-720P, and Wan2.1-T2V-14B-480P models. Experimental results show that TurboDiffusion achieves 100-200x speedup for video generation even on a single RTX 5090 GPU, while maintaining comparable video quality. The GitHub repository, which includes model checkpoints and easy-to-use code, is available at https://github.com/thu-ml/TurboDiffusion.",
      "fetch_date": "2025-12-25",
      "num_comments": 7,
      "ai_summary": "TurboDiffusion accelerates video generation by 100-200x using attention acceleration, step distillation, and quantization, while maintaining video quality.",
      "ai_keywords": [
        "SageAttention",
        "Sparse-Linear Attention",
        "rCM",
        "W8A8 quantization",
        "diffusion generation",
        "video generation",
        "RTX 5090 GPU"
      ]
    }
  },
  {
    "id": "1c5e2b4f68f2d0420edc475a4ac95129",
    "source": "huggingface",
    "type": "paper",
    "title": "SAM Audio: Segment Anything in Audio",
    "description": "General audio source separation is a key capability for multimodal AI systems that can perceive and reason about sound. Despite substantial progress in recent years, existing separation models are eit...<br/>Upvotes: 22<br/>GitHub Stars: 2986<br/>Authors: Bowen Shi, Andros Tjandra, John Hoffman<br/>ðŸ”— <a href=\"https://github.com/facebookresearch/sam-audio\">GitHub</a><br/>ðŸ”— <a href=\"https://ai.meta.com/samaudio/\">Project Page</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2512.18099\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2512.18099",
    "external_url": "https://github.com/facebookresearch/sam-audio",
    "published_date": "2025-12-19T17:14:23.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2512.18099",
      "upvotes": 22,
      "github_stars": 2986,
      "github_url": "https://github.com/facebookresearch/sam-audio",
      "project_url": "https://ai.meta.com/samaudio/",
      "authors": [
        "Bowen Shi",
        "Andros Tjandra",
        "John Hoffman",
        "Helin Wang",
        "Yi-Chiao Wu",
        "Luya Gao",
        "Julius Richter",
        "Matt Le",
        "Apoorv Vyas",
        "Sanyuan Chen",
        "Christoph Feichtenhofer",
        "Piotr DollÃ¡r",
        "Wei-Ning Hsu",
        "Ann Lee"
      ],
      "summary": "General audio source separation is a key capability for multimodal AI systems that can perceive and reason about sound. Despite substantial progress in recent years, existing separation models are either domain-specific, designed for fixed categories such as speech or music, or limited in controllability, supporting only a single prompting modality such as text. In this work, we present SAM Audio, a foundation model for general audio separation that unifies text, visual, and temporal span prompting within a single framework. Built on a diffusion transformer architecture, SAM Audio is trained with flow matching on large-scale audio data spanning speech, music, and general sounds, and can flexibly separate target sources described by language, visual masks, or temporal spans. The model achieves state-of-the-art performance across a diverse suite of benchmarks, including general sound, speech, music, and musical instrument separation in both in-the-wild and professionally produced audios, substantially outperforming prior general-purpose and specialized systems. Furthermore, we introduce a new real-world separation benchmark with human-labeled multimodal prompts and a reference-free evaluation model that correlates strongly with human judgment.",
      "fetch_date": "2025-12-24",
      "num_comments": 1,
      "ai_summary": "SAM Audio, a diffusion transformer-based foundation model, achieves superior performance in general audio separation using unified text, visual, and temporal span prompts across various audio types.",
      "ai_keywords": [
        "diffusion transformer architecture",
        "flow matching",
        "audio separation",
        "general sound",
        "speech",
        "music",
        "musical instrument separation",
        "in-the-wild audio",
        "professionally produced audio",
        "real-world separation benchmark",
        "human-labeled multimodal prompts",
        "reference-free evaluation model"
      ]
    }
  },
  {
    "id": "1f04a85a317b82f2c574c078f215f35e",
    "source": "huggingface",
    "type": "paper",
    "title": "Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem",
    "description": "Agentic crafting requires LLMs to operate in real-world environments over multiple turns by taking actions, observing outcomes, and iteratively refining artifacts. Despite its importance, the open-sou...<br/>Upvotes: 96<br/>GitHub Stars: 2622<br/>Authors: Weixun Wang, XiaoXiao Xu, Wanhe An<br/>ðŸ”— <a href=\"https://github.com/alibaba/ROLL\">GitHub</a><br/>ðŸ”— <a href=\"https://huggingface.co/papers/2512.24873\">Hugging Face</a>",
    "url": "https://arxiv.org/abs/2512.24873",
    "external_url": "https://github.com/alibaba/ROLL",
    "published_date": "2025-12-31T09:03:39.000Z",
    "categories": [
      "research",
      "paper"
    ],
    "metadata": {
      "paper_id": "2512.24873",
      "upvotes": 96,
      "github_stars": 2622,
      "github_url": "https://github.com/alibaba/ROLL",
      "project_url": "",
      "authors": [
        "Weixun Wang",
        "XiaoXiao Xu",
        "Wanhe An",
        "Fangwen Dai",
        "Wei Gao",
        "Yancheng He",
        "Ju Huang",
        "Qiang Ji",
        "Hanqi Jin",
        "Xiaoyang Li",
        "Yang Li",
        "Zhongwen Li",
        "Shirong Lin",
        "Jiashun Liu",
        "Zenan Liu",
        "Tao Luo",
        "Dilxat Muhtar",
        "Yuanbin Qu",
        "Jiaqiang Shi",
        "Qinghui Sun",
        "Yingshui Tan",
        "Hao Tang",
        "Runze Wang",
        "Yi Wang",
        "Zhaoguo Wang",
        "Yanan Wu",
        "Shaopan Xiong",
        "Binchen Xu",
        "Xander Xu",
        "Yuchi Xu",
        "Qipeng Zhang",
        "Xixia Zhang",
        "Haizhou Zhao",
        "Jie Zhao",
        "Shuaibing Zhao",
        "Baihui Zheng",
        "Jianhui Zheng",
        "Suhang Zheng",
        "Yanni Zhu",
        "Mengze Cai",
        "Kerui Cao",
        "Xitong Chen",
        "Yue Dai",
        "Lifan Du",
        "Tao Feng",
        "Tao He",
        "Jin Hu",
        "Yijie Hu",
        "Ziyu Jiang",
        "Cheng Li",
        "Xiang Li",
        "Jing Liang",
        "Chonghuan Liu",
        "ZhenDong Liu",
        "Haodong Mi",
        "Yanhu Mo",
        "Junjia Ni",
        "Shixin Pei",
        "Jingyu Shen",
        "XiaoShuai Song",
        "Cecilia Wang",
        "Chaofan Wang",
        "Kangyu Wang",
        "Pei Wang",
        "Tao Wang",
        "Wei Wang",
        "Ke Xiao",
        "Mingyu Xu",
        "Tiange Xu",
        "Nan Ya",
        "Siran Yang",
        "Jianan Ye",
        "Yaxing Zang",
        "Duo Zhang",
        "Junbo Zhang",
        "Boren Zheng",
        "Wanxi Deng",
        "Ling Pan",
        "Lin Qu",
        "Wenbo Su",
        "Jiamang Wang",
        "Wei Wang",
        "Hu Wei",
        "Minggang Wu",
        "Cheng Yu",
        "Bing Zhao",
        "Zhicheng Zheng",
        "Bo Zheng"
      ],
      "summary": "Agentic crafting requires LLMs to operate in real-world environments over multiple turns by taking actions, observing outcomes, and iteratively refining artifacts. Despite its importance, the open-source community lacks a principled, end-to-end ecosystem to streamline agent development. We introduce the Agentic Learning Ecosystem (ALE), a foundational infrastructure that optimizes the production pipeline for agent LLMs. ALE consists of three components: ROLL, a post-training framework for weight optimization; ROCK, a sandbox environment manager for trajectory generation; and iFlow CLI, an agent framework for efficient context engineering. We release ROME (ROME is Obviously an Agentic Model), an open-source agent grounded by ALE and trained on over one million trajectories. Our approach includes data composition protocols for synthesizing complex behaviors and a novel policy optimization algorithm, Interaction-based Policy Alignment (IPA), which assigns credit over semantic interaction chunks rather than individual tokens to improve long-horizon training stability. Empirically, we evaluate ROME within a structured setting and introduce Terminal Bench Pro, a benchmark with improved scale and contamination control. ROME demonstrates strong performance across benchmarks like SWE-bench Verified and Terminal Bench, proving the effectiveness of the ALE infrastructure.",
      "fetch_date": "2026-01-01",
      "num_comments": 4,
      "ai_summary": "The Agentic Learning Ecosystem (ALE) introduces a principled infrastructure for agent development, combining post-training optimization, sandbox environments, and policy alignment to enhance long-horizon training stability and performance in real-world tasks.",
      "ai_keywords": [
        "ROLL (post-training framework)",
        "ROCK (sandbox environment manager)",
        "iFlow CLI (agent framework)",
        "ROME (agentic model)",
        "data composition protocols",
        "Interaction-based Policy Alignment (IPA)",
        "semantic interaction chunks",
        "Terminal Bench Pro",
        "SWE-bench Verified"
      ]
    }
  }
]