<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Bytedance Seed Blog</title>
  <id>tag:ai-news-direct.local,2025:bytedance_seed_blog</id>
  <icon>https://lf3-static.bytednsdoc.com/obj/eden-cn/lapzild-tss/ljhwZthlaukjlkulzlp/favicon_1/favicon.ico</icon>
  <logo>https://lf3-static.bytednsdoc.com/obj/eden-cn/lapzild-tss/ljhwZthlaukjlkulzlp/favicon_1/favicon.ico</logo>
  <updated>2025-12-20T09:24:15.486691+00:00</updated>
  <author>
    <name>AI News Direct</name>
  </author>
  <link href="https://raw.githubusercontent.com/mibuhand/AI-News-Direct/main/feeds/bytedance_seed_blog.xml" rel="self"/>
  <entry>
    <title>Seed Research | GR-RL Released: A Breakthrough in High-Precision Manipulation for VLA Models, Applying Real-World Reinforcement Learning to Shoe Lacing for the First Time</title>
    <id>4d4e57b901cc16f31b9d51ddc526329d</id>
    <link href="https://seed.bytedance.com/en/blog/seed-research-gr-rl-released-a-breakthrough-in-high-precision-manipulation-for-vla-models-applying-real-world-reinforcement-learning-to-shoe-lacing-for-the-first-time"/>
    <updated>2025-12-01T16:00:00+00:00</updated>
    <category term="Research"/>
    <summary>Enabling VLA models to learn through real-world interaction</summary>
  </entry>
  <entry>
    <title>Seed Research | Depth Anything 3: A Single-Transformer Architecture for Space Reconstruction from Any Views</title>
    <id>cb874905c7f92588b7248641fff175fc</id>
    <link href="https://seed.bytedance.com/en/blog/seed-research-depth-anything-3-a-single-transformer-architecture-for-space-reconstruction-from-any-views"/>
    <updated>2025-11-26T16:00:00+00:00</updated>
    <category term="Models"/>
    <summary>Overcome the technical bottlenecks in monocular depth estimation and multi-view reconstruction</summary>
  </entry>
  <entry>
    <title>Seed3D 1.0 Released: Generate High-Fidelity 3D Models from Single Images, Featuring SOTA Texturing</title>
    <id>e823e9d061925d42ce719cd2fcc2e42b</id>
    <link href="https://seed.bytedance.com/en/blog/seed3d-1-0-released-generate-high-fidelity-3d-models-from-single-images-featuring-sota-texturing"/>
    <updated>2025-10-22T16:00:00+00:00</updated>
    <category term="Models"/>
    <summary>Extensible to scene generation, providing a foundation for world simulators</summary>
  </entry>
  <entry>
    <title>Seedream 4.0 Officially Released: Beyond Drawing, Into Imagination</title>
    <id>563f7e9621a9097ed3c0b704d8432a3b</id>
    <link href="https://seed.bytedance.com/en/blog/seedream-4-0-officially-released-beyond-drawing-into-imagination"/>
    <updated>2025-09-08T16:00:00+00:00</updated>
    <category term="Models"/>
    <summary>Multimodal Generation Fully Upgraded</summary>
  </entry>
  <entry>
    <title>Seed-OSS Open-Source Models Release</title>
    <id>66bbbe0495ac2295ef3910af938b6148</id>
    <link href="https://seed.bytedance.com/en/blog/seed-oss-open-source-models-release"/>
    <updated>2025-08-20T16:00:00+00:00</updated>
    <category term="Models"/>
    <summary>Native long context window and flexible control of thinking budget</summary>
  </entry>
  <entry>
    <title>ByteDance Seed Open-Sources VeOmni, Unlocking Any Modality Model Training</title>
    <id>c389e85717949a6c1a56203aedfdb189</id>
    <link href="https://seed.bytedance.com/en/blog/bytedance-seed-open-sources-veomni-unlocking-any-modality-model-training"/>
    <updated>2025-08-13T16:00:00+00:00</updated>
    <category term="Research"/>
    <summary>Reduces engineering development time from weeks to days.</summary>
  </entry>
  <entry>
    <title>ByteDance Seed Helps Tsinghua Win RoboCup Championship</title>
    <id>a4fc7dfc99074ddaac6b3cab641997f1</id>
    <link href="https://seed.bytedance.com/en/blog/bytedance-seed-helps-tsinghua-win-robocup-championship"/>
    <updated>2025-07-31T16:00:00+00:00</updated>
    <category term="Team News"/>
    <summary>The first championship win for a Chinese team in the Humanoid AdultSize league.</summary>
  </entry>
  <entry>
    <title>Seed Researchï¿¨Seed Diffusion Preview Released: A Diffusion Language Model Delivering Breakthrough 2,146 Tokens/s Inference Speed</title>
    <id>4362630fd63cad6da2aa8b090a5a7f35</id>
    <link href="https://seed.bytedance.com/en/blog/seed-research-seed-diffusion-preview-released-a-diffusion-language-model-delivering-breakthrough-2-146-tokens-s-inference-speed"/>
    <updated>2025-07-30T16:00:00+00:00</updated>
    <category term="Models"/>
    <summary>Over 100 lines of code can be generated within one second.</summary>
  </entry>
  <entry>
    <title>Seed LiveInterpret 2.0 Released: An End-to-End Simultaneous Interpretation Model Featuring Ultra-High Accuracy Close to Human Interpreters, Low Latency of 3 Seconds, and Real-Time Voice Cloning</title>
    <id>c5e2f744f99160fe415ca2b50f92b88d</id>
    <link href="https://seed.bytedance.com/en/blog/seed-liveinterpret-2-0-released-an-end-to-end-simultaneous-interpretation-model-featuring-ultra-high-accuracy-close-to-human-interpreters-low-latency-of-3-seconds-and-real-time-voice-cloning"/>
    <updated>2025-07-23T16:00:00+00:00</updated>
    <category term="Models"/>
    <summary>Seed LiveInterpret 2.0 achieves SOTA quality in both Chinese-to-English and English-to-Chinese interpretation with ultra-low latency.</summary>
  </entry>
</feed>
