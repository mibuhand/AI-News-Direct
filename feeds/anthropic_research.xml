<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Anthropic Research</title>
  <id>tag:ai-news-direct.local,2025:anthropic_research</id>
  <icon>https://www.anthropic.com/favicon.ico</icon>
  <logo>https://www.anthropic.com/favicon.ico</logo>
  <updated>2025-08-15T09:25:23.780486+00:00</updated>
  <author>
    <name>AI News Direct</name>
  </author>
  <link href="https://www.anthropic.com/feeds/anthropic_research.xml" rel="self"/>
  <entry>
    <title>Persona vectors: Monitoring and controlling character traits in language models</title>
    <id>e967e818a8c5cff4ce840b4077d31f4a</id>
    <link href="https://www.anthropic.com/research/persona-vectors"/>
    <updated>2025-08-01T12:38:00+00:00</updated>
    <category term="Interpretability"/>
    <summary/>
  </entry>
  <entry>
    <title>Project Vend: Can Claude run a small shop? (And why does that matter?)</title>
    <id>9688f0a21ac3ae079ba2cfe0bdd55749</id>
    <link href="https://www.anthropic.com/research/project-vend-1"/>
    <updated>2025-06-27T06:05:00+00:00</updated>
    <category term="Policy"/>
    <summary/>
  </entry>
  <entry>
    <title>Agentic Misalignment: How LLMs could be insider threats</title>
    <id>141153d9c830e689fb408e655ade95f0</id>
    <link href="https://www.anthropic.com/research/agentic-misalignment"/>
    <updated>2025-06-20T22:30:00+00:00</updated>
    <category term="Alignment"/>
    <summary/>
  </entry>
  <entry>
    <title>Confidential Inference via Trusted Virtual Machines</title>
    <id>ec8b697add66c774aecf4a43503c7ecf</id>
    <link href="https://www.anthropic.com/research/confidential-inference-trusted-vms"/>
    <updated>2025-06-18T13:27:00+00:00</updated>
    <category term="Announcements"/>
    <summary/>
  </entry>
  <entry>
    <title>SHADE-Arena: Evaluating sabotage and monitoring in LLM agents</title>
    <id>133edbb581215c0ea40a0e3d66e0817c</id>
    <link href="https://www.anthropic.com/research/shade-arena-sabotage-monitoring"/>
    <updated>2025-06-16T20:20:00+00:00</updated>
    <category term="Alignment"/>
    <summary/>
  </entry>
  <entry>
    <title>Open-sourcing circuit tracing tools</title>
    <id>a87867f678cff6c1a79d065d6d923194</id>
    <link href="https://www.anthropic.com/research/open-source-circuit-tracing"/>
    <updated>2025-05-29T12:13:00+00:00</updated>
    <category term="Interpretability"/>
    <summary/>
  </entry>
  <entry>
    <title>Anthropic Economic Index: AIâs impact on software development</title>
    <id>44dcf4df758bf55b49879de5ce638418</id>
    <link href="https://www.anthropic.com/research/impact-software-development"/>
    <updated>2025-04-28T09:36:00+00:00</updated>
    <category term="Societal Impacts"/>
    <summary/>
  </entry>
  <entry>
    <title>Exploring model welfare</title>
    <id>4e17075ddeded00589b7596b112c439d</id>
    <link href="https://www.anthropic.com/research/exploring-model-welfare"/>
    <updated>2025-04-24T10:59:00+00:00</updated>
    <category term="Alignment"/>
    <summary/>
  </entry>
  <entry>
    <title>Values in the wild: Discovering and analyzing values in real-world language model interactions</title>
    <id>bd4fc0fd335e2de5ca88162f048c8782</id>
    <link href="https://www.anthropic.com/research/values-wild"/>
    <updated>2025-04-21T11:50:00+00:00</updated>
    <category term="Societal Impacts"/>
    <summary/>
  </entry>
  <entry>
    <title>Anthropic Education Report: How university students use Claude</title>
    <id>1475016ec3ebb26e283128200dca68d9</id>
    <link href="https://www.anthropic.com/research/anthropic-education-report-how-university-students-use-claude"/>
    <updated>2025-04-08T15:00:00+00:00</updated>
    <category term="Announcements"/>
    <category term="Societal Impacts"/>
    <summary/>
  </entry>
  <entry>
    <title>Reasoning models don't always say what they think</title>
    <id>30da1224fd786cf54978f4da5abad41e</id>
    <link href="https://www.anthropic.com/research/reasoning-models-dont-say-think"/>
    <updated>2025-04-03T14:32:00+00:00</updated>
    <category term="Alignment"/>
    <summary/>
  </entry>
  <entry>
    <title>Anthropic Economic Index: Insights from Claude 3.7 Sonnet</title>
    <id>f38ffbe1db3ead38649bbf81389e1a55</id>
    <link href="https://www.anthropic.com/research/anthropic-economic-index-insights-from-claude-sonnet-3-7"/>
    <updated>2025-03-27T21:00:00+00:00</updated>
    <category term="Announcements"/>
    <category term="Societal Impacts"/>
    <summary/>
  </entry>
  <entry>
    <title>Tracing the thoughts of a large language model</title>
    <id>05c8417a43001a9925fe7beec977b495</id>
    <link href="https://www.anthropic.com/research/tracing-thoughts-language-model"/>
    <updated>2025-03-27T09:16:00+00:00</updated>
    <category term="Interpretability"/>
    <summary/>
  </entry>
  <entry>
    <title>Auditing language models for hidden objectives</title>
    <id>1081b411767a719b9508b2015ccdd1d9</id>
    <link href="https://www.anthropic.com/research/auditing-hidden-objectives"/>
    <updated>2025-03-13T16:00:00+00:00</updated>
    <category term="Alignment"/>
    <category term="Interpretability"/>
    <summary/>
  </entry>
  <entry>
    <title>Forecasting rare language model behaviors</title>
    <id>e62a649d0883f0275eb3f460e6383dda</id>
    <link href="https://www.anthropic.com/research/forecasting-rare-behaviors"/>
    <updated>2025-02-25T20:17:00+00:00</updated>
    <category term="Alignment"/>
    <summary/>
  </entry>
  <entry>
    <title>Claudeâs extended thinking</title>
    <id>596e0640adc504df40e1eec75875d9de</id>
    <link href="https://www.anthropic.com/research/visible-extended-thinking"/>
    <updated>2025-02-24T14:38:00+00:00</updated>
    <category term="Announcements"/>
    <summary/>
  </entry>
  <entry>
    <title>Insights on Crosscoder Model Diffing</title>
    <id>42914ebe87bfbc1f47e67bc36e0c3a58</id>
    <link href="https://www.anthropic.com/research/crosscoder-model-diffing"/>
    <updated>2025-02-20T23:50:00+00:00</updated>
    <category term="Interpretability"/>
    <summary/>
  </entry>
  <entry>
    <title>The Anthropic Economic Index</title>
    <id>5ec185859cc74b8293c4ce8f312f48f6</id>
    <link href="https://www.anthropic.com/research/the-anthropic-economic-index"/>
    <updated>2025-02-10T13:00:00+00:00</updated>
    <category term="Announcements"/>
    <category term="Societal Impacts"/>
    <summary/>
  </entry>
  <entry>
    <title>Constitutional Classifiers: Defending against universal jailbreaks</title>
    <id>4d7f5867c140495f3e612b0c0dba2295</id>
    <link href="https://www.anthropic.com/research/constitutional-classifiers"/>
    <updated>2025-02-03T12:35:00+00:00</updated>
    <category term="Alignment"/>
    <summary/>
  </entry>
  <entry>
    <title>Building effective agents</title>
    <id>d8598e0c2d724836244acef020ea7e35</id>
    <link href="https://www.anthropic.com/research/building-effective-agents"/>
    <updated>2024-12-19T21:14:00+00:00</updated>
    <category term="Product"/>
    <summary/>
  </entry>
  <entry>
    <title>Alignment faking in large language models</title>
    <id>bb1110b0466162e1e8fc2d4c8d94fb9e</id>
    <link href="https://www.anthropic.com/research/alignment-faking"/>
    <updated>2024-12-18T14:16:00+00:00</updated>
    <category term="Alignment"/>
    <summary/>
  </entry>
  <entry>
    <title>Clio: A system for privacy-preserving insights into real-world AI use</title>
    <id>2cc555fc3d8556785dfe946c0cf6e65f</id>
    <link href="https://www.anthropic.com/research/clio"/>
    <updated>2024-12-12T13:08:00+00:00</updated>
    <category term="Societal Impacts"/>
    <summary/>
  </entry>
  <entry>
    <title>A statistical approach to model evaluations</title>
    <id>008d3efc3583b39aa5dc94aa2584ef02</id>
    <link href="https://www.anthropic.com/research/statistical-approach-to-model-evals"/>
    <updated>2024-11-19T16:11:00+00:00</updated>
    <category term="Evaluations"/>
    <summary/>
  </entry>
  <entry>
    <title>Raising the bar on SWE-bench Verified with Claude 3.5 Sonnet</title>
    <id>71c39437b6a27051a44395b51c6d1d3b</id>
    <link href="https://www.anthropic.com/research/swe-bench-sonnet"/>
    <updated>2024-10-30T23:25:00+00:00</updated>
    <category term="Product"/>
    <summary/>
  </entry>
  <entry>
    <title>Evaluating feature steering: A case study in mitigating social biases</title>
    <id>6aef985a567ce674e82a0c1a98b60bf7</id>
    <link href="https://www.anthropic.com/research/evaluating-feature-steering"/>
    <updated>2024-10-25T13:42:00+00:00</updated>
    <category term="Societal Impacts"/>
    <category term="Interpretability"/>
    <summary/>
  </entry>
  <entry>
    <title>Developing a computer use model</title>
    <id>ddb112757419d8b04d0b5b3c6134c628</id>
    <link href="https://www.anthropic.com/research/developing-computer-use"/>
    <updated>2024-10-22T19:42:00+00:00</updated>
    <category term="Announcements"/>
    <category term="Product"/>
    <summary/>
  </entry>
  <entry>
    <title>Sabotage evaluations for frontier models</title>
    <id>3e1df9a38d75078fb92842ef2cf230dd</id>
    <link href="https://www.anthropic.com/research/sabotage-evaluations"/>
    <updated>2024-10-18T16:55:00+00:00</updated>
    <category term="Alignment"/>
    <summary/>
  </entry>
  <entry>
    <title>Using dictionary learning features as classifiers</title>
    <id>9395cc2a266696cd77ea966e514cba23</id>
    <link href="https://www.anthropic.com/research/features-as-classifiers"/>
    <updated>2024-10-16T23:49:00+00:00</updated>
    <category term="Interpretability"/>
    <summary/>
  </entry>
  <entry>
    <title>Circuits Updates â September 2024</title>
    <id>e1990ff2e1e62f797efc2806bba386ad</id>
    <link href="https://www.anthropic.com/research/circuits-updates-sept-2024"/>
    <updated>2024-10-01T09:38:00+00:00</updated>
    <category term="Interpretability"/>
    <summary/>
  </entry>
  <entry>
    <title>Circuits Updates â August 2024</title>
    <id>6f3882fa35d3822206b94859b7cbfeac</id>
    <link href="https://www.anthropic.com/research/circuits-updates-august-2024"/>
    <updated>2024-09-06T15:36:00+00:00</updated>
    <category term="Interpretability"/>
    <summary/>
  </entry>
  <entry>
    <title>Circuits Updates â July 2024</title>
    <id>0145e2d50a81bae879f80f1f6e5cbd00</id>
    <link href="https://www.anthropic.com/research/circuits-updates-july-2024"/>
    <updated>2024-07-31T19:21:00+00:00</updated>
    <category term="Interpretability"/>
    <summary/>
  </entry>
  <entry>
    <title>Circuits Updates â June 2024</title>
    <id>cfe4b7f2706195ff8b11d98c8eacf293</id>
    <link href="https://www.anthropic.com/research/circuits-updates-june-2024"/>
    <updated>2024-06-28T21:43:09.349000+00:00</updated>
    <category term="Interpretability"/>
    <summary/>
  </entry>
  <entry>
    <title>Sycophancy to subterfuge: Investigating reward tampering in language models</title>
    <id>91ce6e1642b8295e72d306c7591a51ba</id>
    <link href="https://www.anthropic.com/research/reward-tampering"/>
    <updated>2024-06-17T13:10:48.322000+00:00</updated>
    <category term="Alignment"/>
    <summary/>
  </entry>
  <entry>
    <title>The engineering challenges of scaling interpretability</title>
    <id>cdda5186385b7d9ea99a5e3013d757eb</id>
    <link href="https://www.anthropic.com/research/engineering-challenges-interpretability"/>
    <updated>2024-06-13T08:00:00+00:00</updated>
    <category term="Interpretability"/>
    <summary/>
  </entry>
  <entry>
    <title>Claudeâs Character</title>
    <id>64e84a950a3b0d0c3987f7745ff602e5</id>
    <link href="https://www.anthropic.com/research/claude-character"/>
    <updated>2024-06-08T18:27:00+00:00</updated>
    <category term="Alignment"/>
    <summary/>
  </entry>
  <entry>
    <title>Testing and mitigating elections-related risks</title>
    <id>752a7fe7abdea2f7c331381fca628ad7</id>
    <link href="https://www.anthropic.com/research/testing-and-mitigating-elections-related-risks"/>
    <updated>2024-06-06T13:00:00+00:00</updated>
    <category term="Policy"/>
    <category term="Societal Impacts"/>
    <summary/>
  </entry>
  <entry>
    <title>Mapping the Mind of a Large Language Model</title>
    <id>a23200e5e159acb85ca07a8b21e45012</id>
    <link href="https://www.anthropic.com/research/mapping-mind-language-model"/>
    <updated>2024-05-21T07:00:00+00:00</updated>
    <category term="Interpretability"/>
    <summary/>
  </entry>
  <entry>
    <title>Circuits Updates â April 2024</title>
    <id>fd74138b532062fb4d918fafbd3d7bcf</id>
    <link href="https://www.anthropic.com/research/circuits-updates-april-2024"/>
    <updated>2024-04-26T20:24:00+00:00</updated>
    <category term="Interpretability"/>
    <category term="Research"/>
    <summary/>
  </entry>
  <entry>
    <title>Simple probes can catch sleeper agents</title>
    <id>ae0341058e456fe2ab3cd5af930ab646</id>
    <link href="https://www.anthropic.com/research/probes-catch-sleeper-agents"/>
    <updated>2024-04-23T09:42:44.093000+00:00</updated>
    <category term="Alignment"/>
    <category term="Interpretability"/>
    <summary/>
  </entry>
  <entry>
    <title>Measuring the Persuasiveness of Language Models</title>
    <id>2e78ab925d81248768883b69c834d31c</id>
    <link href="https://www.anthropic.com/research/measuring-model-persuasiveness"/>
    <updated>2024-04-09T15:55:00+00:00</updated>
    <category term="Societal Impacts"/>
    <summary/>
  </entry>
  <entry>
    <title>Many-shot jailbreaking</title>
    <id>700f186cbda240202c509cbec1a1f3a0</id>
    <link href="https://www.anthropic.com/research/many-shot-jailbreaking"/>
    <updated>2024-04-02T16:05:09.100000+00:00</updated>
    <category term="Alignment"/>
    <summary/>
  </entry>
  <entry>
    <title>Reflections on Qualitative Research</title>
    <id>c5e1470c7d22299cee9b38f14e2265c7</id>
    <link href="https://www.anthropic.com/research/transformer-circuits"/>
    <updated>2024-03-08T16:00:00+00:00</updated>
    <category term="Interpretability"/>
    <category term="Research"/>
    <summary/>
  </entry>
  <entry>
    <title>Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training</title>
    <id>83135a999d160b60c0d80531be84bcbc</id>
    <link href="https://www.anthropic.com/research/sleeper-agents-training-deceptive-llms-that-persist-through-safety-training"/>
    <updated>2024-01-14T22:10:00+00:00</updated>
    <category term="Alignment"/>
    <category term="Research"/>
    <summary/>
  </entry>
  <entry>
    <title>Evaluating and Mitigating Discrimination in Language Model Decisions</title>
    <id>baa0deea62abf51059585fa024d3f124</id>
    <link href="https://www.anthropic.com/research/evaluating-and-mitigating-discrimination-in-language-model-decisions"/>
    <updated>2023-12-07T08:50:00+00:00</updated>
    <category term="Societal Impacts"/>
    <summary/>
  </entry>
  <entry>
    <title>Specific versus General Principles for Constitutional AI</title>
    <id>fdc20c21bf4e802f0f53420e5bf98598</id>
    <link href="https://www.anthropic.com/research/specific-versus-general-principles-for-constitutional-ai"/>
    <updated>2023-10-24T09:07:00+00:00</updated>
    <category term="Alignment"/>
    <category term="Research"/>
    <summary/>
  </entry>
  <entry>
    <title>Towards Understanding Sycophancy in Language Models</title>
    <id>5b53ab55afb0c374ebf133d8f6b0033e</id>
    <link href="https://www.anthropic.com/research/towards-understanding-sycophancy-in-language-models"/>
    <updated>2023-10-23T11:57:00+00:00</updated>
    <category term="Alignment"/>
    <category term="Research"/>
    <summary/>
  </entry>
  <entry>
    <title>Collective Constitutional AI: Aligning a Language Model with Public Input</title>
    <id>232c6ae6f1410565f28f1da4ff43625f</id>
    <link href="https://www.anthropic.com/research/collective-constitutional-ai-aligning-a-language-model-with-public-input"/>
    <updated>2023-10-17T00:00:00+00:00</updated>
    <category term="Policy"/>
    <category term="Societal Impacts"/>
    <summary/>
  </entry>
  <entry>
    <title>Decomposing Language Models Into Understandable Components</title>
    <id>427bf5b6881b571233d6e11e3bd406e0</id>
    <link href="https://www.anthropic.com/research/decomposing-language-models-into-understandable-components"/>
    <updated>2023-10-05T00:00:00+00:00</updated>
    <category term="Interpretability"/>
    <category term="Research"/>
    <summary/>
  </entry>
  <entry>
    <title>Towards Monosemanticity: Decomposing Language Models With Dictionary Learning</title>
    <id>01f9fb8859e1043216b8cfb678ccf770</id>
    <link href="https://www.anthropic.com/research/towards-monosemanticity-decomposing-language-models-with-dictionary-learning"/>
    <updated>2023-10-05T00:00:00+00:00</updated>
    <category term="Interpretability"/>
    <category term="Research"/>
    <summary/>
  </entry>
  <entry>
    <title>Challenges in evaluating AI systems</title>
    <id>78c61f9a7504eefcb3ed9551fb64a63d</id>
    <link href="https://www.anthropic.com/research/evaluating-ai-systems"/>
    <updated>2023-10-04T07:00:00+00:00</updated>
    <category term="Policy"/>
    <summary/>
  </entry>
  <entry>
    <title>Tracing Model Outputs to the Training Data</title>
    <id>119c9cfbd7889f0e289b85bc618ba1a0</id>
    <link href="https://www.anthropic.com/research/influence-functions"/>
    <updated>2023-08-08T00:00:00+00:00</updated>
    <category term="Alignment"/>
    <category term="Research"/>
    <summary/>
  </entry>
  <entry>
    <title>Studying Large Language Model Generalization with Influence Functions</title>
    <id>b9db13ac93790ffcfa4ffd47a32fb066</id>
    <link href="https://www.anthropic.com/research/studying-large-language-model-generalization-with-influence-functions"/>
    <updated>2023-08-08T00:00:00+00:00</updated>
    <category term="Alignment"/>
    <category term="Research"/>
    <summary/>
  </entry>
  <entry>
    <title>Question Decomposition Improves the Faithfulness of Model-Generated Reasoning</title>
    <id>d7921fb9672d1635e1003165d5373bb3</id>
    <link href="https://www.anthropic.com/research/question-decomposition-improves-the-faithfulness-of-model-generated-reasoning"/>
    <updated>2023-07-18T00:00:00+00:00</updated>
    <category term="Alignment"/>
    <category term="Research"/>
    <summary/>
  </entry>
  <entry>
    <title>Measuring Faithfulness in Chain-of-Thought Reasoning</title>
    <id>5b5851ece8cc5ff3203317151a85a0e9</id>
    <link href="https://www.anthropic.com/research/measuring-faithfulness-in-chain-of-thought-reasoning"/>
    <updated>2023-07-18T00:00:00+00:00</updated>
    <category term="Alignment"/>
    <category term="Research"/>
    <summary/>
  </entry>
  <entry>
    <title>Towards Measuring the Representation of Subjective Global Opinions in Language Models</title>
    <id>197bad1eed70f713fd65f52179689cef</id>
    <link href="https://www.anthropic.com/research/towards-measuring-the-representation-of-subjective-global-opinions-in-language-models"/>
    <updated>2023-06-29T00:00:00+00:00</updated>
    <category term="Societal Impacts"/>
    <summary/>
  </entry>
  <entry>
    <title>Circuits Updates â May 2023</title>
    <id>09076c81fea2d749e704670ff251bea6</id>
    <link href="https://www.anthropic.com/research/circuits-updates-may-2023"/>
    <updated>2023-05-24T00:00:00+00:00</updated>
    <category term="Interpretability"/>
    <category term="Research"/>
    <summary/>
  </entry>
  <entry>
    <title>Interpretability Dreams</title>
    <id>4bcec845342f4fbccfc629cd6b485035</id>
    <link href="https://www.anthropic.com/research/interpretability-dreams"/>
    <updated>2023-05-24T00:00:00+00:00</updated>
    <category term="Interpretability"/>
    <category term="Research"/>
    <summary/>
  </entry>
  <entry>
    <title>Distributed Representations: Composition &amp; Superposition</title>
    <id>d97a64136ed5bb8e5054adcbfcd307eb</id>
    <link href="https://www.anthropic.com/research/distributed-representations-composition-superposition"/>
    <updated>2023-05-04T00:00:00+00:00</updated>
    <category term="Interpretability"/>
    <category term="Research"/>
    <summary/>
  </entry>
  <entry>
    <title>Privileged Bases in the Transformer Residual Stream</title>
    <id>c7f6ba54cee93310f41907eb186fae31</id>
    <link href="https://www.anthropic.com/research/privileged-bases-in-the-transformer-residual-stream"/>
    <updated>2023-03-16T00:00:00+00:00</updated>
    <category term="Interpretability"/>
    <category term="Research"/>
    <summary/>
  </entry>
  <entry>
    <title>The Capacity for Moral Self-Correction in Large Language Models</title>
    <id>8ad10c54cd8e85a2683522ca3de5bd20</id>
    <link href="https://www.anthropic.com/research/the-capacity-for-moral-self-correction-in-large-language-models"/>
    <updated>2023-02-15T00:00:00+00:00</updated>
    <category term="Societal Impacts"/>
    <summary/>
  </entry>
  <entry>
    <title>Superposition, Memorization, and Double Descent</title>
    <id>f49a05c28c27763d987b4a8809573679</id>
    <link href="https://www.anthropic.com/research/superposition-memorization-and-double-descent"/>
    <updated>2023-01-05T00:00:00+00:00</updated>
    <category term="Interpretability"/>
    <category term="Research"/>
    <summary/>
  </entry>
  <entry>
    <title>Discovering Language Model Behaviors with Model-Written Evaluations</title>
    <id>8b0cd14c5cd39df5bc2897fff197461e</id>
    <link href="https://www.anthropic.com/research/discovering-language-model-behaviors-with-model-written-evaluations"/>
    <updated>2022-12-19T00:00:00+00:00</updated>
    <category term="Alignment"/>
    <category term="Research"/>
    <summary/>
  </entry>
  <entry>
    <title>Constitutional AI: Harmlessness from AI Feedback</title>
    <id>486ef73c11ad0d0b3ed5dbb24afbf3f8</id>
    <link href="https://www.anthropic.com/research/constitutional-ai-harmlessness-from-ai-feedback"/>
    <updated>2022-12-15T00:00:00+00:00</updated>
    <category term="Alignment"/>
    <category term="Research"/>
    <summary/>
  </entry>
  <entry>
    <title>Measuring Progress on Scalable Oversight for Large Language Models</title>
    <id>d11b50801e8e3cf370a26ce50bb428ce</id>
    <link href="https://www.anthropic.com/research/measuring-progress-on-scalable-oversight-for-large-language-models"/>
    <updated>2022-11-04T00:00:00+00:00</updated>
    <category term="Alignment"/>
    <category term="Research"/>
    <summary/>
  </entry>
  <entry>
    <title>Toy Models of Superposition</title>
    <id>f9b2638b9be95a06f0861c57ed27a6e9</id>
    <link href="https://www.anthropic.com/research/toy-models-of-superposition"/>
    <updated>2022-09-14T00:00:00+00:00</updated>
    <category term="Interpretability"/>
    <category term="Research"/>
    <summary/>
  </entry>
  <entry>
    <title>Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned</title>
    <id>78e9dbb51835769eb2ec44bc3b76b0db</id>
    <link href="https://www.anthropic.com/research/red-teaming-language-models-to-reduce-harms-methods-scaling-behaviors-and-lessons-learned"/>
    <updated>2022-08-22T00:00:00+00:00</updated>
    <category term="Societal Impacts"/>
    <summary/>
  </entry>
  <entry>
    <title>Language Models (Mostly) Know What They Know</title>
    <id>d4d2d598733cd1027fa19571fcc3a80f</id>
    <link href="https://www.anthropic.com/research/language-models-mostly-know-what-they-know"/>
    <updated>2022-07-11T00:00:00+00:00</updated>
    <category term="Alignment"/>
    <category term="Research"/>
    <summary/>
  </entry>
  <entry>
    <title>Softmax Linear Units</title>
    <id>99fc2b6026431b15d4d2aa293aad758a</id>
    <link href="https://www.anthropic.com/research/softmax-linear-units"/>
    <updated>2022-06-17T00:00:00+00:00</updated>
    <category term="Interpretability"/>
    <category term="Research"/>
    <summary/>
  </entry>
  <entry>
    <title>Scaling Laws and Interpretability of Learning from Repeated Data</title>
    <id>8765edac320eb650dfb67ec8438e0c0c</id>
    <link href="https://www.anthropic.com/research/scaling-laws-and-interpretability-of-learning-from-repeated-data"/>
    <updated>2022-05-21T00:00:00+00:00</updated>
    <category term="Interpretability"/>
    <category term="Research"/>
    <summary/>
  </entry>
  <entry>
    <title>Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback</title>
    <id>765072b76e2cb7c97d6a116eefae1240</id>
    <link href="https://www.anthropic.com/research/training-a-helpful-and-harmless-assistant-with-reinforcement-learning-from-human-feedback"/>
    <updated>2022-04-12T00:00:00+00:00</updated>
    <category term="Alignment"/>
    <category term="Research"/>
    <summary/>
  </entry>
  <entry>
    <title>In-context Learning and Induction Heads</title>
    <id>6d9985f2bc568aa9ffe5bdf566319097</id>
    <link href="https://www.anthropic.com/research/in-context-learning-and-induction-heads"/>
    <updated>2022-03-08T00:00:00+00:00</updated>
    <category term="Interpretability"/>
    <category term="Research"/>
    <summary/>
  </entry>
  <entry>
    <title>Predictability and Surprise in Large Generative Models</title>
    <id>48f7e904c6cc0f77f1c26b39e9fb5a8c</id>
    <link href="https://www.anthropic.com/research/predictability-and-surprise-in-large-generative-models"/>
    <updated>2022-02-15T00:00:00+00:00</updated>
    <category term="Societal Impacts"/>
    <summary/>
  </entry>
  <entry>
    <title>A Mathematical Framework for Transformer Circuits</title>
    <id>9762c39ccc99dfc1d7fe710fc66fc677</id>
    <link href="https://www.anthropic.com/research/a-mathematical-framework-for-transformer-circuits"/>
    <updated>2021-12-22T00:00:00+00:00</updated>
    <category term="Interpretability"/>
    <category term="Research"/>
    <summary/>
  </entry>
  <entry>
    <title>A General Language Assistant as a Laboratory for Alignment</title>
    <id>e159f8eda2f59a6398e6fb085171906a</id>
    <link href="https://www.anthropic.com/research/a-general-language-assistant-as-a-laboratory-for-alignment"/>
    <updated>2021-12-01T00:00:00+00:00</updated>
    <category term="Alignment"/>
    <category term="Research"/>
    <summary/>
  </entry>
</feed>
