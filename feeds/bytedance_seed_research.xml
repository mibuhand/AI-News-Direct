<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Bytedance Seed Research</title>
  <id>tag:ai-news-direct.local,2025:bytedance_seed_research</id>
  <icon>https://lf3-static.bytednsdoc.com/obj/eden-cn/lapzild-tss/ljhwZthlaukjlkulzlp/favicon_1/favicon.ico</icon>
  <logo>https://lf3-static.bytednsdoc.com/obj/eden-cn/lapzild-tss/ljhwZthlaukjlkulzlp/favicon_1/favicon.ico</logo>
  <updated>2025-08-08T09:31:06.646689+00:00</updated>
  <author>
    <name>AI News Direct</name>
  </author>
  <link href="https://seed.bytedance.com/en/feeds/bytedance_seed_research.xml" rel="self"/>
  <entry>
    <title>Seed LiveInterpret 2.0: End-to-end Simultaneous Speech-to-speech Translation with Your Voice</title>
    <id>b6095ba19e6ae163727b023559dbcecf</id>
    <link href="https://seed.bytedance.com/en/research/seed-liveinterpret-2-0-end-to-end-simultaneous-speech-to-speech-translation-with-your-voice"/>
    <link href="https://arxiv.org/pdf/2507.17527" rel="related"/>
    <updated>2025-07-23T16:00:00+00:00</updated>
    <category term="Speech&amp;Audio"/>
    <summary>Simultaneous Interpretation (SI) represents one of the most daunting frontiers in the translation industry, with product-level automatic systems long plagued by intractable challenges: subpar transcription and translation quality, lack of real-time speech generation, multi-speaker confusion, and translated speech inflation, especially in long-form discourses. In this study, we introduce Seed-LiveInterpret 2.0, an end-to-end SI model that delivers high-fidelity, ultra-low-latency speech-to-speech generation with voice cloning capabilities. As a fully operational product-level solution, Seed-LiveInterpret 2.0 tackles these challenges head-on through our novel duplex speech-to-speech understanding-generating framework. Experimental results demonstrate that through large-scale pretraining and reinforcement learning, the model achieves a significantly better balance between translation accuracy and latency, validated by human interpreters to exceed 70% correctness in complex scenarios. Notably, Seed-LiveInterpret 2.0 outperforms commercial SI solutions by significant margins in translation quality, while slashing the average latency of cloned speech from nearly 10 seconds to a near-real-time 3 seconds, which is around a near 70% reduction that drastically enhances practical usability.</summary>
  </entry>
  <entry>
    <title>Seedance 1.0: Exploring the Boundaries of Video Generation Models</title>
    <id>b300005dd642e1d107c0f5161d3f598a</id>
    <link href="https://seed.bytedance.com/en/research/seedance-1-0-exploring-the-boundaries-of-video-generation-models"/>
    <link href="https://arxiv.org/pdf/2506.09113" rel="related"/>
    <updated>2025-06-11T03:08:06+00:00</updated>
    <category term="Computer Vision"/>
    <summary>Notable advances in diffusion modeling have propelled rapid improvements in video generation, yet current foundational model still confront critical challenges in synergistically balancing prompt following, motion plausibility, and visual quality. In this report, we introduce Seedance 1.0, a high-performance and inference-efficient video foundation generation model that integrates several core technical improvements: (i) multi-source data curation augmented with precision and meaningful video captioning, enabling comprehensive learning across diverse scenarios; (ii) an efficient pre-training paradigm that enables multiple features or functions such as interleaved multimodal positional encoding, native multi-shot generation capacity, and multi-task modeling; (iii) carefully-designed post-training optimization leveraging fine-grained supervised fine-tuning, video-specific RLHF with multi-dimensional reward mechanisms for considerable performance improvements; (iv) excellent model acceleration achieving 10Ã— inference speedup through multi- stage distillation strategies and system-level optimizations. Seedance 1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds. Compared to state-of-the-art video generation models, Seedance 1.0 stands out with high-quality and fast video generation with superior spatiotemporal fluidity with structural stability, precise instruction adherence in complex multi-subject contexts, native multi-shot narrative coherence with consistent subject representation, and ultra-fast inference.</summary>
  </entry>
  <entry>
    <title>SeedEdit 3.0: Fast and High-Quality Generative Image Editing</title>
    <id>e1aa581108c4694a9e6091df5ebba6fb</id>
    <link href="https://seed.bytedance.com/en/research/seededit-3-0-fast-and-high-quality-generative-image-editing"/>
    <link href="https://arxiv.org/abs/2506.05083" rel="related"/>
    <updated>2025-06-04T16:00:00+00:00</updated>
    <category term="Computer Vision"/>
    <summary>We introduce SeedEdit 3.0, in companion with our T2I model Seedream 3.0, which significantly improves over our previous SeedEdit versions in both aspects of edit instruction following and image content (e.g., ID/IP) preservation on real image inputs. Additional to model upgrading with T2I, in this report, we present several key improvements. First, we develop an enhanced data curation pipeline with a meta-info paradigm and meta-info embedding strategy that help mix images from multiple data sources. This allows us to scale editing data effectively, and meta information is helpfult to connect VLM with diffusion model more closely. Second, we introduce a joint learning pipeline for computing a diffusion loss and reward losses. Finally, we evaluate SeedEdit 3.0 on our testing benchmarks, for real/synthetic image editing, where it achieves a best trade-off between multiple aspects, yielding a high usability rate of 56.1%, compared to SeedEdit 1.6 (38.4%), GPT4o (37.1%) and Gemini 2.0 (30.3%).</summary>
  </entry>
  <entry>
    <title>MMaDA: Multimodal Large Diffusion Language Models</title>
    <id>a017383bd0d40d10b4ca9bf35b20b583</id>
    <link href="https://seed.bytedance.com/en/research/mmada-multimodal-large-diffusion-language-models"/>
    <link href="https://arxiv.org/abs/2505.15809" rel="related"/>
    <updated>2025-05-20T16:00:00+00:00</updated>
    <category term="Computer Vision"/>
    <summary>We introduce MMaDA, a novel class of multimodal diffusion foundation models designed to achieve superior performance across diverse domains such as textual reasoning, multimodal understanding, and text-to-image generation. The approach is distinguished by three key innovations: (i) MMaDA adopts a unified diffusion architecture with a shared probabilistic formulation and a modality-agnostic design, eliminating the need for modality-specific components. This architecture ensures seamless integration and processing across different data types. (ii) We implement a mixed long chain-of-thought (CoT) fine-tuning strategy that curates a unified CoT format across modalities. By aligning reasoning processes between textual and visual domains, this strategy facilitates cold-start training for the final reinforcement learning (RL) stage, thereby enhancing the model's ability to handle complex tasks from the outset. (iii) We propose UniGRPO, a unified policy-gradient-based RL algorithm specifically tailored for diffusion foundation models. Utilizing diversified reward modeling, UniGRPO unifies post-training across both reasoning and generation tasks, ensuring consistent performance improvements. Experimental results demonstrate that MMaDA-8B exhibits strong generalization capabilities as a unified multimodal foundation model. It surpasses powerful models like LLaMA-3-7B and Qwen2-7B in textual reasoning, outperforms Show-o and SEED-X in multimodal understanding, and excels over SDXL and Janus in text-to-image generation. These achievements highlight MMaDA's effectiveness in bridging the gap between pretraining and post-training within unified diffusion architectures, providing a comprehensive framework for future research and development. We open-source our code and trained models at:
https://github.com/Gen-Verse/MMaDA</summary>
  </entry>
  <entry>
    <title>Model Merging in Pre-training of Large Language Models</title>
    <id>1c61b776ea7f2d751188a46329c0224c</id>
    <link href="https://seed.bytedance.com/en/research/model-merging-in-pre-training-of-large-language-models"/>
    <link href="https://arxiv.org/abs/2505.12082" rel="related"/>
    <updated>2025-05-16T16:00:00+00:00</updated>
    <category term="LLM"/>
    <summary>Model merging has emerged as a promising technique for enhancing large language models, though its application in large-scale pre-training remains relatively unexplored. In this paper, we present a comprehensive investigation of model merging techniques during the pre-training process. Through extensive experiments with both dense and Mixture-of-Experts (MoE) architectures ranging from millions to over 100 billion parameters, we demonstrate that merging checkpoints trained with constant learning rates not only achieves significant performance improvements but also enables accurate prediction of annealing behavior. These improvements lead to both more efficient model development and significantly lower training costs. Our detailed ablation studies on merging strategies and hyperparameters provide new insights into the underlying mechanisms while uncovering novel applications. Through comprehensive experimental analysis, we offer the open-source community practical pre-training guidelines for effective model merging.</summary>
  </entry>
  <entry>
    <title>Seed1.5-VL Technical Report</title>
    <id>560133989e4b493c5e6e6a0ccf076fd6</id>
    <link href="https://seed.bytedance.com/en/research/seed1-5-vl-technical-report"/>
    <link href="https://arxiv.org/abs/2505.07062" rel="related"/>
    <updated>2025-05-12T16:00:00+00:00</updated>
    <category term="LLM"/>
    <summary>We present Seed1.5-VL, a vision-language foundation model designed to advance general-purpose multimodal understanding and reasoning. Seed1.5-VL is composed with a 532M-parameter vision encoder and a Mixture-of-Experts (MoE) LLM of 20B active parameters. Despite its relatively compact architecture, it delivers strong performance across a wide spectrum of public VLM benchmarks and internal evaluation suites, achieving the state-of-the-art performance on 38 out of 60 public benchmarks. Moreover, in agent-centric tasks such as GUI control and gameplay, Seed1.5-VL outperforms leading multimodal systems, including OpenAI CUA and Claude 3.7. Beyond visual and video understanding, it also demonstrates strong reasoning abilities, making it particularly effective for multimodal reasoning challenges such as visual puzzles. We believe these capabilities will empower broader applications across diverse tasks. In this report, we mainly provide a comprehensive review of our experiences in building Seed1.5-VL across model design, data construction, and training at various stages, hoping that this report can inspire further research. Seed1.5-VL is now accessible at this https URL (Volcano Engine Model ID: doubao-1-5-thinking-vision-pro-250428)</summary>
  </entry>
  <entry>
    <title>Understanding Stragglers in Large Model Training Using What-if Analysis</title>
    <id>1e705737ff78d44f85a4abfc42e6ad38</id>
    <link href="https://seed.bytedance.com/en/research/understanding-stragglers-in-large-model-training-using-what-if-analysis"/>
    <link href="https://arxiv.org/abs/2505.05713" rel="related"/>
    <updated>2025-05-08T16:00:00+00:00</updated>
    <category term="Cluster Computing"/>
    <summary>Large language model (LLM) training is one of the most demanding distributed computations today, often requiring thousands of GPUs with frequent synchronization across machines. Such a workload pattern makes it susceptible to stragglers, where the training can be stalled by few slow workers. At ByteDance we find stragglers are not trivially always caused by hardware failures, but can arise from multiple complex factors. This work aims to present a comprehensive study on the straggler issues in LLM training, using a five-month trace collected from our ByteDance LLM training cluster. The core methodology is what-if analysis that simulates the scenario without any stragglers and contrasts with the actual case. We use this method to study the following questions: (1) how often do stragglers affect training jobs, and what effect do they have on job performance; (2) do stragglers exhibit temporal or spatial patterns; and (3) what are the potential root causes for stragglers?</summary>
  </entry>
  <entry>
    <title>Let the Code LLM Edit Itself When You Edit the Code</title>
    <id>354fe89d8447233763c54f5b1a16e3c3</id>
    <link href="https://seed.bytedance.com/en/research/let-the-code-llm-edit-itself-when-you-edit-the-code"/>
    <link href="https://arxiv.org/abs/2407.03157" rel="related"/>
    <updated>2025-04-23T16:00:00+00:00</updated>
    <category term="NLP"/>
    <summary>In this work, we investigate a typical scenario in code generation where a developer edits existing code in real time and requests a code assistant, e.g., a large language model, to re-predict the next token or next line on the fly. Naively, the LLM needs to re-encode the entire KV cache to provide an accurate prediction. However, this process is computationally expensive, especially when the sequence length is long. Simply encoding the edited subsequence and integrating it to the original KV cache meets the temporal confusion problem, leading to significantly worse performance. We address this efficiency and accuracy trade-off by introducing \underline{\textbf{Positional \textbf{I}ntegrity \textbf{E}ncoding} (PIE). Building upon the rotary positional encoding, PIE first removes the rotary matrices in the Key cache that introduce temporal confusion and then reapplies the correct rotary matrices. This process ensures that positional relationships between tokens are correct and requires only a single round of matrix multiplication. We validate the effectiveness of PIE through extensive experiments on the RepoBench-C-8k dataset, utilizing DeepSeek-Coder models with 1.3B, 6.7B, and 33B parameters. Our evaluation includes three real-world coding tasks: code insertion, code deletion, and multi-place code editing. Results demonstrate that PIE reduces computational overhead by over 85% compared to the standard full recomputation approach across all model sizes and tasks while well approximating the model performance.</summary>
  </entry>
  <entry>
    <title>Hyper-Connections</title>
    <id>dd0a49d0aefc7d1d7c0f17b173d42e16</id>
    <link href="https://seed.bytedance.com/en/research/hyper-connections"/>
    <link href="https://arxiv.org/abs/2409.19606" rel="related"/>
    <updated>2025-03-17T16:00:00+00:00</updated>
    <category term="LLM"/>
    <summary>We present hyper-connections, a simple yet effective method that can serve as an alternative to residual connections. This approach specifically addresses common drawbacks observed in residual connection variants, such as the seesaw effect between gradient vanishing and representation collapse. Theoretically, hyper-connections allow the network to adjust the strength of connections between features at different depths and dynamically rearrange layers. We conduct experiments focusing on the pre-training of large language models, including dense and sparse models, where hyper-connections show significant performance improvements over residual connections. Additional experiments conducted on vision tasks also demonstrate similar improvements. We anticipate that this method will be broadly applicable and beneficial across a wide range of AI problems.</summary>
  </entry>
  <entry>
    <title>FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference</title>
    <id>e20ed697e21ba4f6380983be956a2593</id>
    <link href="https://seed.bytedance.com/en/research/flexprefill-a-context-aware-sparse-attention-mechanism-for-efficient-long-sequence-inference"/>
    <link href="https://arxiv.org/abs/2502.20766" rel="related"/>
    <updated>2025-02-27T16:00:00+00:00</updated>
    <category term="Core Machine Learning"/>
    <summary>Large language models (LLMs) encounter computational challenges during long-sequence inference, especially in the attention pre-filling phase, where the complexity grows quadratically with the prompt length. Previous efforts to mitigate these challenges have relied on fixed sparse attention patterns or identifying sparse attention patterns based on limited cases. However, these methods lacked the flexibility to efficiently adapt to varying input demands. In this paper, we introduce FlexPrefill, a Flexible sparse Pre-filling mechanism that dynamically adjusts sparse attention patterns and computational budget in real-time to meet the specific requirements of each input and attention head. The flexibility of our method is demonstrated through two key innovations: 1) Query-Aware Sparse Pattern Determination: By measuring Jensen-Shannon divergence, this component adaptively switches between query-specific diverse attention patterns and predefined attention patterns. 2) Cumulative-Attention Based Index Selection: This component dynamically selects query-key indexes to be computed based on different attention patterns, ensuring the sum of attention scores meets a predefined threshold. FlexPrefill adaptively optimizes the sparse pattern and sparse ratio of each attention head based on the prompt, enhancing efficiency in long-sequence inference tasks. Experimental results show significant improvements in both speed and accuracy over prior methods, providing a more flexible and efficient solution for LLM inference.</summary>
  </entry>
  <entry>
    <title>Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts</title>
    <id>332fbabd3bf042180a41e26d3db60b1b</id>
    <link href="https://seed.bytedance.com/en/research/comet-fine-grained-computation-communication-overlapping-for-mixture-of-experts"/>
    <link href="https://arxiv.org/abs/2502.19811" rel="related"/>
    <updated>2025-02-26T16:00:00+00:00</updated>
    <category term="System Research"/>
    <summary>Mixture-of-experts (MoE) has been extensively employed to scale large language models to trillion-plus parameters while maintaining a fixed computational cost. The development of large MoE models in the distributed scenario encounters the problem of large communication overhead. The inter-device communication of a MoE layer can occupy 47% time of the entire model execution with popular models and frameworks. Therefore, existing methods suggest the communication in a MoE layer to be pipelined with the computation for overlapping. However, these coarse grained overlapping schemes introduce a notable impairment of computational efficiency and the latency concealing is sub-optimal.
To this end, we present COMET, an optimized MoE system with fine-grained communication-computation overlapping. Leveraging data dependency analysis and task rescheduling, COMET achieves precise fine-grained overlapping of communication and computation. Through adaptive workload assignment, COMET effectively eliminates fine-grained communication bottlenecks and enhances its adaptability across various scenarios. Our evaluation shows that COMET accelerates the execution of a single MoE layer by 1.96Ã— and for end-to-end execution, COMET delivers a 1.71Ã— speedup on average. COMET has been adopted in the production environment of clusters with ten-thousand-scale of GPUs, achieving savings of millions of GPU hours.</summary>
  </entry>
  <entry>
    <title>Ultra-Sparse Memory Network</title>
    <id>6eda73661c79900e17a91a419ca58119</id>
    <link href="https://seed.bytedance.com/en/research/ultra-sparse-memory-network"/>
    <link href="https://arxiv.org/abs/2411.12364" rel="related"/>
    <updated>2025-02-05T16:00:00+00:00</updated>
    <category term="LLM"/>
    <summary>It is widely acknowledged that the performance of Transformer models is logarithmically related to their number of parameters and computational complexity. While approaches like Mixture of Experts (MoE) decouple parameter count from computational complexity, they still face challenges in inference due to high memory access costs. This work introduces UltraMem, incorporating large-scale, ultra-sparse memory layer to address these limitations. Our approach significantly reduces inference latency while maintaining model performance. We also investigate the scaling laws of this new architecture, demonstrating that it not only exhibits favorable scaling properties but outperforms MoE. In experiments, the largest UltraMem we train has 20 million memory slots. The results show that our method achieves state-of-the-art inference speed and model performance within a given computational budget, paving the way for billions of slots or experts.</summary>
  </entry>
  <entry>
    <title>BFS-Prover: Scalable Best-First Tree Search for LLM-based Automatic Theorem Proving</title>
    <id>854fe3d82848b97839c5a6ec02d3dccb</id>
    <link href="https://seed.bytedance.com/en/research/bfs-prover-scalable-best-first-tree-search-for-llm-based-automatic-theorem-proving"/>
    <link href="https://arxiv.org/abs/2502.03438" rel="related"/>
    <updated>2025-02-04T16:00:00+00:00</updated>
    <category term="LLM"/>
    <summary>Recent advancements in large language models (LLMs) have spurred growing interest in automatic theorem proving using Lean4, where effective tree search methods are crucial for navigating proof search spaces. While the existing approaches primarily rely on value functions and Monte Carlo Tree Search (MCTS), the potential of simpler methods like Best-First Search (BFS) remains underexplored. This paper investigates whether BFS can achieve competitive performance in large-scale theorem proving tasks. We present \texttt{BFS-Prover}, a scalable expert iteration framework, featuring three key innovations. First, we implement strategic data filtering at each expert iteration round, excluding problems solvable via beam search node expansion to focus on harder cases. Second, we improve the sample efficiency of BFS through Direct Preference Optimization (DPO) applied to state-tactic pairs automatically annotated with compiler error feedback, refining the LLM's policy to prioritize productive expansions. Third, we employ length normalization in BFS to encourage exploration of deeper proof paths. \texttt{BFS-Prover} achieves a score of 71.31 on the MiniF2F test set and therefore challenges the perceived necessity of complex tree search methods, demonstrating that BFS can achieve competitive performance when properly scaled.</summary>
  </entry>
  <entry>
    <title>DSTC: Direct Preference Learning with Only Self-Generated Tests and Code to Improve Code LMs</title>
    <id>191274ce8280034df09ab67a0b8ab227</id>
    <link href="https://seed.bytedance.com/en/research/dstc-direct-preference-learning-with-only-self-generated-tests-and-code-to-improve-code-lms"/>
    <link href="https://arxiv.org/abs/2411.13611" rel="related"/>
    <updated>2024-11-19T16:00:00+00:00</updated>
    <category term="Code Generation"/>
    <summary>Direct preference learning offers a promising and computation-efficient beyond supervised fine-tuning (SFT) for improving code generation in coding large language models (LMs). However, the scarcity of reliable preference data is a bottleneck for the performance of direct preference learning to improve the coding accuracy of code LMs. In this paper, we introduce \underline{\textbf{D}}irect Preference Learning with Only \underline{\textbf{S}}elf-Generated \underline{\textbf{T}}ests and \underline{\textbf{C}}ode (DSTC), a framework that leverages only self-generated code snippets and tests to construct reliable preference pairs such that direct preference learning can improve LM coding accuracy without external annotations. DSTC combines a minimax selection process and test-code concatenation to improve preference pair quality, reducing the influence of incorrect self-generated tests and enhancing model performance without the need for costly reward models. When applied with direct preference learning methods such as Direct Preference Optimization (DPO) and Kahneman-Tversky Optimization (KTO), DSTC yields stable improvements in coding accuracy (pass@1 score) across diverse coding benchmarks, including HumanEval, MBPP, and BigCodeBench, demonstrating both its effectiveness and scalability for models of various sizes. This approach autonomously enhances code generation accuracy across LLMs of varying sizes, reducing reliance on expensive annotated coding datasets.</summary>
  </entry>
  <entry>
    <title>LSH-MoE: Communication-efficient MoE Training via Locality-Sensitive Hashing</title>
    <id>bcfaf2dc3cc4d99c9de472551d36c7f3</id>
    <link href="https://seed.bytedance.com/en/research/lsh-moe-communication-efficient-moe-training-via-locality-sensitive-hashing"/>
    <link href="https://arxiv.org/abs/2411.08446" rel="related"/>
    <updated>2024-11-12T16:00:00+00:00</updated>
    <category term="LLM"/>
    <summary>Larger transformer models always perform better on various tasks but require more costs to scale up the model size. To efficiently enlarge models, the mixture-of-experts (MoE) architecture is widely adopted, which consists of a gate network and a series of experts and keep the training cost constant by routing the input data to a fixed number of experts instead of all. In existing large-scale MoE training systems, experts would be distributed among different GPUs for parallelization, and thus input data requires additional all-to-all communications to access the target experts and conduct corresponding computations. However, upon evaluating the training process of three mainstream MoE models on commonly used GPU clusters, we found that the all-to-all communication ratio averaged around 45%, which significantly hinders the efficiency and scalability of training MoE models.
In this paper, we propose LSH-MoE, a communication-efficient MoE training framework using locality-sensitive hashing (LSH). We first present the problems of scaling MoE training in existing systems and highlight the potential of exploiting token similarity to facilitate data compression. Then, we introduce an efficient LSH-based compression technique, which utilizes the cross-polytope hashing for rapid clustering and implements a residual-based error compensation scheme to alleviate the adverse impact of compression. To verify the effectiveness of our methods, we conduct experiments on both language models (e.g., RoBERTa, GPT, and T5) and vision models (e.g., Swin) for pre-training and fine-tuning tasks. The results demonstrate that our method substantially outperforms its counterparts across different tasks by 1.28x - 2.2x of speedup.</summary>
  </entry>
  <entry>
    <title>Polynomial Composition Activations: Unleashing the Dynamics of Large Language Models</title>
    <id>42f779fff6e040639f69415ab08dc5b2</id>
    <link href="https://seed.bytedance.com/en/research/polynomial-composition-activations-unleashing-the-dynamics-of-large-language-models"/>
    <link href="https://arxiv.org/abs/2411.03884" rel="related"/>
    <updated>2024-11-05T16:00:00+00:00</updated>
    <category term="LLM"/>
    <summary>Transformers have found extensive applications across various domains due to the powerful fitting capabilities. This success can be partially attributed to their inherent nonlinearity. Thus, in addition to the ReLU function employed in the original transformer architecture, researchers have explored alternative modules such as GeLU and SwishGLU to enhance nonlinearity and thereby augment representational capacity. In this paper, we propose a novel category of polynomial composition activations (PolyCom), designed to optimize the dynamics of transformers. Theoretically, we provide a comprehensive mathematical analysis of PolyCom, highlighting its enhanced expressivity and efficacy relative to other activation functions. Notably, we demonstrate that networks incorporating PolyCom achieve the optimal approximation rate, indicating that PolyCom networks require minimal parameters to approximate general smooth functions in Sobolev spaces. We conduct empirical experiments on the pre-training configurations of large language models (LLMs), including both dense and sparse architectures. By substituting conventional activation functions with PolyCom, we enable LLMs to capture higher-order interactions within the data, thus improving performance metrics in terms of accuracy and convergence rates. Extensive experimental results demonstrate the effectiveness of our method, showing substantial improvements over other activation functions. Code is available at this https URL.</summary>
  </entry>
  <entry>
    <title>HybridFlow: A Flexible and Efficient RLHF Framework</title>
    <id>36c3096dd80939f865809ff47cecf79c</id>
    <link href="https://seed.bytedance.com/en/research/hybridflow-a-flexible-and-efficient-rlhf-framework"/>
    <link href="https://arxiv.org/abs/2409.19256" rel="related"/>
    <updated>2024-10-01T16:00:00+00:00</updated>
    <category term="Reinforcement Learning"/>
    <category term="System Research"/>
    <summary>Reinforcement Learning from Human Feedback (RLHF) is widely used in Large Language Model (LLM) alignment. Traditional RL can be modeled as a dataflow, where each node represents computation of a neural network (NN) and each edge denotes data dependencies between the NNs. RLHF complicates the dataflow by expanding each node into a distributed LLM training or generation program, and each edge into a many-to-many multicast. Traditional RL frameworks execute the dataflow using a single controller to instruct both intra-node computation and inter-node communication, which can be inefficient in RLHF due to large control dispatch overhead for distributed intra-node computation. Existing RLHF systems adopt a multi-controller paradigm, which can be inflexible due to nesting distributed computation and data communication. We propose HybridFlow, which combines single-controller and multi-controller paradigms in a hybrid manner to enable flexible representation and efficient execution of the RLHF dataflow. We carefully design a set of hierarchical APIs that decouple and encapsulate computation and data dependencies in the complex RLHF dataflow, allowing efficient operation orchestration to implement RLHF algorithms and flexible mapping of the computation onto various devices. We further design a 3D-HybridEngine for efficient actor model resharding between training and generation phases, with zero memory redundancy and significantly reduced communication overhead. Our experimental results demonstrate 1.53Ã—~20.57Ã— throughput improvement when running various RLHF algorithms using HybridFlow, as compared with state-of-the-art baselines. HybridFlow source code will be available at this https URLï¼ˆhttps://github.com/volcengine/verlï¼‰.</summary>
  </entry>
  <entry>
    <title>MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs</title>
    <id>c1e3ddb9c963404f531826228de71cf4</id>
    <link href="https://seed.bytedance.com/en/research/megascale-scaling-large-language-model-training-to-more-than-10-000-gpus"/>
    <link href="https://arxiv.org/abs/2402.15627" rel="related"/>
    <updated>2024-02-22T16:00:00+00:00</updated>
    <category term="System Research"/>
    <summary>We present the design, implementation and engineering experience in building and deploying MegaScale, a production system for training large language models (LLMs) at the scale of more than 10,000 GPUs. Training LLMs at this scale brings unprecedented challenges to training efficiency and stability. We take a full-stack approach that co-designs the algorithmic and system components across model block and optimizer design, computation and communication overlapping, operator optimization, data pipeline, and network performance tuning. Maintaining high efficiency throughout the training process (i.e., stability) is an important consideration in production given the long extent of LLM training jobs. Many hard stability issues only emerge at large scale, and in-depth observability is the key to address them. We develop a set of diagnosis tools to monitor system components and events deep in the stack, identify root causes, and derive effective techniques to achieve fault tolerance and mitigate stragglers. MegaScale achieves 55.2% Model FLOPs Utilization (MFU) when training a 175B LLM model on 12,288 GPUs, improving the MFU by 1.34x compared to Megatron-LM. We share our operational experience in identifying and fixing failures and stragglers. We hope by articulating the problems and sharing our experience from a systems perspective, this work can inspire future LLM systems research.</summary>
  </entry>
</feed>
