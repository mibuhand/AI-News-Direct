<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Huggingface Daily Papers</title>
  <id>tag:ai-news-direct.local,2025:huggingface_daily_papers</id>
  <icon>https://huggingface.co/favicon.ico</icon>
  <logo>https://huggingface.co/favicon.ico</logo>
  <updated>2026-02-03T21:36:43.825614+00:00</updated>
  <author>
    <name>AI News Direct</name>
  </author>
  <link href="https://raw.githubusercontent.com/mibuhand/AI-News-Direct/main/feeds/huggingface_daily_papers.xml" rel="self"/>
  <entry>
    <title>GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization</title>
    <id>9f022f923749f1889fd7797edcd67136</id>
    <link href="https://arxiv.org/abs/2601.05242"/>
    <updated>2026-01-08T13:59:24+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To ach...&lt;br/&gt;Upvotes: 218&lt;br/&gt;GitHub Stars: 362&lt;br/&gt;Authors: Shih-Yang Liu, Xin Dong, Ximing Lu&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/NVlabs/GDPO&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://nvlabs.github.io/GDPO/&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2601.05242&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning</title>
    <id>01505ff128b7cc4069a838f067f22852</id>
    <link href="https://arxiv.org/abs/2601.06943"/>
    <updated>2026-01-11T10:07:37+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perfor...&lt;br/&gt;Upvotes: 209&lt;br/&gt;GitHub Stars: 137&lt;br/&gt;Authors: Chengwen Liu, Xiaomin Yu, Zhuoyue Chang&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/QuantaAlpha/VideoDR-Benchmark&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://videodr-benchmark.github.io/#/home&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2601.06943&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>BabyVision: Visual Reasoning Beyond Language</title>
    <id>7d8f529e24394b1b5750aeba9852eed4</id>
    <link href="https://arxiv.org/abs/2601.06521"/>
    <updated>2026-01-10T05:42:44+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understandin...&lt;br/&gt;Upvotes: 195&lt;br/&gt;GitHub Stars: 173&lt;br/&gt;Authors: Liang Chen, Weichu Xie, Yiyan Liang&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/UniPat-AI/BabyVision&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://unipat.ai/blog/BabyVision&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2601.06521&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>STEP3-VL-10B Technical Report</title>
    <id>ed6630b4b50db2ee02e2a5ddb5f32ac1</id>
    <link href="https://arxiv.org/abs/2601.09668"/>
    <updated>2026-01-14T12:58:24+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized t...&lt;br/&gt;Upvotes: 193&lt;br/&gt;GitHub Stars: 387&lt;br/&gt;Authors: Ailin Huang, Chengyuan Yao, Chunrui Han&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/stepfun-ai/Step3-VL-10B&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://stepfun-ai.github.io/Step3-VL-10B&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2601.09668&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>Agentic Reasoning for Large Language Models</title>
    <id>c030a09f9828ad54ac0c86051210dce0</id>
    <link href="https://arxiv.org/abs/2601.12538"/>
    <updated>2026-01-18T13:58:23+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world se...&lt;br/&gt;Upvotes: 187&lt;br/&gt;GitHub Stars: 865&lt;br/&gt;Authors: Tianxin Wei, Ting-Wei Li, Zhining Liu&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/weitianxin/Awesome-Agentic-Reasoning&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2601.12538&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>Can LLMs Clean Up Your Mess? A Survey of Application-Ready Data Preparation with LLMs</title>
    <id>2331688c611695dbe0b6076557332b6c</id>
    <link href="https://arxiv.org/abs/2601.17058"/>
    <updated>2026-01-22T07:02:45+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>Data preparation aims to denoise raw datasets, uncover cross-dataset relationships, and extract valuable insights from them, which is essential for a wide range of data-centric applications. Driven by...&lt;br/&gt;Upvotes: 181&lt;br/&gt;GitHub Stars: 682&lt;br/&gt;Authors: Wei Zhou, Jun Zhou, Haoyu Wang&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/weAIDB/awesome-data-llm&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/weAIDB/awesome-data-llm&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2601.17058&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>Qwen3-TTS Technical Report</title>
    <id>e6cd663cc8eaf53e2ed564e037a90af3</id>
    <link href="https://arxiv.org/abs/2601.15621"/>
    <updated>2026-01-21T22:51:43+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>In this report, we present the Qwen3-TTS series, a family of advanced multilingual, controllable, robust, and streaming text-to-speech models. Qwen3-TTS supports state-of-the-art 3-second voice clonin...&lt;br/&gt;Upvotes: 56&lt;br/&gt;GitHub Stars: 6732&lt;br/&gt;Authors: Hangrui Hu, Xinfa Zhu, Ting He&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/QwenLM/Qwen3-TTS&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2601.15621&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization</title>
    <id>4abe591dec20d02c68570faea169dfcb</id>
    <link href="https://arxiv.org/abs/2512.24615"/>
    <updated>2025-12-30T23:17:36+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual ef...&lt;br/&gt;Upvotes: 119&lt;br/&gt;GitHub Stars: 4359&lt;br/&gt;Authors: Yuchen Shi, Yuzheng Cai, Siqi Cai&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/TencentCloudADP/youtu-agent&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://tencentcloudadp.github.io/youtu-agent/&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2512.24615&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>LTX-2: Efficient Joint Audio-Visual Foundation Model</title>
    <id>7f5474609ca7dc9a4bb2a0fecae8667c</id>
    <link href="https://arxiv.org/abs/2601.03233"/>
    <updated>2026-01-06T13:24:41+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>Recent text-to-video diffusion models can generate compelling video sequences, yet they remain silent -- missing the semantic, emotional, and atmospheric cues that audio provides. We introduce LTX-2, ...&lt;br/&gt;Upvotes: 146&lt;br/&gt;GitHub Stars: 3466&lt;br/&gt;Authors: Yoav HaCohen, Benny Brazowski, Nisan Chiprut&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/Lightricks/LTX-2&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://app.ltx.studio/ltx-2-playground/i2v&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2601.03233&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>HeartMuLa: A Family of Open Sourced Music Foundation Models</title>
    <id>e4456066efdcdf46fe4b108089570f93</id>
    <link href="https://arxiv.org/abs/2601.10547"/>
    <updated>2026-01-15T11:14:25+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>We present a family of open-source Music Foundation Models designed to advance large-scale music understanding and generation across diverse tasks and modalities. Our framework consists of four major ...&lt;br/&gt;Upvotes: 41&lt;br/&gt;GitHub Stars: 2854&lt;br/&gt;Authors: Dongchao Yang, Yuxin Xie, Yuguo Yin&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/HeartMuLa/heartlib&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://heartmula.github.io/&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2601.10547&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>SimpleMem: Efficient Lifelong Memory for LLM Agents</title>
    <id>42acbf549db8c79c9ce60d3d38eb8c52</id>
    <link href="https://arxiv.org/abs/2601.02553"/>
    <updated>2026-01-05T16:02:49+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>To support reliable long-term interaction in complex environments, LLM agents require memory systems that efficiently manage historical experiences. Existing approaches either retain full interaction ...&lt;br/&gt;Upvotes: 36&lt;br/&gt;GitHub Stars: 2666&lt;br/&gt;Authors: Jiaqi Liu, Yaofeng Su, Peng Xia&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/aiming-lab/SimpleMem&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://aiming-lab.github.io/SimpleMem-Page/&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2601.02553&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>Advancing Open-source World Models</title>
    <id>de0ab4deaf169a5f7442c17a4444ef3f</id>
    <link href="https://arxiv.org/abs/2601.20540"/>
    <updated>2026-01-28T07:37:01+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>We present LingBot-World, an open-sourced world simulator stemming from video generation. Positioned as a top-tier world model, LingBot-World offers the following features. (1) It maintains high fidel...&lt;br/&gt;Upvotes: 109&lt;br/&gt;GitHub Stars: 2279&lt;br/&gt;Authors: Robbyant Team, Zelin Gao, Qiuyu Wang&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/Robbyant/lingbot-world/&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://technology.robbyant.com/lingbot-world&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2601.20540&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
</feed>
