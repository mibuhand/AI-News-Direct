<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Huggingface Daily Papers</title>
  <id>tag:ai-news-direct.local,2025:huggingface_daily_papers</id>
  <icon>https://huggingface.co/favicon.ico</icon>
  <logo>https://huggingface.co/favicon.ico</logo>
  <updated>2025-12-04T09:30:02.815005+00:00</updated>
  <author>
    <name>AI News Direct</name>
  </author>
  <link href="https://raw.githubusercontent.com/mibuhand/AI-News-Direct/main/feeds/huggingface_daily_papers.xml" rel="self"/>
  <entry>
    <title>Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation</title>
    <id>0139ab6619114102f84bcc9c449fd176</id>
    <link href="https://arxiv.org/abs/2511.14993"/>
    <updated>2025-11-18T19:23:22+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kand...&lt;br/&gt;Upvotes: 220&lt;br/&gt;GitHub Stars: 518&lt;br/&gt;Authors: Vladimir Arkhipkin, Vladimir Korviakov, Nikolai Gerasimenko&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/kandinskylab/kandinsky-5&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://kandinskylab.ai/&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2511.14993&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence</title>
    <id>7e43a061a51737f2587e45e69aa58ff1</id>
    <link href="https://arxiv.org/abs/2511.18538"/>
    <updated>2025-11-23T12:09:34+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adop...&lt;br/&gt;Upvotes: 218&lt;br/&gt;Authors: Jian Yang, Xianglong Liu, Weifeng Lv&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2511.18538&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>Thinking with Video: Video Generation as a Promising Multimodal
  Reasoning Paradigm</title>
    <id>b81e62a9e2bd28a9fe78c760c27308b6</id>
    <link href="https://arxiv.org/abs/2511.04570"/>
    <updated>2025-11-06T12:25:23+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>&quot;Thinking with Text&quot; and &quot;Thinking with Images&quot; paradigm significantly
improve the reasoning ability of large language models (LLMs) and Vision
Language Models (VLMs). However, these paradigms have in...&lt;br/&gt;Upvotes: 208&lt;br/&gt;GitHub Stars: 215&lt;br/&gt;Authors: Jingqi Tong, Yurong Mou, Hangcheng Li&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/tongjingqi/Thinking-with-Video&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://thinking-with-video.github.io/&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2511.04570&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds</title>
    <id>4622a47084403f1652a1baac4335b4fa</id>
    <link href="https://arxiv.org/abs/2511.08892"/>
    <updated>2025-11-11T21:01:26+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>We introduce Lumine, the first open recipe for developing generalist agents capable of completing hours-long complex missions in real time within challenging 3D open-world environments. Lumine adopts ...&lt;br/&gt;Upvotes: 192&lt;br/&gt;Authors: Weihao Tan, Xiangyang Li, Yunhao Fang&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://www.lumine-ai.org/&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2511.08892&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>ROOT: Robust Orthogonalized Optimizer for Neural Network Training</title>
    <id>307d918f05f7df6902e7de22a3fa64bc</id>
    <link href="https://arxiv.org/abs/2511.20626"/>
    <updated>2025-11-25T13:48:05+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advance...&lt;br/&gt;Upvotes: 167&lt;br/&gt;GitHub Stars: 921&lt;br/&gt;Authors: Wei He, Kai Han, Hang Zhou&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/huawei-noah/noah-research&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/huawei-noah/noah-research/tree/master/ROOT&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2511.20626&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling</title>
    <id>0a60cb0f535685cc7eb12b1b01de62bc</id>
    <link href="https://arxiv.org/abs/2511.11793"/>
    <updated>2025-11-14T13:52:07+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>We present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or co...&lt;br/&gt;Upvotes: 156&lt;br/&gt;GitHub Stars: 1212&lt;br/&gt;Authors: MiroMind Team, Song Bai, Lidong Bing&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/MiroMindAI/MiroThinker&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://dr.miromind.ai/&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2511.11793&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>UFO^3: Weaving the Digital Agent Galaxy</title>
    <id>f37a636c7cc5f515c6866d60bda27643</id>
    <link href="https://arxiv.org/abs/2511.11332"/>
    <updated>2025-11-14T09:05:31+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>Large language model (LLM)-powered agents are transforming digital devices from passive tools into proactive intelligent collaborators. However, most existing frameworks remain confined to a single OS...&lt;br/&gt;Upvotes: 18&lt;br/&gt;GitHub Stars: 7760&lt;br/&gt;Authors: Chaoyun Zhang, Liqun Li, He Huang&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/microsoft/UFO/&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://microsoft.github.io/UFO/&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2511.11332&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>SAM 3: Segment Anything with Concepts</title>
    <id>3c094172f9e0e4e808f9c218d4d266f1</id>
    <link href="https://arxiv.org/abs/2511.16719"/>
    <updated>2025-11-20T13:59:56+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>We present Segment Anything Model (SAM) 3, a unified model that detects, segments, and tracks objects in images and videos based on concept prompts, which we define as either short noun phrases (e.g.,...&lt;br/&gt;Upvotes: 100&lt;br/&gt;GitHub Stars: 5117&lt;br/&gt;Authors: Nicolas Carion, Laura Gustafson, Yuan-Ting Hu&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/facebookresearch/sam3&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://ai.meta.com/sam3/&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2511.16719&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>Decoupled DMD: CFG Augmentation as the Spear, Distribution Matching as the Shield</title>
    <id>18503dda198b1fcbf7bb01dbf7296d31</id>
    <link href="https://arxiv.org/abs/2511.22677"/>
    <updated>2025-11-27T13:24:28+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>Diffusion model distillation has emerged as a powerful technique for creating efficient few-step and single-step generators. Among these, Distribution Matching Distillation (DMD) and its variants stan...&lt;br/&gt;Upvotes: 16&lt;br/&gt;GitHub Stars: 4408&lt;br/&gt;Authors: Dongyang Liu, Peng Gao, David Liu&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/Tongyi-MAI/Z-Image/tree/main&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://tongyi-mai.github.io/Z-Image-blog/&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2511.22677&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>SAM 3D: 3Dfy Anything in Images</title>
    <id>979bc288b697d0a635887e56b67befef</id>
    <link href="https://arxiv.org/abs/2511.16624"/>
    <updated>2025-11-20T13:31:46+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>We present SAM 3D, a generative model for visually grounded 3D object reconstruction, predicting geometry, texture, and layout from a single image. SAM 3D excels in natural images, where occlusion and...&lt;br/&gt;Upvotes: 104&lt;br/&gt;GitHub Stars: 4401&lt;br/&gt;Authors: SAM 3D Team, Xingyu Chen, Fu-Jen Chu&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/facebookresearch/sam-3d-objects&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://ai.meta.com/sam3d/&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2511.16624&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer</title>
    <id>8e9429d7feaa43a78c484db81c3eddf8</id>
    <link href="https://arxiv.org/abs/2511.22699"/>
    <updated>2025-11-27T13:52:07+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Imag...&lt;br/&gt;Upvotes: 129&lt;br/&gt;GitHub Stars: 4389&lt;br/&gt;Authors: Z-Image Team, Huanqia Cai, Sihan Cao&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/Tongyi-MAI/Z-Image&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://tongyi-mai.github.io/Z-Image-blog/&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2511.22699&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>Black-Box On-Policy Distillation of Large Language Models</title>
    <id>56964b5c4b70a7c48e204a4541b8da35</id>
    <link href="https://arxiv.org/abs/2511.10643"/>
    <updated>2025-11-13T13:58:37+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>Black-box distillation creates student large language models (LLMs) by learning from a proprietary teacher model's text outputs alone, without access to its internal logits or parameters. In this work...&lt;br/&gt;Upvotes: 46&lt;br/&gt;GitHub Stars: 4216&lt;br/&gt;Authors: Tianzhu Ye, Li Dong, Zewen Chi&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/microsoft/LMOps/tree/main/gad&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://aka.ms/GAD-project&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2511.10643&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
</feed>
