<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Huggingface Daily Papers</title>
  <id>tag:ai-news-direct.local,2025:huggingface_daily_papers</id>
  <icon>https://huggingface.co/favicon.ico</icon>
  <logo>https://huggingface.co/favicon.ico</logo>
  <updated>2025-12-22T21:20:00.177538+00:00</updated>
  <author>
    <name>AI News Direct</name>
  </author>
  <link href="https://raw.githubusercontent.com/mibuhand/AI-News-Direct/main/feeds/huggingface_daily_papers.xml" rel="self"/>
  <entry>
    <title>From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence</title>
    <id>7e43a061a51737f2587e45e69aa58ff1</id>
    <link href="https://arxiv.org/abs/2511.18538"/>
    <updated>2025-11-23T12:09:34+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adop...&lt;br/&gt;Upvotes: 272&lt;br/&gt;Authors: Jian Yang, Xianglong Liu, Weifeng Lv&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2511.18538&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models</title>
    <id>c360b805a86862ca56d4316aea6a684d</id>
    <link href="https://arxiv.org/abs/2512.02556"/>
    <updated>2025-12-02T04:25:14+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) De...&lt;br/&gt;Upvotes: 223&lt;br/&gt;Authors: DeepSeek-AI, Aixin Liu, Aoxue Mei&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2512.02556&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer</title>
    <id>8e9429d7feaa43a78c484db81c3eddf8</id>
    <link href="https://arxiv.org/abs/2511.22699"/>
    <updated>2025-11-27T13:52:07+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Imag...&lt;br/&gt;Upvotes: 207&lt;br/&gt;GitHub Stars: 7639&lt;br/&gt;Authors: Z-Image Team, Huanqia Cai, Sihan Cao&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/Tongyi-MAI/Z-Image&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://tongyi-mai.github.io/Z-Image-blog/&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2511.22699&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length</title>
    <id>bdd6e066d5b32b9c61996386576f3682</id>
    <link href="https://arxiv.org/abs/2512.04677"/>
    <updated>2025-12-04T06:11:24+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audi...&lt;br/&gt;Upvotes: 168&lt;br/&gt;GitHub Stars: 1152&lt;br/&gt;Authors: Yubo Huang, Hailong Guo, Fangtai Wu&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/Alibaba-Quark/LiveAvatar&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://liveavatar.github.io/&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2512.04677&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>General Agentic Memory Via Deep Research</title>
    <id>07bfc80d1c8c45f5b6088d438bb272d2</id>
    <link href="https://arxiv.org/abs/2511.18423"/>
    <updated>2025-11-23T07:29:33+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitati...&lt;br/&gt;Upvotes: 160&lt;br/&gt;GitHub Stars: 734&lt;br/&gt;Authors: B. Y. Yan, Chaofan Li, Hongjin Qian&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/VectorSpaceLab/general-agentic-memory&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/VectorSpaceLab/general-agentic-memory&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2511.18423&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>LongVT: Incentivizing &quot;Thinking with Long Videos&quot; via Native Tool Calling</title>
    <id>0f96413995b3ae42aea420d3714efb11</id>
    <link href="https://arxiv.org/abs/2511.20785"/>
    <updated>2025-11-25T14:22:48+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>Large multimodal models (LMMs) have shown great potential for video reasoning with textual Chain-of-Thought. However, they remain vulnerable to hallucinations, especially when processing long-form vid...&lt;br/&gt;Upvotes: 152&lt;br/&gt;GitHub Stars: 154&lt;br/&gt;Authors: Zuhao Yang, Sudong Wang, Kaichen Zhang&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/EvolvingLMMs-Lab/LongVT&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://evolvinglmms-lab.github.io/LongVT/&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2511.20785&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>Qwen3-VL Technical Report</title>
    <id>5b6ff13ede65701e5f764a0267eca8df</id>
    <link href="https://arxiv.org/abs/2511.21631"/>
    <updated>2025-11-26T12:59:08+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks. It natively supports interleaved...&lt;br/&gt;Upvotes: 131&lt;br/&gt;GitHub Stars: 17328&lt;br/&gt;Authors: Shuai Bai, Yuxuan Cai, Ruizhe Chen&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/QwenLM/Qwen3-VL&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://chat.qwen.ai&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2511.21631&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>DeepCode: Open Agentic Coding</title>
    <id>84e0c9ab3b32b3eab139bdd727fe4f8c</id>
    <link href="https://arxiv.org/abs/2512.07921"/>
    <updated>2025-12-08T11:07:13+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>Recent advances in large language models (LLMs) have given rise to powerful coding agents, making it possible for code assistants to evolve into code engineers. However, existing methods still face si...&lt;br/&gt;Upvotes: 30&lt;br/&gt;GitHub Stars: 12956&lt;br/&gt;Authors: Zongwei Li, Zhonghang Li, Zirui Guo&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/HKUDS/DeepCode&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2512.07921&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>Soft Adaptive Policy Optimization</title>
    <id>4a2bfe8d09c1cb8d7f0252ad4b16ac1e</id>
    <link href="https://arxiv.org/abs/2511.20347"/>
    <updated>2025-11-25T09:25:19+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>Reinforcement learning (RL) plays an increasingly important role in enhancing the reasoning capabilities of large language models (LLMs), yet stable and performant policy optimization remains challeng...&lt;br/&gt;Upvotes: 41&lt;br/&gt;GitHub Stars: 11779&lt;br/&gt;Authors: Chang Gao, Chujie Zheng, Xiong-Hui Chen&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/modelscope/ms-swift&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://swift.readthedocs.io/en/latest/Instruction/GRPO/AdvancedResearch/SAPO.html&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2511.20347&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>Decoupled DMD: CFG Augmentation as the Spear, Distribution Matching as the Shield</title>
    <id>18503dda198b1fcbf7bb01dbf7296d31</id>
    <link href="https://arxiv.org/abs/2511.22677"/>
    <updated>2025-11-27T13:24:28+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>Diffusion model distillation has emerged as a powerful technique for creating efficient few-step and single-step generators. Among these, Distribution Matching Distillation (DMD) and its variants stan...&lt;br/&gt;Upvotes: 27&lt;br/&gt;GitHub Stars: 7616&lt;br/&gt;Authors: Dongyang Liu, Peng Gao, David Liu&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/Tongyi-MAI/Z-Image/tree/main&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://tongyi-mai.github.io/Z-Image-blog/&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2511.22677&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>SAM 3: Segment Anything with Concepts</title>
    <id>3c094172f9e0e4e808f9c218d4d266f1</id>
    <link href="https://arxiv.org/abs/2511.16719"/>
    <updated>2025-11-20T13:59:56+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>We present Segment Anything Model (SAM) 3, a unified model that detects, segments, and tracks objects in images and videos based on concept prompts, which we define as either short noun phrases (e.g.,...&lt;br/&gt;Upvotes: 119&lt;br/&gt;GitHub Stars: 6329&lt;br/&gt;Authors: Nicolas Carion, Laura Gustafson, Yuan-Ting Hu&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/facebookresearch/sam3&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://ai.meta.com/sam3/&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2511.16719&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
</feed>
