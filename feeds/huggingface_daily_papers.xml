<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Huggingface Daily Papers</title>
  <id>tag:ai-news-direct.local,2025:huggingface_daily_papers</id>
  <icon>https://huggingface.co/favicon.ico</icon>
  <logo>https://huggingface.co/favicon.ico</logo>
  <updated>2025-11-05T21:20:12.810123+00:00</updated>
  <author>
    <name>AI News Direct</name>
  </author>
  <link href="https://raw.githubusercontent.com/mibuhand/AI-News-Direct/main/feeds/huggingface_daily_papers.xml" rel="self"/>
  <entry>
    <title>Less is More: Recursive Reasoning with Tiny Networks</title>
    <id>3d55a02b14bdc7a04256ce32475eb075</id>
    <link href="https://arxiv.org/abs/2510.04871"/>
    <updated>2025-10-06T10:58:08+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>Hierarchical Reasoning Model (HRM) is a novel approach using two small neural
networks recursing at different frequencies. This biologically inspired method
beats Large Language models (LLMs) on hard ...&lt;br/&gt;Upvotes: 464&lt;br/&gt;GitHub Stars: 5429&lt;br/&gt;Authors: Alexia Jolicoeur-Martineau&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/SamsungSAILMontreal/TinyRecursiveModels&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://alexiajm.github.io/2025/09/29/tiny_recursive_models.html#&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2510.04871&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>Agent Learning via Early Experience</title>
    <id>b5eadc2228eda785fe3e8da98ea25174</id>
    <link href="https://arxiv.org/abs/2510.08558"/>
    <updated>2025-10-09T13:59:17+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>A long-term goal of language agents is to learn and improve through their own
experience, ultimately outperforming humans in complex, real-world tasks.
However, training agents from experience data wi...&lt;br/&gt;Upvotes: 260&lt;br/&gt;Authors: Kai Zhang, Xiangchao Chen, Bo Liu&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2510.08558&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>Scaling Latent Reasoning via Looped Language Models</title>
    <id>8c7bb673a7c23605ce494554bc937b12</id>
    <link href="https://arxiv.org/abs/2510.25741"/>
    <updated>2025-10-29T13:45:42+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>Modern LLMs are trained to &quot;think&quot; primarily via explicit text generation,
such as chain-of-thought (CoT), which defers reasoning to post-training and
under-leverages pre-training data. We present and...&lt;br/&gt;Upvotes: 198&lt;br/&gt;Authors: Rui-Jie Zhu, Zixuan Wang, Kai Hua&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://ouro-llm.github.io/&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2510.25741&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning
  for LLMs</title>
    <id>f5ecd82d2d6a90e1b4eb7c0d2afe9c3e</id>
    <link href="https://arxiv.org/abs/2510.11696"/>
    <updated>2025-10-13T13:55:09+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>We propose QeRL, a Quantization-enhanced Reinforcement Learning framework for
large language models (LLMs). While RL is essential for LLMs' reasoning
capabilities, it is resource-intensive, requiring ...&lt;br/&gt;Upvotes: 173&lt;br/&gt;GitHub Stars: 410&lt;br/&gt;Authors: Wei Huang, Yi Ge, Shuai Yang&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/NVlabs/QeRL&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/NVlabs/QeRL&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2510.11696&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>Concerto: Joint 2D-3D Self-Supervised Learning Emerges Spatial
  Representations</title>
    <id>9f97533076b36bd588d78f78c4352369</id>
    <link href="https://arxiv.org/abs/2510.23607"/>
    <updated>2025-10-27T13:59:59+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>Humans learn abstract concepts through multisensory synergy, and once formed,
such representations can often be recalled from a single modality. Inspired by
this principle, we introduce Concerto, a mi...&lt;br/&gt;Upvotes: 172&lt;br/&gt;GitHub Stars: 328&lt;br/&gt;Authors: Yujia Zhang, Xiaoyang Wu, Yixing Lao&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/Pointcept/Concerto&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://pointcept.github.io/Concerto/&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2510.23607&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>Diffusion Transformers with Representation Autoencoders</title>
    <id>1f97c06c73ceb6a2aa9ee2bb5f344034</id>
    <link href="https://arxiv.org/abs/2510.11690"/>
    <updated>2025-10-13T13:51:39+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>Latent generative modeling, where a pretrained autoencoder maps pixels into a
latent space for the diffusion process, has become the standard strategy for
Diffusion Transformers (DiT); however, the au...&lt;br/&gt;Upvotes: 160&lt;br/&gt;GitHub Stars: 1479&lt;br/&gt;Authors: Boyang Zheng, Nanye Ma, Shengbang Tong&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/bytetriper/RAE&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://rae-dit.github.io/&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2510.11690&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B
  Ultra-Compact Vision-Language Model</title>
    <id>d98bf11f4930fd1159bb5f90e3f76afd</id>
    <link href="https://arxiv.org/abs/2510.14528"/>
    <updated>2025-10-16T06:18:48+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model
tailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a
compact yet powerful vision-language model (VLM) ...&lt;br/&gt;Upvotes: 81&lt;br/&gt;GitHub Stars: 62711&lt;br/&gt;Authors: Cheng Cui, Ting Sun, Suyin Liang&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2510.14528&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>BitNet Distillation</title>
    <id>4477d6ceb0f6af4c535f87fb4c3e55f5</id>
    <link href="https://arxiv.org/abs/2510.13998"/>
    <updated>2025-10-15T14:28:12+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>In this paper, we present BitNet Distillation (BitDistill), a lightweight
pipeline that fine-tunes off-the-shelf full-precision LLMs (e.g., Qwen) into
1.58-bit precision (i.e., ternary weights {-1, 0,...&lt;br/&gt;Upvotes: 52&lt;br/&gt;GitHub Stars: 24352&lt;br/&gt;Authors: Xun Wu, Shaohan Huang, Wenhui Wang&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/microsoft/BitNet&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2510.13998&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>FAPO: Flawed-Aware Policy Optimization for Efficient and Reliable
  Reasoning</title>
    <id>bf88f955951ceb1c5ba0121481fe4bf5</id>
    <link href="https://arxiv.org/abs/2510.22543"/>
    <updated>2025-10-26T01:49:38+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>Reinforcement learning with verifiable rewards (RLVR) has emerged as a
promising paradigm for enhancing the reasoning capabilities of large language
models (LLMs). In this context, models explore reas...&lt;br/&gt;Upvotes: 6&lt;br/&gt;GitHub Stars: 15137&lt;br/&gt;Authors: Yuyang Ding, Chi Zhang, Juntao Li&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/volcengine/verl/tree/main/recipe/fapo&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://fapo-rl.github.io/&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2510.22543&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>RAG-Anything: All-in-One RAG Framework</title>
    <id>2431e39c5ab567b865fe3f25b69b4d35</id>
    <link href="https://arxiv.org/abs/2510.12323"/>
    <updated>2025-10-14T05:25:35+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>Retrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm
for expanding Large Language Models beyond their static training limitations.
However, a critical misalignment exists between...&lt;br/&gt;Upvotes: 46&lt;br/&gt;GitHub Stars: 9979&lt;br/&gt;Authors: Zirui Guo, Xubin Ren, Lingrui Xu&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/HKUDS/RAG-Anything&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2510.12323&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>Chronos-2: From Univariate to Universal Forecasting</title>
    <id>32811c5aee11a368e90161a3c41bd70b</id>
    <link href="https://arxiv.org/abs/2510.15821"/>
    <updated>2025-10-17T13:00:53+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>Pretrained time series models have enabled inference-only forecasting systems
that produce accurate predictions without task-specific training. However,
existing approaches largely focus on univariate...&lt;br/&gt;Upvotes: 17&lt;br/&gt;GitHub Stars: 4203&lt;br/&gt;Authors: Abdul Fatir Ansari, Oleksandr Shchur, Jaris KÃ¼ken&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/amazon-science/chronos-forecasting&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/amazon/chronos-2&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2510.15821&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
</feed>
