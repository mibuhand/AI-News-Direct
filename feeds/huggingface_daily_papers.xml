<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Huggingface Daily Papers</title>
  <id>tag:ai-news-direct.local,2025:huggingface_daily_papers</id>
  <icon>https://huggingface.co/favicon.ico</icon>
  <logo>https://huggingface.co/favicon.ico</logo>
  <updated>2026-01-08T09:33:08.292784+00:00</updated>
  <author>
    <name>AI News Direct</name>
  </author>
  <link href="https://raw.githubusercontent.com/mibuhand/AI-News-Direct/main/feeds/huggingface_daily_papers.xml" rel="self"/>
  <entry>
    <title>mHC: Manifold-Constrained Hyper-Connections</title>
    <id>2d9fe809957806c0d1ec27e6e37053aa</id>
    <link href="https://arxiv.org/abs/2512.24880"/>
    <updated>2025-12-31T09:16:26+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>Recently, studies exemplified by Hyper-Connections (HC) have extended the ubiquitous residual connection paradigm established over the past decade by expanding the residual stream width and diversifyi...&lt;br/&gt;Upvotes: 225&lt;br/&gt;Authors: Zhenda Xie, Yixuan Wei, Huanqi Cao&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2512.24880&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI</title>
    <id>adcafb2025f84cc88e13db2e967d246f</id>
    <link href="https://arxiv.org/abs/2512.16676"/>
    <updated>2025-12-18T10:46:15+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current pra...&lt;br/&gt;Upvotes: 203&lt;br/&gt;GitHub Stars: 2180&lt;br/&gt;Authors: Hao Liang, Xiaochen Ma, Zhou Liu&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/OpenDCAI/DataFlow&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/OpenDCAI/DataFlow&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2512.16676&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>Kling-Omni Technical Report</title>
    <id>8328813bd23f18dae6c4e2c63c9de3c9</id>
    <link href="https://arxiv.org/abs/2512.16776"/>
    <updated>2025-12-18T12:08:12+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bri...&lt;br/&gt;Upvotes: 164&lt;br/&gt;Authors: Kling Team, Jialu Chen, Yuanzheng Ci&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2512.16776&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>Memory in the Age of AI Agents</title>
    <id>7e91c360c930c55d2bb71f49e0548368</id>
    <link href="https://arxiv.org/abs/2512.13564"/>
    <updated>2025-12-15T12:22:34+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As research on agent memory rapidly expands and attracts unprecedented attention, the field has als...&lt;br/&gt;Upvotes: 132&lt;br/&gt;GitHub Stars: 699&lt;br/&gt;Authors: Yuyang Hu, Shichun Liu, Yanwei Yue&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/Shichun-Liu/Agent-Memory-Paper-List&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2512.13564&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>Step-GUI Technical Report</title>
    <id>c5594c0f22e32e8bb34b27ab240dc8cb</id>
    <link href="https://arxiv.org/abs/2512.15431"/>
    <updated>2025-12-17T08:26:30+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training da...&lt;br/&gt;Upvotes: 129&lt;br/&gt;GitHub Stars: 1857&lt;br/&gt;Authors: Haolong Yan, Jia Wang, Xin Huang&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/stepfun-ai/gelab-zero&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://opengelab.github.io/&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2512.15431&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance</title>
    <id>00865636816805dd176a5123d69c4b66</id>
    <link href="https://arxiv.org/abs/2512.08765"/>
    <updated>2025-12-09T11:13:55+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and li...&lt;br/&gt;Upvotes: 128&lt;br/&gt;GitHub Stars: 514&lt;br/&gt;Authors: Ruihang Chu, Yefei He, Zhekai Chen&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/ali-vilab/Wan-Move&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://wan-move.github.io/&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2512.08765&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>DeepCode: Open Agentic Coding</title>
    <id>84e0c9ab3b32b3eab139bdd727fe4f8c</id>
    <link href="https://arxiv.org/abs/2512.07921"/>
    <updated>2025-12-08T11:07:13+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>Recent advances in large language models (LLMs) have given rise to powerful coding agents, making it possible for code assistants to evolve into code engineers. However, existing methods still face si...&lt;br/&gt;Upvotes: 31&lt;br/&gt;GitHub Stars: 13708&lt;br/&gt;Authors: Zongwei Li, Zhonghang Li, Zirui Guo&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/HKUDS/DeepCode&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2512.07921&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>Sharp Monocular View Synthesis in Less Than a Second</title>
    <id>b0feeb7f591514204e557e85f0c2b6ca</id>
    <link href="https://arxiv.org/abs/2512.10685"/>
    <updated>2025-12-11T09:34:11+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>We present SHARP, an approach to photorealistic view synthesis from a single image. Given a single photograph, SHARP regresses the parameters of a 3D Gaussian representation of the depicted scene. Thi...&lt;br/&gt;Upvotes: 26&lt;br/&gt;GitHub Stars: 6606&lt;br/&gt;Authors: Lars Mescheder, Wei Dong, Shiwei Li&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/apple/ml-sharp&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://apple.github.io/ml-sharp/&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2512.10685&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization</title>
    <id>4abe591dec20d02c68570faea169dfcb</id>
    <link href="https://arxiv.org/abs/2512.24615"/>
    <updated>2025-12-30T23:17:36+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual ef...&lt;br/&gt;Upvotes: 102&lt;br/&gt;GitHub Stars: 4150&lt;br/&gt;Authors: Yuchen Shi, Yuzheng Cai, Siqi Cai&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/TencentCloudADP/youtu-agent&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://tencentcloudadp.github.io/youtu-agent/&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2512.24615&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times</title>
    <id>5bb778f47e493a1d7bcf12aaaec0e25e</id>
    <link href="https://arxiv.org/abs/2512.16093"/>
    <updated>2025-12-17T21:21:30+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>We introduce TurboDiffusion, a video generation acceleration framework that can speed up end-to-end diffusion generation by 100-200x while maintaining video quality. TurboDiffusion mainly relies on se...&lt;br/&gt;Upvotes: 92&lt;br/&gt;GitHub Stars: 3110&lt;br/&gt;Authors: Jintao Zhang, Kaiwen Zheng, Kai Jiang&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/thu-ml/TurboDiffusion&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/thu-ml/TurboDiffusion&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2512.16093&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>SAM Audio: Segment Anything in Audio</title>
    <id>1c5e2b4f68f2d0420edc475a4ac95129</id>
    <link href="https://arxiv.org/abs/2512.18099"/>
    <updated>2025-12-19T17:14:23+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>General audio source separation is a key capability for multimodal AI systems that can perceive and reason about sound. Despite substantial progress in recent years, existing separation models are eit...&lt;br/&gt;Upvotes: 21&lt;br/&gt;GitHub Stars: 2876&lt;br/&gt;Authors: Bowen Shi, Andros Tjandra, John Hoffman&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/facebookresearch/sam-audio&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://ai.meta.com/samaudio/&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2512.18099&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem</title>
    <id>1f04a85a317b82f2c574c078f215f35e</id>
    <link href="https://arxiv.org/abs/2512.24873"/>
    <updated>2025-12-31T09:03:39+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>Agentic crafting requires LLMs to operate in real-world environments over multiple turns by taking actions, observing outcomes, and iteratively refining artifacts. Despite its importance, the open-sou...&lt;br/&gt;Upvotes: 91&lt;br/&gt;GitHub Stars: 2587&lt;br/&gt;Authors: Weixun Wang, XiaoXiao Xu, Wanhe An&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/alibaba/ROLL&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2512.24873&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
</feed>
