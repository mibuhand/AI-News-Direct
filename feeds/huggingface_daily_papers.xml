<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Huggingface Daily Papers</title>
  <id>tag:ai-news-direct.local,2025:huggingface_daily_papers</id>
  <icon>https://huggingface.co/favicon.ico</icon>
  <logo>https://huggingface.co/favicon.ico</logo>
  <updated>2026-02-09T10:06:50.046002+00:00</updated>
  <author>
    <name>AI News Direct</name>
  </author>
  <link href="https://raw.githubusercontent.com/mibuhand/AI-News-Direct/main/feeds/huggingface_daily_papers.xml" rel="self"/>
  <entry>
    <title>Green-VLA: Staged Vision-Language-Action Model for Generalist Robots</title>
    <id>60666ed538be4ebda8058b1f954547e6</id>
    <link href="https://arxiv.org/abs/2602.00919"/>
    <updated>2026-01-31T17:13:23+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>We introduce Green-VLA, a staged Vision-Language-Action (VLA) framework for real-world deployment on the Green humanoid robot while maintaining generalization across diverse embodiments. Green-VLA fol...&lt;br/&gt;Upvotes: 263&lt;br/&gt;GitHub Stars: 32&lt;br/&gt;Authors: I. Apanasevich, M. Artemyev, R. Babakyan&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/greenvla/GreenVLA&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://greenvla.github.io&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2602.00919&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>ERNIE 5.0 Technical Report</title>
    <id>b1cff772289aab941c862bdff282297c</id>
    <link href="https://arxiv.org/abs/2602.04705"/>
    <updated>2026-02-04T11:18:15+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>In this report, we introduce ERNIE 5.0, a natively autoregressive foundation model desinged for unified multimodal understanding and generation across text, image, video, and audio. All modalities are...&lt;br/&gt;Upvotes: 241&lt;br/&gt;Authors: Haifeng Wang, Hua Wu, Tian Wu&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2602.04705&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>Kimi K2.5: Visual Agentic Intelligence</title>
    <id>4f9739b510c1386a39502a9995b593ed</id>
    <link href="https://arxiv.org/abs/2602.02276"/>
    <updated>2026-02-02T11:17:38+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhan...&lt;br/&gt;Upvotes: 210&lt;br/&gt;GitHub Stars: 839&lt;br/&gt;Authors: Kimi Team, Tongtong Bai, Yifan Bai&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/MoonshotAI/Kimi-K2.5&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://www.kimi.com/blog/kimi-k2-5.html&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2602.02276&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning</title>
    <id>01505ff128b7cc4069a838f067f22852</id>
    <link href="https://arxiv.org/abs/2601.06943"/>
    <updated>2026-01-11T10:07:37+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perfor...&lt;br/&gt;Upvotes: 210&lt;br/&gt;GitHub Stars: 140&lt;br/&gt;Authors: Chengwen Liu, Xiaomin Yu, Zhuoyue Chang&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/QuantaAlpha/VideoDR-Benchmark&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://videodr-benchmark.github.io/#/home&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2601.06943&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>BabyVision: Visual Reasoning Beyond Language</title>
    <id>7d8f529e24394b1b5750aeba9852eed4</id>
    <link href="https://arxiv.org/abs/2601.06521"/>
    <updated>2026-01-10T05:42:44+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understandin...&lt;br/&gt;Upvotes: 196&lt;br/&gt;GitHub Stars: 175&lt;br/&gt;Authors: Liang Chen, Weichu Xie, Yiyan Liang&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/UniPat-AI/BabyVision&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://unipat.ai/blog/BabyVision&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2601.06521&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>Agentic Reasoning for Large Language Models</title>
    <id>c030a09f9828ad54ac0c86051210dce0</id>
    <link href="https://arxiv.org/abs/2601.12538"/>
    <updated>2026-01-18T13:58:23+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world se...&lt;br/&gt;Upvotes: 194&lt;br/&gt;GitHub Stars: 907&lt;br/&gt;Authors: Tianxin Wei, Ting-Wei Li, Zhining Liu&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/weitianxin/Awesome-Agentic-Reasoning&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2601.12538&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>Qwen3-TTS Technical Report</title>
    <id>e6cd663cc8eaf53e2ed564e037a90af3</id>
    <link href="https://arxiv.org/abs/2601.15621"/>
    <updated>2026-01-21T22:51:43+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>In this report, we present the Qwen3-TTS series, a family of advanced multilingual, controllable, robust, and streaming text-to-speech models. Qwen3-TTS supports state-of-the-art 3-second voice clonin...&lt;br/&gt;Upvotes: 60&lt;br/&gt;GitHub Stars: 7217&lt;br/&gt;Authors: Hangrui Hu, Xinfa Zhu, Ting He&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/QwenLM/Qwen3-TTS&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2601.15621&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>HeartMuLa: A Family of Open Sourced Music Foundation Models</title>
    <id>e4456066efdcdf46fe4b108089570f93</id>
    <link href="https://arxiv.org/abs/2601.10547"/>
    <updated>2026-01-15T11:14:25+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>We present a family of open-source Music Foundation Models designed to advance large-scale music understanding and generation across diverse tasks and modalities. Our framework consists of four major ...&lt;br/&gt;Upvotes: 42&lt;br/&gt;GitHub Stars: 3253&lt;br/&gt;Authors: Dongchao Yang, Yuxin Xie, Yuguo Yin&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/HeartMuLa/heartlib&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://heartmula.github.io/&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2601.10547&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>PaperBanana: Automating Academic Illustration for AI Scientists</title>
    <id>3ca1c9448a1392dc62dcb1e40f0a07ae</id>
    <link href="https://arxiv.org/abs/2601.23265"/>
    <updated>2026-01-30T13:33:37+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>Despite rapid advances in autonomous AI scientists powered by language models, generating publication-ready illustrations remains a labor-intensive bottleneck in the research workflow. To lift this bu...&lt;br/&gt;Upvotes: 156&lt;br/&gt;GitHub Stars: 2723&lt;br/&gt;Authors: Dawei Zhu, Rui Meng, Yale Song&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/dwzhu-pku/PaperBanana&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://dwzhu-pku.github.io/PaperBanana/&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2601.23265&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>Advancing Open-source World Models</title>
    <id>de0ab4deaf169a5f7442c17a4444ef3f</id>
    <link href="https://arxiv.org/abs/2601.20540"/>
    <updated>2026-01-28T07:37:01+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>We present LingBot-World, an open-sourced world simulator stemming from video generation. Positioned as a top-tier world model, LingBot-World offers the following features. (1) It maintains high fidel...&lt;br/&gt;Upvotes: 120&lt;br/&gt;GitHub Stars: 2722&lt;br/&gt;Authors: Robbyant Team, Zelin Gao, Qiuyu Wang&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/Robbyant/lingbot-world/&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://technology.robbyant.com/lingbot-world&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2601.20540&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>Why Steering Works: Toward a Unified View of Language Model Parameter Dynamics</title>
    <id>9cef780d0ecdc7bc89377cb0a062dde8</id>
    <link href="https://arxiv.org/abs/2602.02343"/>
    <updated>2026-02-02T12:04:36+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>Methods for controlling large language models (LLMs), including local weight fine-tuning, LoRA-based adaptation, and activation-based interventions, are often studied in isolation, obscuring their con...&lt;br/&gt;Upvotes: 13&lt;br/&gt;GitHub Stars: 2712&lt;br/&gt;Authors: Ziwen Xu, Chenyan Wu, Hengyu Sun&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/zjunlp/EasyEdit/blob/main/examples/SPLIT.md&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2602.02343&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning</title>
    <id>e77f7ec067148dac222fa8934786f0dd</id>
    <link href="https://arxiv.org/abs/2602.04634"/>
    <updated>2026-02-04T10:05:12+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>Recent advancements in Large Language Models (LLMs) have largely focused on depth scaling, where a single agent solves long-horizon problems with multi-turn reasoning and tool use. However, as tasks g...&lt;br/&gt;Upvotes: 87&lt;br/&gt;GitHub Stars: 2414&lt;br/&gt;Authors: Zelai Xu, Zhexuan Xu, Ruize Zhang&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/RLinf/RLinf/tree/main/examples/wideseek_r1&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://wideseek-r1.github.io/&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2602.04634&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
</feed>
