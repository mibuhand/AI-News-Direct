<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Huggingface Daily Papers</title>
  <id>tag:ai-news-direct.local,2025:huggingface_daily_papers</id>
  <icon>https://huggingface.co/favicon.ico</icon>
  <logo>https://huggingface.co/favicon.ico</logo>
  <updated>2025-12-29T21:20:25.117615+00:00</updated>
  <author>
    <name>AI News Direct</name>
  </author>
  <link href="https://raw.githubusercontent.com/mibuhand/AI-News-Direct/main/feeds/huggingface_daily_papers.xml" rel="self"/>
  <entry>
    <title>From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence</title>
    <id>7e43a061a51737f2587e45e69aa58ff1</id>
    <link href="https://arxiv.org/abs/2511.18538"/>
    <updated>2025-11-23T12:09:34+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adop...&lt;br/&gt;Upvotes: 278&lt;br/&gt;Authors: Jian Yang, Xianglong Liu, Weifeng Lv&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2511.18538&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models</title>
    <id>c360b805a86862ca56d4316aea6a684d</id>
    <link href="https://arxiv.org/abs/2512.02556"/>
    <updated>2025-12-02T04:25:14+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) De...&lt;br/&gt;Upvotes: 237&lt;br/&gt;Authors: DeepSeek-AI, Aixin Liu, Aoxue Mei&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2512.02556&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer</title>
    <id>8e9429d7feaa43a78c484db81c3eddf8</id>
    <link href="https://arxiv.org/abs/2511.22699"/>
    <updated>2025-11-27T13:52:07+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Imag...&lt;br/&gt;Upvotes: 215&lt;br/&gt;GitHub Stars: 8172&lt;br/&gt;Authors: Z-Image Team, Huanqia Cai, Sihan Cao&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/Tongyi-MAI/Z-Image&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://tongyi-mai.github.io/Z-Image-blog/&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2511.22699&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI</title>
    <id>adcafb2025f84cc88e13db2e967d246f</id>
    <link href="https://arxiv.org/abs/2512.16676"/>
    <updated>2025-12-18T10:46:15+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current pra...&lt;br/&gt;Upvotes: 192&lt;br/&gt;GitHub Stars: 1914&lt;br/&gt;Authors: Hao Liang, Xiaochen Ma, Zhou Liu&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/OpenDCAI/DataFlow&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/OpenDCAI/DataFlow&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2512.16676&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length</title>
    <id>bdd6e066d5b32b9c61996386576f3682</id>
    <link href="https://arxiv.org/abs/2512.04677"/>
    <updated>2025-12-04T06:11:24+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audi...&lt;br/&gt;Upvotes: 167&lt;br/&gt;GitHub Stars: 1225&lt;br/&gt;Authors: Yubo Huang, Hailong Guo, Fangtai Wu&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/Alibaba-Quark/LiveAvatar&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://liveavatar.github.io/&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2512.04677&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>LongVT: Incentivizing &quot;Thinking with Long Videos&quot; via Native Tool Calling</title>
    <id>0f96413995b3ae42aea420d3714efb11</id>
    <link href="https://arxiv.org/abs/2511.20785"/>
    <updated>2025-11-25T14:22:48+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>Large multimodal models (LMMs) have shown great potential for video reasoning with textual Chain-of-Thought. However, they remain vulnerable to hallucinations, especially when processing long-form vid...&lt;br/&gt;Upvotes: 167&lt;br/&gt;GitHub Stars: 157&lt;br/&gt;Authors: Zuhao Yang, Sudong Wang, Kaichen Zhang&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/EvolvingLMMs-Lab/LongVT&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://evolvinglmms-lab.github.io/LongVT/&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2511.20785&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>Qwen3-VL Technical Report</title>
    <id>5b6ff13ede65701e5f764a0267eca8df</id>
    <link href="https://arxiv.org/abs/2511.21631"/>
    <updated>2025-11-26T12:59:08+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks. It natively supports interleaved...&lt;br/&gt;Upvotes: 144&lt;br/&gt;GitHub Stars: 17453&lt;br/&gt;Authors: Shuai Bai, Yuxuan Cai, Ruizhe Chen&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/QwenLM/Qwen3-VL&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://chat.qwen.ai&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2511.21631&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>DeepCode: Open Agentic Coding</title>
    <id>84e0c9ab3b32b3eab139bdd727fe4f8c</id>
    <link href="https://arxiv.org/abs/2512.07921"/>
    <updated>2025-12-08T11:07:13+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>Recent advances in large language models (LLMs) have given rise to powerful coding agents, making it possible for code assistants to evolve into code engineers. However, existing methods still face si...&lt;br/&gt;Upvotes: 31&lt;br/&gt;GitHub Stars: 13179&lt;br/&gt;Authors: Zongwei Li, Zhonghang Li, Zirui Guo&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/HKUDS/DeepCode&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2512.07921&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>Decoupled DMD: CFG Augmentation as the Spear, Distribution Matching as the Shield</title>
    <id>18503dda198b1fcbf7bb01dbf7296d31</id>
    <link href="https://arxiv.org/abs/2511.22677"/>
    <updated>2025-11-27T13:24:28+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>Diffusion model distillation has emerged as a powerful technique for creating efficient few-step and single-step generators. Among these, Distribution Matching Distillation (DMD) and its variants stan...&lt;br/&gt;Upvotes: 28&lt;br/&gt;GitHub Stars: 8173&lt;br/&gt;Authors: Dongyang Liu, Peng Gao, David Liu&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/Tongyi-MAI/Z-Image/tree/main&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://tongyi-mai.github.io/Z-Image-blog/&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2511.22677&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>Sharp Monocular View Synthesis in Less Than a Second</title>
    <id>b0feeb7f591514204e557e85f0c2b6ca</id>
    <link href="https://arxiv.org/abs/2512.10685"/>
    <updated>2025-12-11T09:34:11+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>We present SHARP, an approach to photorealistic view synthesis from a single image. Given a single photograph, SHARP regresses the parameters of a 3D Gaussian representation of the depicted scene. Thi...&lt;br/&gt;Upvotes: 15&lt;br/&gt;GitHub Stars: 6026&lt;br/&gt;Authors: Lars Mescheder, Wei Dong, Shiwei Li&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/apple/ml-sharp&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://apple.github.io/ml-sharp/&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2512.10685&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>Skywork-R1V4: Toward Agentic Multimodal Intelligence through Interleaved Thinking with Images and DeepResearch</title>
    <id>8fb3e0f897edda830f8a5713a671e918</id>
    <link href="https://arxiv.org/abs/2512.02395"/>
    <updated>2025-12-01T23:12:57+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>Despite recent progress in multimodal agentic systems, existing approaches often treat image manipulation and web search as disjoint capabilities, rely heavily on costly reinforcement learning, and la...&lt;br/&gt;Upvotes: 47&lt;br/&gt;GitHub Stars: 3134&lt;br/&gt;Authors: Yifan Zhang, Liang Hu, Haofeng Sun&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/SkyworkAI/Skywork-R1V/tree/main/r1v4&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://docs.skyworkmodel.ai/r1v4/api-reference/completions.html&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2512.02395&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
</feed>
