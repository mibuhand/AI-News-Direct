<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Huggingface Daily Papers</title>
  <id>tag:ai-news-direct.local,2025:huggingface_daily_papers</id>
  <icon>https://huggingface.co/favicon.ico</icon>
  <logo>https://huggingface.co/favicon.ico</logo>
  <updated>2025-10-09T21:17:47.867548+00:00</updated>
  <author>
    <name>AI News Direct</name>
  </author>
  <link href="https://raw.githubusercontent.com/mibuhand/AI-News-Direct/main/feeds/huggingface_daily_papers.xml" rel="self"/>
  <entry>
    <title>Sharing is Caring: Efficient LM Post-Training with Collective RL
  Experience Sharing</title>
    <id>8ccd2aceeb4aa375ae3aaa881748eeca</id>
    <link href="https://arxiv.org/abs/2509.08721"/>
    <updated>2025-09-10T12:14:20+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>Post-training language models (LMs) with reinforcement learning (RL) can
enhance their complex reasoning capabilities without supervised fine-tuning, as
demonstrated by DeepSeek-R1-Zero. However, effe...&lt;br/&gt;Upvotes: 668&lt;br/&gt;GitHub Stars: 1487&lt;br/&gt;Authors: Jeffrey Amico, Gabriel Passamani Andrade, John Donaghy&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/gensyn-ai/rl-swarm&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://blog.gensyn.ai/sapo-efficient-lm-post-training-with-collective-rl/&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2509.08721&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>The Dragon Hatchling: The Missing Link between the Transformer and
  Models of the Brain</title>
    <id>7a9ae43171e964d5b9a58c9a76f8c373</id>
    <link href="https://arxiv.org/abs/2509.26507"/>
    <updated>2025-09-30T12:49:01+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>The relationship between computing systems and the brain has served as
motivation for pioneering theoreticians since John von Neumann and Alan Turing.
Uniform, scale-free biological networks, such as ...&lt;br/&gt;Upvotes: 447&lt;br/&gt;GitHub Stars: 2984&lt;br/&gt;Authors: Adrian Kosowski, PrzemysÅ‚aw UznaÅ„ski, Jan Chorowski&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/pathwaycom/bdh&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2509.26507&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action
  Model</title>
    <id>a785dc23394ddf009c4095e44be6b818</id>
    <link href="https://arxiv.org/abs/2509.09372"/>
    <updated>2025-09-11T07:42:21+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>Vision-Language-Action (VLA) models typically bridge the gap between
perceptual and action spaces by pre-training a large-scale Vision-Language
Model (VLM) on robotic data. While this approach greatly...&lt;br/&gt;Upvotes: 223&lt;br/&gt;GitHub Stars: 884&lt;br/&gt;Authors: Yihao Wang, Pengxiang Ding, Lingxiao Li&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/OpenHelix-Team/VLA-Adapter&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://vla-adapter.github.io/&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2509.09372&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>Less is More: Recursive Reasoning with Tiny Networks</title>
    <id>3d55a02b14bdc7a04256ce32475eb075</id>
    <link href="https://arxiv.org/abs/2510.04871"/>
    <updated>2025-10-06T10:58:08+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>Hierarchical Reasoning Model (HRM) is a novel approach using two small neural
networks recursing at different frequencies. This biologically inspired method
beats Large Language models (LLMs) on hard ...&lt;br/&gt;Upvotes: 178&lt;br/&gt;GitHub Stars: 1936&lt;br/&gt;Authors: Alexia Jolicoeur-Martineau&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/SamsungSAILMontreal/TinyRecursiveModels&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://alexiajm.github.io/2025/09/29/tiny_recursive_models.html#&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2510.04871&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>A Survey of Reinforcement Learning for Large Reasoning Models</title>
    <id>5b09690dc892c88c31036888ff90135f</id>
    <link href="https://arxiv.org/abs/2509.08827"/>
    <updated>2025-09-10T13:59:43+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>In this paper, we survey recent advances in Reinforcement Learning (RL) for
reasoning with Large Language Models (LLMs). RL has achieved remarkable success
in advancing the frontier of LLM capabilitie...&lt;br/&gt;Upvotes: 177&lt;br/&gt;GitHub Stars: 1685&lt;br/&gt;Authors: Kaiyan Zhang, Yuxin Zuo, Bingxiang He&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2509.08827&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>LongLive: Real-time Interactive Long Video Generation</title>
    <id>4618188e5babf65ab96739c065ddfd61</id>
    <link href="https://arxiv.org/abs/2509.22622"/>
    <updated>2025-09-26T13:48:24+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>We present LongLive, a frame-level autoregressive (AR) framework for
real-time and interactive long video generation. Long video generation presents
challenges in both efficiency and quality. Diffusio...&lt;br/&gt;Upvotes: 174&lt;br/&gt;GitHub Stars: 629&lt;br/&gt;Authors: Shuai Yang, Wei Huang, Ruihang Chu&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/NVlabs/LongLive&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://nvlabs.github.io/LongLive/&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2509.22622&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>MinerU2.5: A Decoupled Vision-Language Model for Efficient
  High-Resolution Document Parsing</title>
    <id>b1fe8132db01bc9650eda3e26023bc30</id>
    <link href="https://arxiv.org/abs/2509.22186"/>
    <updated>2025-09-26T06:45:48+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>We introduce MinerU2.5, a 1.2B-parameter document parsing vision-language
model that achieves state-of-the-art recognition accuracy while maintaining
exceptional computational efficiency. Our approach...&lt;br/&gt;Upvotes: 101&lt;br/&gt;GitHub Stars: 45503&lt;br/&gt;Authors: Junbo Niu, Zheng Liu, Zhuangcheng Gu&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/opendatalab/MinerU&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://opendatalab.github.io/MinerU/&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2509.22186&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and
  Training Recipe</title>
    <id>59f26db9f2ae0d3d43973c1ffbd40568</id>
    <link href="https://arxiv.org/abs/2509.18154"/>
    <updated>2025-09-16T15:41:48+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>Multimodal Large Language Models (MLLMs) are undergoing rapid progress and
represent the frontier of AI development. However, their training and inference
efficiency have emerged as a core bottleneck ...&lt;br/&gt;Upvotes: 46&lt;br/&gt;GitHub Stars: 22050&lt;br/&gt;Authors: Tianyu Yu, Zefan Wang, Chongyi Wang&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/OpenBMB/MiniCPM-V&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2509.18154&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>Scaling Agents via Continual Pre-training</title>
    <id>17d435d0d4a4864c8247d4e12399b781</id>
    <link href="https://arxiv.org/abs/2509.13310"/>
    <updated>2025-09-16T13:57:19+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>Large language models (LLMs) have evolved into agentic systems capable of
autonomous tool use and multi-step reasoning for complex problem-solving.
However, post-training approaches building upon gene...&lt;br/&gt;Upvotes: 109&lt;br/&gt;GitHub Stars: 15642&lt;br/&gt;Authors: Liangcai Su, Zhen Zhang, Guangyu Li&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/Alibaba-NLP/DeepResearch///&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://tongyi-agent.github.io/blog/&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2509.13310&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for
  Open-Ended Deep Research</title>
    <id>27945bd6f1634bfd914ab287d0fdaf8e</id>
    <link href="https://arxiv.org/abs/2509.13312"/>
    <updated>2025-09-16T13:57:21+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>This paper tackles open-ended deep research (OEDR), a complex challenge where
AI agents must synthesize vast web-scale information into insightful reports.
Current approaches are plagued by dual-fold ...&lt;br/&gt;Upvotes: 103&lt;br/&gt;GitHub Stars: 15642&lt;br/&gt;Authors: Zijian Li, Xin Guan, Bo Zhang&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/Alibaba-NLP/DeepResearch&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://tongyi-agent.github.io/blog/&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2509.13312&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic
  Data and Scalable Reinforcement Learning</title>
    <id>d4b4f91354d2792a0826158fb9d00e0b</id>
    <link href="https://arxiv.org/abs/2509.13305"/>
    <updated>2025-09-16T13:57:03+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>Transcending human cognitive limitations represents a critical frontier in
LLM training. Proprietary agentic systems like DeepResearch have demonstrated
superhuman capabilities on extremely complex in...&lt;br/&gt;Upvotes: 86&lt;br/&gt;GitHub Stars: 15642&lt;br/&gt;Authors: Kuan Li, Zhongwang Zhang, Huifeng Yin&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/Alibaba-NLP/DeepResearch/&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://tongyi-agent.github.io/blog/&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2509.13305&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>ReSum: Unlocking Long-Horizon Search Intelligence via Context
  Summarization</title>
    <id>56ef9e3095cf3666002acddf23dee4ac</id>
    <link href="https://arxiv.org/abs/2509.13313"/>
    <updated>2025-09-16T13:57:22+00:00</updated>
    <category term="research"/>
    <category term="paper"/>
    <summary>Large Language Model (LLM)-based web agents demonstrate strong performance on
knowledge-intensive tasks but are hindered by context window limitations in
paradigms like ReAct. Complex queries involvin...&lt;br/&gt;Upvotes: 76&lt;br/&gt;GitHub Stars: 15642&lt;br/&gt;Authors: Xixi Wu, Kuan Li, Yida Zhao&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://github.com/Alibaba-NLP/DeepResearch//&quot;&gt;GitHub&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://tongyi-agent.github.io/blog/&quot;&gt;Project Page&lt;/a&gt;&lt;br/&gt;ðŸ”— &lt;a href=&quot;https://huggingface.co/papers/2509.13313&quot;&gt;Hugging Face&lt;/a&gt;</summary>
  </entry>
</feed>
